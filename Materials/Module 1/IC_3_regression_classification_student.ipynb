{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Scratch to Scikit-learn: Regression & Classification (CSC 422)\n",
        "\n",
        "**Duration:** 2 hours  \n",
        "**Format:** Live coding with student participation  \n",
        "**Course:** CSC 422 - Machine and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "By the end of class, students should:\n",
        "- Recognize the value of using libraries (scikit-learn) vs. coding from scratch\n",
        "- Implement regression with scikit-learn and compare with their scratch version\n",
        "- Understand the general pipeline of supervised ML (fit ‚Üí predict ‚Üí evaluate)\n",
        "- Be introduced to core shallow classifiers (kNN, logistic regression, Na√Øve Bayes)\n",
        "- Practice applying models to small datasets with scikit-learn\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è± Timeline\n",
        "\n",
        "- **0‚Äì15 min** ‚Äî Bridge from Scratch to Library\n",
        "- **15‚Äì40 min** ‚Äî Supervised ML Workflow with Scikit-learn\n",
        "- **40‚Äì60 min** ‚Äî Transition to Classification\n",
        "- **60‚Äì90 min** ‚Äî Shallow Classification Models with Scikit-learn\n",
        "- **90‚Äì115 min** ‚Äî Guided Lab Exercise\n",
        "- **115‚Äì120 min** ‚Äî Wrap-Up & Forward Look"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "We'll need both basic scientific computing tools and scikit-learn for today's exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for scientific computing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scikit-learn imports for machine learning\n",
        "from sklearn.linear_model import _____________, _____________\n",
        "from sklearn.neighbors import _____________\n",
        "from sklearn.naive_bayes import _____________\n",
        "from sklearn.model_selection import _____________\n",
        "from sklearn.datasets import _____________, _____________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation metrics and utilities\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reproducibility and verification\n",
        "np.random.seed(42)\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 0‚Äì15 min: Bridge from Scratch to Library\n",
        "\n",
        "**Goal:** Connect IC_2's mathematical foundations to professional machine learning tools\n",
        "\n",
        "## Review: What We Built in IC_2\n",
        "\n",
        "Last class, we implemented gradient descent from scratch. Today, we'll see how scikit-learn uses the same mathematical principles with a much cleaner interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the same dataset from IC_2\n",
        "a_true, b_true = 2.5, -1.0\n",
        "n_points = 100\n",
        "\n",
        "# Generate identical noisy linear data\n",
        "x = np.random.uniform(-2, 2, n_points)\n",
        "y = a_true * x + b_true + np.random.normal(0, 0.5, n_points)\n",
        "\n",
        "print(f\"Dataset: {n_points} points with noise\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split (same as IC_2)\n",
        "split_idx = int(0.8 * n_points)\n",
        "x_train, y_train = x[:split_idx], y[:split_idx]\n",
        "x_test, y_test = x[split_idx:], y[split_idx:]\n",
        "\n",
        "print(f\"Split: {len(x_train)} train, {len(x_test)} test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Manual Approach (IC_2 Review)\n",
        "\n",
        "Let's quickly recreate our gradient descent solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our scratch gradient descent (simplified version)\n",
        "def train_scratch(x, y, learning_rate=0.1, steps=100):\n",
        "    a, b = 0.0, 0.0\n",
        "    n = len(x)\n",
        "    \n",
        "    for _ in range(steps):\n",
        "        # Compute gradients\n",
        "        pred = a * x + b\n",
        "        grad_a = (2/n) * np.sum((pred - y) * x)\n",
        "        grad_b = (2/n) * np.sum(pred - y)\n",
        "        \n",
        "        # Update parameters\n",
        "        a -= learning_rate * grad_a\n",
        "        b -= learning_rate * grad_b\n",
        "    \n",
        "    return a, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with our scratch implementation\n",
        "a_scratch, b_scratch = train_scratch(x_train, y_train)\n",
        "scratch_mse = np.mean((y_test - (a_scratch * x_test + b_scratch))**2)\n",
        "\n",
        "print(f\"Scratch result: a={a_scratch:.3f}, b={b_scratch:.3f}\")\n",
        "print(f\"Test MSE: {scratch_mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Scikit-Learn Way\n",
        "\n",
        "Watch how ~15 lines of gradient descent becomes 3 lines of sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape data for sklearn (expects 2D arrays)\n",
        "X_train = x_train.reshape(-1, 1)\n",
        "X_test = x_test.reshape(-1, 1)\n",
        "\n",
        "print(f\"Reshaped: {x_train.shape} ‚Üí {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The sklearn magic: fit ‚Üí predict ‚Üí evaluate\n",
        "model = _____________()\n",
        "model.______(X_train, y_train)\n",
        "sklearn_mse = mean_squared_error(y_test, model._______(X_test))\n",
        "\n",
        "print(f\"Sklearn result: a={model.coef_[0]:.3f}, b={model.intercept_:.3f}\")\n",
        "print(f\"Test MSE: {sklearn_mse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare results\n",
        "print(f\"üéØ COMPARISON:\")\n",
        "print(f\"Scratch:  MSE = {scratch_mse:.4f}\")\n",
        "print(f\"Sklearn:  MSE = {sklearn_mse:.4f}\")\n",
        "print(f\"Difference: {abs(scratch_mse - sklearn_mse):.6f}\")\n",
        "print(\"‚úÖ Nearly identical results!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion Break (2 minutes)\n",
        "\n",
        "**Question for students:** *\"When would you implement from scratch vs. use a library like scikit-learn?\"*\n",
        "\n",
        "**Think about:**\n",
        "- Learning and understanding\n",
        "- Production systems\n",
        "- Custom requirements\n",
        "- Time constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 15‚Äì40 min: Supervised ML Workflow with Scikit-learn\n",
        "\n",
        "**Goal:** Master the universal pipeline that works for ANY supervised learning problem\n",
        "\n",
        "## The Universal ML Pipeline\n",
        "\n",
        "Every supervised ML project follows these 5 steps, regardless of algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The Universal ML Pipeline\n",
        "print(\"üîÑ THE 5-STEP ML PIPELINE:\")\n",
        "print(\"1Ô∏è‚É£  LOAD ‚Üí Get your dataset\")\n",
        "print(\"2Ô∏è‚É£  SPLIT ‚Üí Separate train/test\")\n",
        "print(\"3Ô∏è‚É£  FIT ‚Üí Train the model\")\n",
        "print(\"4Ô∏è‚É£  PREDICT ‚Üí Make predictions\")\n",
        "print(\"5Ô∏è‚É£  EVALUATE ‚Üí Measure performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Dataset\n",
        "\n",
        "Let's apply this pipeline to a real medical dataset - diabetes progression prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load a real dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data      # Features: age, BMI, blood pressure, etc.\n",
        "y_diabetes = diabetes.target    # Target: disease progression score\n",
        "\n",
        "print(f\"Dataset shape: {X_diabetes.shape}\")\n",
        "print(f\"Features: {len(diabetes.feature_names)}\")\n",
        "print(f\"Target range: {y_diabetes.min():.0f} to {y_diabetes.max():.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the features\n",
        "print(\"üìä Medical features measured:\")\n",
        "for i, feature in enumerate(diabetes.feature_names):\n",
        "    print(f\"   {i+1}. {feature}\")\n",
        "\n",
        "print(f\"\\nüéØ Task: Predict disease progression from medical measurements\")\n",
        "print(\"   This is REGRESSION (continuous target)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Split Train/Test\n",
        "\n",
        "Sklearn provides an automatic splitting function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Automatic train/test split\n",
        "X_train_diab, X_test_diab, y_train_diab, y_test_diab = _____________(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train_diab.shape[0]} patients\")\n",
        "print(f\"Test set: {X_test_diab.shape[0]} patients\")\n",
        "print(f\"Features: {X_train_diab.shape[1]} per patient\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steps 3-5: Fit ‚Üí Predict ‚Üí Evaluate\n",
        "\n",
        "The same API pattern we just learned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Fit the model (same API!)\n",
        "diabetes_model = _____________()\n",
        "diabetes_model.______(X_train_diab, y_train_diab)\n",
        "\n",
        "print(f\"‚úÖ Model trained on {len(diabetes.feature_names)} features\")\n",
        "print(f\"Intercept: {diabetes_model.intercept_:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Make predictions (same API!)\n",
        "y_train_pred = diabetes_model._______(X_train_diab)\n",
        "y_test_pred = diabetes_model._______(X_test_diab)\n",
        "\n",
        "print(f\"Predictions made for {len(y_test_pred)} test patients\")\n",
        "print(f\"Example: actual={y_test_diab[0]:.0f}, predicted={y_test_pred[0]:.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Evaluate performance\n",
        "train_mse = mean_squared_error(y_train_diab, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test_diab, y_test_pred)\n",
        "test_r2 = diabetes_model.score(X_test_diab, y_test_diab)\n",
        "\n",
        "print(f\"Train MSE: {train_mse:.1f}\")\n",
        "print(f\"Test MSE: {test_mse:.1f}\")\n",
        "print(f\"R¬≤ score: {test_r2:.3f} (higher = better)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results\n",
        "\n",
        "A quick plot to see how well our model performed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test_diab, y_test_pred, alpha=0.6)\n",
        "plt.plot([y_test_diab.min(), y_test_diab.max()], \n",
        "         [y_test_diab.min(), y_test_diab.max()], 'r--')\n",
        "plt.xlabel('Actual Progression')\n",
        "plt.ylabel('Predicted Progression')\n",
        "plt.title(f'Actual vs Predicted (R¬≤ = {test_r2:.3f})')\n",
        "\n",
        "# Feature importance\n",
        "plt.subplot(1, 2, 2)\n",
        "importance = np.abs(diabetes_model.coef_)\n",
        "plt.barh(diabetes.feature_names, importance)\n",
        "plt.xlabel('Coefficient Magnitude')\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion Break (3 minutes)\n",
        "\n",
        "**Question for students:** *\"To switch from LinearRegression to a RandomForest, what would you need to change in our code?\"*\n",
        "\n",
        "**Think about:**\n",
        "- Which lines stay the same?\n",
        "- Which lines change?\n",
        "- Why is this API design powerful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 40‚Äì60 min: Transition to Classification\n",
        "\n",
        "**Goal:** Understand when to predict categories vs. continuous values\n",
        "\n",
        "## Regression vs. Classification: The Key Difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The fundamental difference\n",
        "print(\"üîç REGRESSION vs CLASSIFICATION:\")\n",
        "print()\n",
        "print(\"üìà REGRESSION:\")\n",
        "print(\"   ‚Ä¢ Predict NUMBERS: 150.5, 23.7, 89.2\")\n",
        "print(\"   ‚Ä¢ Examples: house prices, temperature, disease progression\")\n",
        "print(\"   ‚Ä¢ Goal: Find best-fit line/curve\")\n",
        "\n",
        "print(\"\\nüè∑Ô∏è  CLASSIFICATION:\")\n",
        "print(\"   ‚Ä¢ Predict CATEGORIES: 'spam', 'cat', 'malignant'\")\n",
        "print(\"   ‚Ä¢ Examples: email type, animal species, cancer diagnosis\")\n",
        "print(\"   ‚Ä¢ Goal: Find decision boundaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual Comparison: Lines vs. Boundaries\n",
        "\n",
        "Let's see the difference in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create example data for comparison\n",
        "np.random.seed(42)\n",
        "\n",
        "# Regression example\n",
        "x_reg = np.random.uniform(-2, 2, 50)\n",
        "y_reg = 1.5 * x_reg + 0.5 + np.random.normal(0, 0.3, 50)\n",
        "\n",
        "# Classification example\n",
        "x1_class = np.random.uniform(-2, 2, 60)\n",
        "x2_class = np.random.uniform(-2, 2, 60)\n",
        "class_labels = (x1_class + x2_class > 0).astype(int)\n",
        "\n",
        "print(\"‚úÖ Example datasets created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the difference\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Left: Regression\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(x_reg, y_reg, alpha=0.6)\n",
        "plt.plot(x_reg, 1.5 * x_reg + 0.5, 'r-', linewidth=2, label='Best-fit line')\n",
        "plt.xlabel('Input Feature')\n",
        "plt.ylabel('Continuous Target')\n",
        "plt.title('REGRESSION\\nPredict Numbers')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: Classification\n",
        "plt.subplot(1, 2, 2)\n",
        "colors = ['red', 'blue']\n",
        "for i in range(2):\n",
        "    mask = class_labels == i\n",
        "    plt.scatter(x1_class[mask], x2_class[mask], c=colors[i], \n",
        "               label=f'Class {i}', alpha=0.6)\n",
        "\n",
        "# Decision boundary\n",
        "boundary_x = np.linspace(-2, 2, 100)\n",
        "boundary_y = -boundary_x\n",
        "plt.plot(boundary_x, boundary_y, 'k-', linewidth=2, label='Decision boundary')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('CLASSIFICATION\\nPredict Categories')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Three Key Classification Algorithms\n",
        "\n",
        "Today we'll explore three fundamental approaches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Three classification algorithms overview\n",
        "print(\"THREE CLASSIFICATION APPROACHES:\")\n",
        "print()\n",
        "print(\"k-NEAREST NEIGHBORS (k-NN)\")\n",
        "print(\"'You are who your neighbors are'\")\n",
        "print(\"Look at k closest points, vote on class\")\n",
        "\n",
        "print(\"\\nLOGISTIC REGRESSION\")\n",
        "print(\"'Find the best linear separator'\")\n",
        "print(\"Draw straight line between classes\")\n",
        "\n",
        "print(\"NA√èVE BAYES\")\n",
        "print(\"Use probability with independence assumption'\")\n",
        "print(\"Apply statistics to classify\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion Break (2 minutes)\n",
        "\n",
        "**Question for students:** *\"For spam email detection, which algorithm would you try first and why?\"*\n",
        "\n",
        "**Consider:**\n",
        "- Email features (words, sender, etc.)\n",
        "- Need for interpretability\n",
        "- Speed requirements\n",
        "- Data size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 60‚Äì90 min: Shallow Classification Models with Scikit-learn\n",
        "\n",
        "**Goal:** Apply the same pipeline to classification and compare three algorithms\n",
        "\n",
        "## Dataset: The Classic Iris Dataset\n",
        "\n",
        "We'll use the famous iris flowers dataset - perfect for learning classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data        # 4 features: sepal/petal length/width\n",
        "y_iris = iris.target      # 3 species: setosa, versicolor, virginica\n",
        "\n",
        "print(f\"Dataset: {X_iris.shape[0]} flowers, {X_iris.shape[1]} measurements\")\n",
        "print(f\"Species: {iris.target_names}\")\n",
        "print(f\"Features: {iris.feature_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train/test split (same function!)\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "print(f\"Training: {X_train_iris.shape[0]} flowers\")\n",
        "print(f\"Testing: {X_test_iris.shape[0]} flowers\")\n",
        "print(\"‚úÖ Balanced split maintained with stratify\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's explore our classification dataset\n",
        "print(\"Iris dataset shape:\", X_iris.shape, y_iris.shape)\n",
        "print(\"Feature names:\", iris.feature_names)\n",
        "print(\"Target classes:\", iris.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the iris dataset (2D projection)\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap='viridis')\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.title('Iris Dataset: Sepal Dimensions')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_iris[:, 2], X_iris[:, 3], c=y_iris, cmap='viridis')\n",
        "plt.xlabel('Petal Length (cm)')\n",
        "plt.ylabel('Petal Width (cm)')\n",
        "plt.title('Iris Dataset: Petal Dimensions')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the data points form distinct clusters by color (species). This is what makes classification possible - we can learn decision boundaries that separate these groups.\n",
        "\n",
        "### Training Our First Classification Model: k-Nearest Neighbors\n",
        "\n",
        "Let's start with k-NN, which makes predictions based on the k closest training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the iris data\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train k-NN classifier (k=3)\n",
        "knn = _____________(n_neighbors=3)\n",
        "knn.______(X_train_iris, y_train_iris)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions and evaluate\n",
        "y_pred_knn = knn._______(X_test_iris)\n",
        "accuracy_knn = knn.______(X_test_iris, y_test_iris)\n",
        "print(f\"k-NN Accuracy: {accuracy_knn:.3f}\")\n",
        "print(f\"Predictions: {y_pred_knn[:10]}\")\n",
        "print(f\"Actual:      {y_test_iris[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression for Classification\n",
        "\n",
        "Despite its name, logistic regression is actually a classification algorithm! It uses probability curves instead of straight lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "log_reg = _____________(random_state=42)\n",
        "log_reg.______(X_train_iris, y_train_iris)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate logistic regression\n",
        "y_pred_log = log_reg._______(X_test_iris)\n",
        "accuracy_log = log_reg.______(X_test_iris, y_test_iris)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_log:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Na√Øve Bayes Classification\n",
        "\n",
        "Na√Øve Bayes uses probability theory, assuming features are independent (which is often \"na√Øve\" but works well in practice)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Na√Øve Bayes\n",
        "nb = _____________()\n",
        "nb.______(X_train_iris, y_train_iris)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Na√Øve Bayes\n",
        "y_pred_nb = nb._______(X_test_iris)\n",
        "accuracy_nb = nb.______(X_test_iris, y_test_iris)\n",
        "print(f\"Na√Øve Bayes Accuracy: {accuracy_nb:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Our Models\n",
        "\n",
        "Let's see how all three algorithms performed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all three models\n",
        "models = ['k-NN', 'Logistic Regression', 'Na√Øve Bayes']\n",
        "accuracies = [accuracy_knn, accuracy_log, accuracy_nb]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, accuracies, color=['blue', 'green', 'orange'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Classification Model Comparison')\n",
        "plt.ylim(0, 1)\n",
        "for i, acc in enumerate(accuracies):\n",
        "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Guided Lab Exercise (90-115 min)\n",
        "\n",
        "**üî¨ Your Turn: Practice the Complete Workflow**\n",
        "\n",
        "Now it's time to practice what you've learned! Work through this exercise step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Wine Quality Prediction\n",
        "\n",
        "Let's work with a wine quality dataset. Your task is to predict wine quality (classification) using the same workflow we've practiced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load wine dataset\n",
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine()\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "\n",
        "print(\"Wine dataset loaded!\")\n",
        "print(f\"Features: {wine.feature_names[:5]}...\") # Show first 5\n",
        "print(f\"Classes: {wine.target_names}\")\n",
        "print(f\"Shape: {X_wine.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1:** Split the wine data into training and testing sets (use 70% for training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: YOUR CODE HERE\n",
        "# Split the wine dataset using train_test_split\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = _____________(\n",
        "    _____________, _____________, test_size=_____________, random_state=42\n",
        ")\n",
        "print(f\"Training set: {X_train_wine.shape}\")\n",
        "print(f\"Test set: {X_test_wine.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2:** Train all three classification models on the wine data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: YOUR CODE HERE\n",
        "# Create and train all three models\n",
        "wine_knn = _____________(n_neighbors=5)\n",
        "wine_log = _____________(random_state=42, max_iter=1000)\n",
        "wine_nb = _____________()\n",
        "\n",
        "# Train all models\n",
        "wine_knn.______(_____________, _____________)\n",
        "wine_log.______(_____________, _____________)\n",
        "wine_nb.______(_____________, _____________)\n",
        "print(\"All models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3:** Evaluate and compare the models' performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: YOUR CODE HERE\n",
        "# Evaluate all models and compare\n",
        "wine_accuracies = [\n",
        "    wine_knn.______(_____________, _____________),\n",
        "    wine_log.______(_____________, _____________),\n",
        "    wine_nb.______(_____________, _____________)\n",
        "]\n",
        "\n",
        "for model, acc in zip(['k-NN', 'Logistic Regression', 'Na√Øve Bayes'], wine_accuracies):\n",
        "    print(f\"{model}: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Wrap-up and Key Takeaways (115-120 min)\n",
        "\n",
        "**What We've Accomplished Today**\n",
        "\n",
        "In this session, you've successfully bridged from mathematical foundations to practical machine learning tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Concepts Mastered:\n",
        "\n",
        "1. **Universal ML Workflow**: Load ‚Üí Split ‚Üí Fit ‚Üí Predict ‚Üí Evaluate\n",
        "2. **Scikit-learn Consistency**: Same API across all algorithms\n",
        "3. **Classification vs Regression**: Discrete categories vs continuous values\n",
        "4. **Three Classification Algorithms**: k-NN, Logistic Regression, Na√Øve Bayes\n",
        "5. **Model Comparison**: How to evaluate and compare different approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps:\n",
        "\n",
        "- **Practice**: Try these techniques on your own datasets\n",
        "- **Explore**: Experiment with different parameter values (k in k-NN, etc.)\n",
        "- **Learn More**: Look into other classification algorithms (Decision Trees, Random Forest, SVM)\n",
        "- **Real Applications**: Consider how classification applies to your field of interest\n",
        "\n",
        "**Great work today! You've taken a major step from theory to practice in machine learning! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
