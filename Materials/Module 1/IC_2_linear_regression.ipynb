{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Linear Regression & Optimization (CSC 422)\n",
    "\n",
    "**Duration:** 2 hours  \n",
    "**Format:** Live coding with student participation  \n",
    "**Course:** CSC 422 - Machine and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of class, students will:\n",
    "- Understand why brute force parameter search doesn't scale to real ML\n",
    "- Implement gradient descent from scratch for linear regression\n",
    "- Visualize loss landscapes and optimization paths\n",
    "- Connect mathematical gradients to practical \"walking downhill\" intuition\n",
    "- Bridge from simple linear models to upcoming neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Timeline\n",
    "\n",
    "- **0‚Äì10 min** ‚Äî Hook: Why We Need Smart Optimization\n",
    "- **10‚Äì45 min** ‚Äî Brute Force Grid Search (The Baseline)\n",
    "- **45‚Äì60 min** ‚Äî Theory Bridge: Gradients & Intuition\n",
    "- **60‚Äì90 min** ‚Äî Gradient Descent Implementation\n",
    "- **90‚Äì110 min** ‚Äî Variants & Performance Comparison\n",
    "- **110‚Äì120 min** ‚Äî Wrap-up & Neural Network Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hook-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0‚Äì10 min: Hook - Why We Need Smart Optimization\n",
    "\n",
    "**Goal:** Show the elegant final result, then work backwards to understand how we got there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hook-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The end result we're building towards\n",
    "print(\"TODAY'S GOAL: Train a linear model y = ax + b\")\n",
    "print(\"Method 1: Try every possible (a,b) combination  ‚Üí SLOW\")\n",
    "print(\"Method 2: Use calculus to walk directly downhill ‚Üí FAST\")\n",
    "print(\"This is the foundation of ALL neural network training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-generation-header",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True relationship we're trying to discover\n",
    "a_true, b_true = 2.5, -1.0\n",
    "n_points = 100\n",
    "\n",
    "# Generate noisy linear data\n",
    "x = np.random.uniform(-2, 2, n_points)\n",
    "y = a_true * x + b_true + np.random.normal(0, 0.5, n_points)\n",
    "\n",
    "# Split into train/test\n",
    "split_idx = int(0.8 * n_points)\n",
    "x_train, y_train = x[:split_idx], y[:split_idx]\n",
    "x_test, y_test = x[split_idx:], y[split_idx:]\n",
    "\n",
    "# Quick visualization\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train, y_train, alpha=0.6, label='Train')\n",
    "plt.plot(x, a_true * x + b_true, 'r--', label=f'True: y = {a_true}x + {b_true}')\n",
    "plt.legend()\n",
    "plt.title('Training Data')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test, y_test, alpha=0.6, color='orange', label='Test')\n",
    "plt.plot(x, a_true * x + b_true, 'r--', label=f'True: y = {a_true}x + {b_true}')\n",
    "plt.legend()\n",
    "plt.title('Test Data')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Data ready: {len(x_train)} train, {len(x_test)} test points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-loss-header",
   "metadata": {},
   "source": [
    "## Define Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-loss-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(a, b, x):\n",
    "    \"\"\"Linear model: y = ax + b\"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "def mse_loss(a, b, x, y):\n",
    "    \"\"\"Mean squared error loss\"\"\"\n",
    "    predictions = predict(a, b, x)\n",
    "    residuals = y - predictions\n",
    "    return np.mean(residuals ** 2)\n",
    "\n",
    "# Test with true parameters\n",
    "true_loss = mse_loss(a_true, b_true, x_train, y_train)\n",
    "print(f\"MSE with true parameters: {true_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-1",
   "metadata": {},
   "source": [
    "**Ask students:** *\"How would you find the best values for `a` and `b`?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brute-force-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10‚Äì45 min: Brute Force Grid Search (The Baseline)\n",
    "\n",
    "**Goal:** Implement exhaustive search to understand the problem, then see why it doesn't scale\n",
    "\n",
    "## The Naive Approach: Try Everything\n",
    "\n",
    "**Concept:** Instead of being smart about optimization, let's just try every possible combination of parameters and pick the best one.\n",
    "\n",
    "Think of it like finding the lowest point in a landscape:\n",
    "- **Brute force approach:** Visit every single spot on a grid and measure the elevation\n",
    "- **Smart approach (coming later):** Use the slope to walk downhill efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive-approach-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual analogy\n",
    "print(\"üó∫Ô∏è  PARAMETER SPACE EXPLORATION:\")\n",
    "print(\"   Our goal: Find the best (a, b) values for y = ax + b\")\n",
    "print(\"   Naive method: Test every combination in a grid\")\n",
    "print(\"   - Try a = -1.0, -0.97, -0.94, ..., 4.97, 5.0\")\n",
    "print(\"   - Try b = -3.0, -2.97, -2.94, ..., 1.97, 2.0\")\n",
    "print(\"   - For each (a, b) pair: compute loss, remember the best\")\n",
    "print(\"   ‚ö†Ô∏è  This requires testing 40,401 combinations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-2",
   "metadata": {},
   "source": [
    "**Ask students:** *\"How would you find the lowest point in a mountainous area if you had unlimited time but were blindfolded to gradients?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grid-search-implementation-header",
   "metadata": {},
   "source": [
    "## Grid Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-search-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "a_grid = np.linspace(-1, 5, 201)    # 201 values for slope\n",
    "b_grid = np.linspace(-3, 2, 201)    # 201 values for intercept\n",
    "\n",
    "print(f\"Grid size: {len(a_grid)} √ó {len(b_grid)} = {len(a_grid) * len(b_grid):,} combinations\")\n",
    "\n",
    "# Compute loss for every combination (this is the expensive part!)\n",
    "A_mesh, B_mesh = np.meshgrid(a_grid, b_grid)\n",
    "loss_grid = np.zeros_like(A_mesh)\n",
    "\n",
    "for i, a in enumerate(a_grid):\n",
    "    for j, b in enumerate(b_grid):\n",
    "        loss_grid[j, i] = mse_loss(a, b, x_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Grid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-parameters-header",
   "metadata": {},
   "source": [
    "## Find Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-best-parameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum loss location\n",
    "min_idx = np.unravel_index(np.argmin(loss_grid), loss_grid.shape)\n",
    "a_best = a_grid[min_idx[1]]\n",
    "b_best = b_grid[min_idx[0]]\n",
    "best_loss = loss_grid[min_idx]\n",
    "\n",
    "print(f\"üéØ GRID SEARCH RESULTS:\")\n",
    "print(f\"   Best (a, b): ({a_best:.3f}, {b_best:.3f})\")\n",
    "print(f\"   Train loss: {best_loss:.4f}\")\n",
    "print(f\"   Test loss: {mse_loss(a_best, b_best, x_test, y_test):.4f}\")\n",
    "print(f\"   True params: ({a_true:.3f}, {b_true:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-loss-landscape-header",
   "metadata": {},
   "source": [
    "## Visualize the Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-loss-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = plt.subplot(1, 3, 1, projection='3d')\n",
    "ax1.plot_surface(A_mesh, B_mesh, loss_grid, alpha=0.7, cmap='viridis')\n",
    "ax1.scatter([a_best], [b_best], [best_loss], color='red', s=100, label='Grid Best')\n",
    "ax1.scatter([a_true], [b_true], [mse_loss(a_true, b_true, x_train, y_train)], \n",
    "           color='blue', s=100, label='True')\n",
    "ax1.set_xlabel('a (slope)')\n",
    "ax1.set_ylabel('b (intercept)')\n",
    "ax1.set_zlabel('MSE Loss')\n",
    "ax1.set_title('3D Loss Surface')\n",
    "\n",
    "# Contour plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.contour(A_mesh, B_mesh, loss_grid, levels=20, cmap='viridis')\n",
    "plt.colorbar(label='MSE Loss')\n",
    "plt.scatter(a_best, b_best, color='red', s=100, label='Grid Best', zorder=5)\n",
    "plt.scatter(a_true, b_true, color='blue', s=100, label='True', zorder=5)\n",
    "plt.xlabel('a (slope)')\n",
    "plt.ylabel('b (intercept)')\n",
    "plt.legend()\n",
    "plt.title('Loss Contours')\n",
    "\n",
    "# Final fit visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(x_train, y_train, alpha=0.6, label='Data')\n",
    "plt.plot(x, predict(a_true, b_true, x), 'b--', label=f'True: y = {a_true}x + {b_true}')\n",
    "plt.plot(x, predict(a_best, b_best, x), 'r-', label=f'Grid: y = {a_best:.2f}x + {b_best:.2f}')\n",
    "plt.legend()\n",
    "plt.title('Model Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini-exercise-1-header",
   "metadata": {},
   "source": [
    "## Mini Exercise (5 minutes)\n",
    "\n",
    "**Challenge:** What happens if we make the grid coarser or finer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini-exercise-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student TODO: Try different grid resolutions and see the effect\n",
    "# Coarse grid: 51 x 51 points\n",
    "# Fine grid: 401 x 401 points\n",
    "# Which is better? What's the tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-3",
   "metadata": {},
   "source": [
    "**Ask students:** *\"Why can't we use this approach for a neural network with 1 million parameters?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-bridge-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 45‚Äì60 min: Theory Bridge - Gradients & Intuition\n",
    "\n",
    "**Goal:** Connect mathematical gradients to the intuitive idea of \"walking downhill\"\n",
    "\n",
    "## The Mathematical Foundation\n",
    "\n",
    "For linear regression with loss function:\n",
    "$$L(a, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (ax_i + b))^2$$\n",
    "\n",
    "The gradients are:\n",
    "$$\\frac{\\partial L}{\\partial a} = \\frac{2}{n} \\sum_{i=1}^{n} (ax_i + b - y_i) x_i$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (ax_i + b - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intuitive-explanation-header",
   "metadata": {},
   "source": [
    "## The Intuitive Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-intuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient intuition\n",
    "def plot_gradient_intuition():\n",
    "    # Take a slice of the loss surface at b = b_true\n",
    "    a_slice = np.linspace(0, 4, 100)\n",
    "    loss_slice = [mse_loss(a, b_true, x_train, y_train) for a in a_slice]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plot loss curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(a_slice, loss_slice, 'b-', linewidth=2, label='Loss vs. slope (a)')\n",
    "    plt.axvline(a_true, color='green', linestyle='--', label=f'True a = {a_true}')\n",
    "    plt.xlabel('Slope (a)')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Landscape Slice')\n",
    "    \n",
    "    # Show gradient direction\n",
    "    test_points = [1.0, 2.5, 3.5]\n",
    "    colors = ['red', 'orange', 'purple']\n",
    "    \n",
    "    for i, a_test in enumerate(test_points):\n",
    "        loss_val = mse_loss(a_test, b_true, x_train, y_train)\n",
    "        # Approximate gradient with finite difference\n",
    "        eps = 0.01\n",
    "        grad = (mse_loss(a_test + eps, b_true, x_train, y_train) - loss_val) / eps\n",
    "        \n",
    "        plt.scatter(a_test, loss_val, color=colors[i], s=100, zorder=5)\n",
    "        # Draw arrow showing gradient direction (opposite to gradient for descent)\n",
    "        arrow_len = 0.3\n",
    "        arrow_dx = -arrow_len * np.sign(grad)\n",
    "        plt.arrow(a_test, loss_val, arrow_dx, 0, head_width=5, \n",
    "                 head_length=0.05, fc=colors[i], ec=colors[i])\n",
    "        plt.text(a_test, loss_val + 10, f'‚àáL = {grad:.1f}', \n",
    "                ha='center', color=colors[i], fontweight='bold')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show what \"walking downhill\" means\n",
    "    x_demo = np.linspace(-2, 2, 50)\n",
    "    for i, a_test in enumerate(test_points):\n",
    "        y_pred = predict(a_test, b_true, x_demo)\n",
    "        plt.plot(x_demo, y_pred, color=colors[i], alpha=0.7, \n",
    "                label=f'a = {a_test}, loss = {mse_loss(a_test, b_true, x_train, y_train):.1f}')\n",
    "    \n",
    "    plt.scatter(x_train, y_train, alpha=0.5, color='gray', label='Data')\n",
    "    plt.plot(x_demo, predict(a_true, b_true, x_demo), 'g--', linewidth=2, label='True line')\n",
    "    plt.legend()\n",
    "    plt.title('Different Slopes & Their Fit')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gradient_intuition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-insight",
   "metadata": {},
   "source": [
    "**Key insight:** *The gradient tells us which direction makes the loss worse. So we step in the opposite direction!*\n",
    "\n",
    "**Ask students:** *\"If you were standing on a hill in fog, how would you find the bottom?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-descent-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 60‚Äì90 min: Gradient Descent Implementation\n",
    "\n",
    "**Goal:** Code the gradient descent algorithm from scratch and watch it converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorized-gradients-header",
   "metadata": {},
   "source": [
    "## Vectorized Gradient Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(a, b, x, y):\n",
    "    \"\"\"Compute gradients of MSE loss with respect to a and b\"\"\"\n",
    "    n = len(x)\n",
    "    predictions = predict(a, b, x)\n",
    "    residuals = predictions - y\n",
    "    \n",
    "    grad_a = (2.0 / n) * np.sum(residuals * x)\n",
    "    grad_b = (2.0 / n) * np.sum(residuals)\n",
    "    \n",
    "    return grad_a, grad_b\n",
    "\n",
    "# Test gradient computation\n",
    "grad_a, grad_b = compute_gradients(1.0, 0.0, x_train, y_train)\n",
    "print(f\"Gradients at (a=1, b=0): grad_a = {grad_a:.3f}, grad_b = {grad_b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-descent-training-header",
   "metadata": {},
   "source": [
    "## Gradient Descent Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-descent-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_descent(x, y, learning_rate=0.1, max_steps=1000, tolerance=1e-6):\n",
    "    \"\"\"Train linear regression using gradient descent\"\"\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    a, b = 0.0, 0.0\n",
    "    history = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Compute current loss and gradients\n",
    "        loss = mse_loss(a, b, x, y)\n",
    "        grad_a, grad_b = compute_gradients(a, b, x, y)\n",
    "        \n",
    "        # Store history\n",
    "        history.append({'step': step, 'a': a, 'b': b, 'loss': loss, \n",
    "                       'grad_a': grad_a, 'grad_b': grad_b})\n",
    "        \n",
    "        # Check convergence\n",
    "        if abs(grad_a) < tolerance and abs(grad_b) < tolerance:\n",
    "            print(f\"‚úÖ Converged at step {step}\")\n",
    "            break\n",
    "        \n",
    "        # Update parameters (gradient descent step)\n",
    "        a = a - learning_rate * grad_a\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "        # Print progress\n",
    "        if step % 100 == 0 or step < 10:\n",
    "            print(f\"Step {step:3d}: a = {a:6.3f}, b = {b:6.3f}, loss = {loss:.6f}\")\n",
    "    \n",
    "    return a, b, history\n",
    "\n",
    "# Train the model\n",
    "print(\"üöÄ Training with Gradient Descent:\")\n",
    "a_gd, b_gd, history = train_gradient_descent(x_train, y_train, learning_rate=0.1)\n",
    "\n",
    "print(f\"\\nüéØ GRADIENT DESCENT RESULTS:\")\n",
    "print(f\"   Final (a, b): ({a_gd:.3f}, {b_gd:.3f})\")\n",
    "print(f\"   Train loss: {mse_loss(a_gd, b_gd, x_train, y_train):.6f}\")\n",
    "print(f\"   Test loss: {mse_loss(a_gd, b_gd, x_test, y_test):.6f}\")\n",
    "print(f\"   Grid search: ({a_best:.3f}, {b_best:.3f})\")\n",
    "print(f\"   True params: ({a_true:.3f}, {b_true:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-training-progress-header",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract history for plotting\n",
    "history_array = np.array([(h['step'], h['a'], h['b'], h['loss']) for h in history])\n",
    "steps, a_path, b_path, loss_path = history_array.T\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Parameter convergence\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(steps, a_path, 'b-', label='a (slope)', linewidth=2)\n",
    "plt.plot(steps, b_path, 'r-', label='b (intercept)', linewidth=2)\n",
    "plt.axhline(a_true, color='blue', linestyle='--', alpha=0.7, label=f'True a = {a_true}')\n",
    "plt.axhline(b_true, color='red', linestyle='--', alpha=0.7, label=f'True b = {b_true}')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.legend()\n",
    "plt.title('Parameter Convergence')\n",
    "\n",
    "# Loss convergence\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(steps, loss_path, 'g-', linewidth=2)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Loss Convergence')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Optimization path on loss contours\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.contour(A_mesh, B_mesh, loss_grid, levels=20, alpha=0.7, cmap='viridis')\n",
    "plt.plot(a_path, b_path, 'ro-', markersize=3, linewidth=2, label='GD Path')\n",
    "plt.scatter(a_path[0], b_path[0], color='green', s=100, label='Start', zorder=5)\n",
    "plt.scatter(a_path[-1], b_path[-1], color='red', s=100, label='End', zorder=5)\n",
    "plt.scatter(a_true, b_true, color='blue', s=100, label='True', zorder=5)\n",
    "plt.xlabel('a (slope)')\n",
    "plt.ylabel('b (intercept)')\n",
    "plt.legend()\n",
    "plt.title('Optimization Path')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-rate-experiments-header",
   "metadata": {},
   "source": [
    "## Learning Rate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning-rate-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "results = {}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"\\nüìä Testing learning rate = {lr}\")\n",
    "    try:\n",
    "        a_test, b_test, hist_test = train_gradient_descent(x_train, y_train, \n",
    "                                                          learning_rate=lr, max_steps=200)\n",
    "        results[lr] = hist_test\n",
    "        \n",
    "        # Plot convergence\n",
    "        loss_history = [h['loss'] for h in hist_test]\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(loss_history, linewidth=2)\n",
    "        plt.title(f'Learning Rate = {lr}')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        final_loss = loss_history[-1]\n",
    "        print(f\"   Final loss: {final_loss:.6f} ({len(hist_test)} steps)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-4",
   "metadata": {},
   "source": [
    "**Ask students:** *\"What happens with learning rate too high? Too low? What's the sweet spot?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variants-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 90‚Äì110 min: Variants & Performance Comparison\n",
    "\n",
    "**Goal:** Show mini-batch SGD and compare all approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini-batch-sgd-header",
   "metadata": {},
   "source": [
    "## Mini-Batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini-batch-sgd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mini_batch_sgd(x, y, batch_size=16, learning_rate=0.1, max_steps=500):\n",
    "    \"\"\"Train using mini-batch stochastic gradient descent\"\"\"\n",
    "    \n",
    "    n = len(x)\n",
    "    a, b = 0.0, 0.0\n",
    "    history = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Randomly sample a mini-batch\n",
    "        batch_indices = np.random.choice(n, size=batch_size, replace=False)\n",
    "        x_batch = x[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        # Compute gradients on mini-batch\n",
    "        grad_a, grad_b = compute_gradients(a, b, x_batch, y_batch)\n",
    "        \n",
    "        # Update parameters\n",
    "        a = a - learning_rate * grad_a\n",
    "        b = b - learning_rate * grad_b\n",
    "        \n",
    "        # Record full dataset loss (for comparison)\n",
    "        if step % 50 == 0:\n",
    "            full_loss = mse_loss(a, b, x, y)\n",
    "            history.append({'step': step, 'a': a, 'b': b, 'loss': full_loss})\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step:3d}: a = {a:6.3f}, b = {b:6.3f}, loss = {full_loss:.6f}\")\n",
    "    \n",
    "    return a, b, history\n",
    "\n",
    "# Train with SGD\n",
    "print(\"üîÑ Training with Mini-Batch SGD:\")\n",
    "a_sgd, b_sgd, sgd_history = train_mini_batch_sgd(x_train, y_train, batch_size=16)\n",
    "\n",
    "print(f\"\\nüéØ SGD RESULTS:\")\n",
    "print(f\"   Final (a, b): ({a_sgd:.3f}, {b_sgd:.3f})\")\n",
    "print(f\"   Train loss: {mse_loss(a_sgd, b_sgd, x_train, y_train):.6f}\")\n",
    "print(f\"   Test loss: {mse_loss(a_sgd, b_sgd, x_test, y_test):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-comparison-header",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "methods = {\n",
    "    'True Parameters': (a_true, b_true),\n",
    "    'Grid Search': (a_best, b_best),\n",
    "    'Gradient Descent': (a_gd, b_gd),\n",
    "    'Mini-Batch SGD': (a_sgd, b_sgd)\n",
    "}\n",
    "\n",
    "print(\"üìä FINAL COMPARISON:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Method':<20} {'a':<8} {'b':<8} {'Train Loss':<12} {'Test Loss':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, (a, b) in methods.items():\n",
    "    train_loss = mse_loss(a, b, x_train, y_train)\n",
    "    test_loss = mse_loss(a, b, x_test, y_test)\n",
    "    print(f\"{name:<20} {a:8.3f} {b:8.3f} {train_loss:12.6f} {test_loss:12.6f}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visual-comparison-header",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visual-comparison",
   "metadata": {},
   "outputs": [],
   "source": "plt.figure(figsize=(20, 5))\n\n# All regression lines\nplt.subplot(1, 4, 1)\nx_line = np.linspace(-2, 2, 100)\nplt.scatter(x_train, y_train, alpha=0.6, color='gray', label='Training Data')\n\ncolors = ['blue', 'red', 'green', 'orange']\nfor (name, (a, b)), color in zip(methods.items(), colors):\n    y_line = predict(a, b, x_line)\n    plt.plot(x_line, y_line, color=color, linewidth=2, label=f'{name}')\n\nplt.legend()\nplt.title('All Model Fits')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Parameter space comparison\nplt.subplot(1, 4, 2)\nplt.contour(A_mesh, B_mesh, loss_grid, levels=15, alpha=0.7, cmap='viridis')\nfor (name, (a, b)), color in zip(methods.items(), colors):\n    plt.scatter(a, b, color=color, s=100, label=name, zorder=5)\nplt.xlabel('a (slope)')\nplt.ylabel('b (intercept)')\nplt.legend()\nplt.title('Parameter Space')\n\n# Loss comparison\nplt.subplot(1, 4, 3)\nmethod_names = list(methods.keys())[1:]  # Skip true parameters\ntrain_losses = [mse_loss(methods[name][0], methods[name][1], x_train, y_train) \n                for name in method_names]\ntest_losses = [mse_loss(methods[name][0], methods[name][1], x_test, y_test) \n               for name in method_names]\n\nx_pos = np.arange(len(method_names))\nplt.bar(x_pos - 0.2, train_losses, 0.4, label='Train Loss', alpha=0.8)\nplt.bar(x_pos + 0.2, test_losses, 0.4, label='Test Loss', alpha=0.8)\nplt.xticks(x_pos, method_names, rotation=45)\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.title('Loss Comparison')\n\n# Convergence speed comparison (NEW!)\nplt.subplot(1, 4, 4)\nconvergence_data = {\n    'Grid Search': len(a_grid) * len(b_grid),  # Total evaluations\n    'Gradient Descent': len(history),  # Steps to converge\n    'Mini-Batch SGD': len(sgd_history) * 10  # Multiply by 10 to show relative cost\n}\n\nmethod_names_conv = list(convergence_data.keys())\nsteps = list(convergence_data.values())\ncolors_conv = ['red', 'green', 'orange']\n\nbars = plt.bar(method_names_conv, steps, color=colors_conv, alpha=0.8)\nplt.ylabel('Computational Steps')\nplt.title('Convergence Speed\\n(Lower = Faster)')\nplt.yscale('log')\n\n# Add value labels on bars\nfor bar, value in zip(bars, steps):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height,\n             f'{value:,}', ha='center', va='bottom', fontsize=10)\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Print convergence analysis\nprint(\"\\n‚è±Ô∏è CONVERGENCE ANALYSIS:\")\nprint(\"-\" * 50)\nprint(f\"Grid Search:      {len(a_grid) * len(b_grid):,} loss evaluations\")\nprint(f\"Gradient Descent: {len(history):,} iterations to converge\")\nprint(f\"Mini-Batch SGD:   {len(sgd_history):,} iterations (smaller batches)\")\nprint(\"-\" * 50)\nprint(f\"Speedup of GD vs Grid: {(len(a_grid) * len(b_grid)) / len(history):.0f}x faster!\")"
  },
  {
   "cell_type": "markdown",
   "id": "wrap-up-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 110‚Äì120 min: Wrap-up & Neural Network Connections\n",
    "\n",
    "**Goal:** Connect today's concepts to upcoming deep learning topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checklist-header",
   "metadata": {},
   "source": [
    "## Checklist - What You've Mastered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skills-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TODAY'S OPTIMIZATION SKILLS ‚úÖ ===\")\n",
    "print(\"‚úì Brute force search - understand the baseline approach\")\n",
    "print(\"‚úì Loss landscapes - visualize optimization as walking downhill\")  \n",
    "print(\"‚úì Gradient computation - mathematical foundation of all ML\")\n",
    "print(\"‚úì Gradient descent - the core algorithm behind neural networks\")\n",
    "print(\"‚úì Learning rates - the most important hyperparameter to tune\")\n",
    "print(\"‚úì Mini-batch SGD - bridge to deep learning optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network-connections-header",
   "metadata": {},
   "source": [
    "## Connect Forward to Neural Networks\n",
    "\n",
    "**Next week's preview:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-network-connections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we learned today scales directly to neural networks!\n",
    "print(\"\\nüß† NEURAL NETWORK CONNECTION:\")\n",
    "print(\"üìà Linear regression:     y = ax + b\")\n",
    "print(\"üîó Neural network:        y = œÉ(W‚ÇÇœÉ(W‚ÇÅx + b‚ÇÅ) + b‚ÇÇ)\")\n",
    "print(\"‚ö° Same optimization:      Gradient descent on W‚ÇÅ, W‚ÇÇ, b‚ÇÅ, b‚ÇÇ\")\n",
    "print(\"üìä Same loss functions:   MSE, cross-entropy, etc.\")\n",
    "print(\"üéØ Same goal:             Find parameters that minimize loss\")\n",
    "print(\"\\nThe only difference? More parameters and more complex gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-insights",
   "metadata": {},
   "source": [
    "## Key Insights for Deep Learning\n",
    "\n",
    "- **Scalability:** Grid search ‚Üí impossible, Gradient descent ‚Üí essential\n",
    "- **Loss landscapes:** Neural networks have the same \"hills and valleys\"\n",
    "- **Learning rates:** Even more critical with millions of parameters\n",
    "- **Mini-batches:** Standard practice for large datasets\n",
    "- **Convergence:** Same principles, just more complex optimization\n",
    "\n",
    "## Quick Quiz (2 minutes)\n",
    "\n",
    "**Challenge questions for students:**\n",
    "1. Why does gradient descent scale to millions of parameters but grid search doesn't?\n",
    "2. What happens if the learning rate is too high?\n",
    "3. How does the gradient tell us which direction to move?\n",
    "\n",
    "## Optional Advanced Topics\n",
    "\n",
    "For students who want to explore further:\n",
    "- **Momentum and Adam optimizers** - Advanced versions of gradient descent\n",
    "- **Automatic differentiation** - How frameworks like PyTorch compute gradients\n",
    "- **Second-order methods** - Newton's method and beyond\n",
    "\n",
    "**Remember:** *Today's linear regression is tomorrow's neural network foundation!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}