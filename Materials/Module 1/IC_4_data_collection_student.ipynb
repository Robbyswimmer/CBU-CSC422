{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Quality Assessment (CSC 422)\n",
    "\n",
    "**Duration:** 45 minutes  \n",
    "**Format:** Live coding + group exercise  \n",
    "**Course:** CSC 422 - Machine and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of class, students will:\n",
    "- Navigate Kaggle effectively to discover relevant datasets\n",
    "- Use the HuggingFace datasets library to programmatically load data\n",
    "- Assess basic dataset quality metrics for ML projects\n",
    "- Apply dataset validation techniques hands-on\n",
    "- Select appropriate datasets for their own ML projects\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱Timeline\n",
    "\n",
    "- **0–10 min** — Hook: Dataset Quality + Kaggle Review\n",
    "- **10–25 min** — HuggingFace Datasets Library Hands-on\n",
    "- **25–45 min** — Group Exercise: Find, Load & Explore\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need the HuggingFace datasets library plus our standard data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets library if needed\n",
    "# !pip install datasets\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# HuggingFace datasets\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0–10 min: Hook - Why Dataset Quality Matters + Kaggle Review\n",
    "\n",
    "**Goal:** Understand the foundation of successful ML projects and review Kaggle as a data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ML Truth: \"Garbage In, Garbage Out\"\n",
    "\n",
    "The most sophisticated algorithm in the world won't save you from bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 THE DATA QUALITY IMPERATIVE:\")\n",
    "print(\"📊 80% of ML project time = Data collection & cleaning\")\n",
    "print(\"🤖 20% of ML project time = Model building & tuning\")\n",
    "print(\"\")\n",
    "print(\"⚠️  COMMON DATA PROBLEMS:\")\n",
    "print(\"   • Missing values and inconsistent formats\")\n",
    "print(\"   • Biased or unrepresentative samples\")\n",
    "print(\"   • Insufficient data for reliable training\")\n",
    "print(\"   • Mislabeled or noisy target variables\")\n",
    "print(\"   • Data leakage (future info in features)\")\n",
    "print(\"\")\n",
    "print(\"✅ TODAY'S GOAL: Learn to find and validate quality datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: Your Data Goldmine 🏆\n",
    "\n",
    "**What is Kaggle?**\n",
    "- World's largest data science community\n",
    "- 50,000+ public datasets across all domains\n",
    "- Competitions, kernels, and discussions\n",
    "- Real-world, industry-contributed data\n",
    "\n",
    "**Key Navigation Tips:**\n",
    "1. **Search by topic**: \"healthcare\", \"finance\", \"nlp\", \"computer vision\"\n",
    "2. **Filter by size**: Start with medium datasets (1K-100K rows)\n",
    "3. **Check usability score**: Look for 8.0+ ratings\n",
    "4. **Read the description**: Understand data collection methodology\n",
    "5. **Examine data card**: Check for missing values, data types\n",
    "\n",
    "**Quality Indicators to Look For:**\n",
    "- ✅ Clear documentation and data dictionary\n",
    "- ✅ Reasonable data size (not too small, not too massive)\n",
    "- ✅ Recent updates and community engagement\n",
    "- ✅ Proper licensing (usually CC0 or Open Database)\n",
    "- ✅ Low percentage of missing values\n",
    "- ✅ Balanced classes (for classification problems)\n",
    "\n",
    "**Red Flags to Avoid:**\n",
    "- ❌ No documentation or unclear data sources\n",
    "- ❌ Extremely imbalanced datasets (99% one class)\n",
    "- ❌ Too many missing values (>20% generally)\n",
    "- ❌ Suspicious perfect correlations\n",
    "- ❌ Data that seems \"too good to be true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10–25 min: HuggingFace Datasets Library Hands-on\n",
    "\n",
    "**Goal:** Learn to programmatically load and explore datasets using the HuggingFace datasets library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is HuggingFace Datasets?\n",
    "\n",
    "- **Unified interface** for loading 1000+ datasets\n",
    "- **Automatic caching** - download once, use forever\n",
    "- **Memory efficient** - works with datasets larger than RAM\n",
    "- **Consistent API** - same interface for all datasets\n",
    "- **Built-in preprocessing** - tokenization, feature extraction\n",
    "\n",
    "Think of it as \"scikit-learn for dataset loading\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Available Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See some available datasets (there are 1000+!)\n",
    "available_datasets = list_datasets()\n",
    "print(f\"Total available datasets: {len(available_datasets)}\")\n",
    "print(\"\\nFirst 10 datasets:\")\n",
    "for i, dataset_name in enumerate(available_datasets[:10]):\n",
    "    print(f\"  {i+1}. {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for datasets containing specific keywords\n",
    "ml_datasets = [name for name in available_datasets if 'classification' in name.lower()]\n",
    "print(f\"Datasets with 'classification': {len(ml_datasets)}\")\n",
    "print(\"Examples:\", ml_datasets[:5])\n",
    "\n",
    "text_datasets = [name for name in available_datasets if any(keyword in name.lower() \n",
    "                for keyword in ['text', 'sentiment', 'review'])]\n",
    "print(f\"\\nText-related datasets: {len(text_datasets)}\")\n",
    "print(\"Examples:\", text_datasets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your First Dataset\n",
    "\n",
    "Let's start with a classic: the Wine Quality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine quality dataset\n",
    "dataset = load_dataset(\"wine_quality\")\n",
    "print(\"✅ Dataset loaded!\")\n",
    "print(f\"Dataset object type: {type(dataset)}\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training split\n",
    "train_data = dataset['train']\n",
    "print(f\"Training data size: {len(train_data)} examples\")\n",
    "print(f\"Features: {train_data.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first few examples\n",
    "print(\"First 3 examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}: {train_data[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Pandas for Analysis\n",
    "\n",
    "HuggingFace datasets play nicely with pandas for familiar data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = train_data.to_pandas()\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Dataset Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"📊 DATASET QUALITY REPORT:\")\n",
    "print(f\"   • Size: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"   • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"   • Missing values: {df.isnull().sum().sum()} total\")\n",
    "print(f\"   • Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\n📈 DATA TYPES:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "print(\"🔍 MISSING VALUES BY COLUMN:\")\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Percentage': missing_percent\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "else:\n",
    "    print(\"✅ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis (assuming 'quality' is the target)\n",
    "if 'quality' in df.columns:\n",
    "    print(\"🎯 TARGET VARIABLE ANALYSIS:\")\n",
    "    print(f\"   • Unique values: {df['quality'].nunique()}\")\n",
    "    print(f\"   • Value range: {df['quality'].min()} to {df['quality'].max()}\")\n",
    "    print(\"\\n   • Distribution:\")\n",
    "    print(df['quality'].value_counts().sort_index())\n",
    "    \n",
    "    # Quick visualization\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['quality'].hist(bins=10, alpha=0.7)\n",
    "    plt.title('Target Distribution')\n",
    "    plt.xlabel('Quality Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df.boxplot(column='quality')\n",
    "    plt.title('Target Box Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Template\n",
    "\n",
    "Here's a reusable template for loading and quickly assessing any dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_dataset_assessment(dataset_name, split='train'):\n",
    "    \"\"\"\n",
    "    Load a HuggingFace dataset and perform quick quality assessment\n",
    "    \"\"\"\n",
    "    print(f\"📦 Loading dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        data = dataset[split]\n",
    "        df = data.to_pandas()\n",
    "        \n",
    "        print(f\"✅ Dataset loaded successfully!\")\n",
    "        print(f\"   • Shape: {df.shape}\")\n",
    "        print(f\"   • Columns: {list(df.columns)}\")\n",
    "        print(f\"   • Missing values: {df.isnull().sum().sum()}\")\n",
    "        print(f\"   • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing our assessment function:\")\n",
    "test_df = quick_dataset_assessment(\"wine_quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 25–45 min: Group Exercise - Find, Load & Explore\n",
    "\n",
    "**Goal:** Apply your new skills in small groups to discover and assess datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Your Mission: Dataset Discovery & Assessment\n",
    "\n",
    "**Time:** 20 minutes  \n",
    "**Format:** Groups of 2-3 students  \n",
    "**Deliverable:** Brief presentation of your findings  \n",
    "\n",
    "### Step 1: Choose Your Dataset (5 minutes)\n",
    "\n",
    "**Option A: Pick from our curated list**\n",
    "- `\"imdb\"` - Movie reviews (text classification)\n",
    "- `\"california_housing\"` - Housing prices (regression)\n",
    "- `\"breast_cancer\"` - Cancer diagnosis (classification)\n",
    "- `\"diabetes\"` - Diabetes progression (regression)\n",
    "- `\"fashion_mnist\"` - Clothing images (image classification)\n",
    "- `\"amazon_reviews_multi\"` - Product reviews (text classification)\n",
    "\n",
    "**Option B: Find your own**\n",
    "- Browse the HuggingFace Hub: https://huggingface.co/datasets\n",
    "- Search for datasets in your domain of interest\n",
    "- Pick something that looks interesting!\n",
    "\n",
    "### Step 2: Load & Explore (10 minutes)\n",
    "\n",
    "Use the cells below to:\n",
    "1. Load your chosen dataset\n",
    "2. Explore its structure and content\n",
    "3. Assess its quality using our techniques\n",
    "4. Identify potential ML applications\n",
    "\n",
    "### Step 3: Prepare Your Report (3 minutes)\n",
    "\n",
    "Be ready to share:\n",
    "- **Dataset name and domain**\n",
    "- **Size and structure** (rows, columns, data types)\n",
    "- **Quality assessment** (missing values, balance, etc.)\n",
    "- **Potential ML problem** (classification, regression, clustering)\n",
    "- **One interesting finding** from your exploration\n",
    "\n",
    "### Step 4: Group Presentations (2 minutes each)\n",
    "\n",
    "Quick 1-2 minute presentations from each group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Your Group's Workspace\n",
    "\n",
    "**Group Members:** _[Write your names here]_  \n",
    "**Dataset Choice:** _[Write dataset name here]_  \n",
    "**Domain:** _[e.g., healthcare, NLP, computer vision, etc.]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your chosen dataset\n",
    "# Replace \"your_dataset_name\" with your actual choice\n",
    "\n",
    "dataset_name = \"_____________\"  # Fill in your dataset name\n",
    "print(f\"Loading dataset: {dataset_name}\")\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# dataset = load_dataset(dataset_name)\n",
    "# data = dataset['train']  # or appropriate split\n",
    "# df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Basic exploration\n",
    "# YOUR CODE HERE:\n",
    "# Explore shape, columns, data types, first few rows\n",
    "\n",
    "print(\"📊 BASIC INFO:\")\n",
    "# print(f\"Shape: {df.shape}\")\n",
    "# print(f\"Columns: {list(df.columns)}\")\n",
    "# print(f\"Data types: {df.dtypes.value_counts()}\")\n",
    "\n",
    "# Show first few examples\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Quality assessment\n",
    "# YOUR CODE HERE:\n",
    "# Check for missing values, duplicates, target distribution\n",
    "\n",
    "print(\"🔍 QUALITY ASSESSMENT:\")\n",
    "# missing_values = df.isnull().sum()\n",
    "# print(f\"Missing values: {missing_values.sum()}\")\n",
    "# print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# If you have a target column, analyze its distribution\n",
    "# target_column = \"_____\"  # Fill in if applicable\n",
    "# df[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a visualization (optional but recommended)\n",
    "# YOUR CODE HERE:\n",
    "# Create a plot that shows something interesting about your data\n",
    "\n",
    "# Examples:\n",
    "# - Histogram of target variable\n",
    "# - Correlation heatmap\n",
    "# - Sample images (if image dataset)\n",
    "# - Word cloud (if text dataset)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Your visualization code here\n",
    "plt.title(\"Your Dataset Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Your Group's Findings\n",
    "\n",
    "**Complete this summary for your presentation:**\n",
    "\n",
    "**Dataset:** _[Name and brief description]_\n",
    "\n",
    "**Size & Structure:**\n",
    "- Rows: _[number]_\n",
    "- Columns: _[number]_\n",
    "- Data types: _[mix of numeric/text/categorical]_\n",
    "\n",
    "**Quality Assessment:**\n",
    "- Missing values: _[percentage or count]_\n",
    "- Data quality: _[Good/Fair/Poor and why]_\n",
    "- Balance: _[For classification: balanced/imbalanced]_\n",
    "\n",
    "**ML Application:**\n",
    "- Problem type: _[Classification/Regression/Other]_\n",
    "- Potential use case: _[What could you predict/classify?]_\n",
    "\n",
    "**Most Interesting Finding:**\n",
    "_[What surprised you or seemed most noteworthy?]_\n",
    "\n",
    "**Recommendation:**\n",
    "- Would you use this dataset for a project? _[Yes/No and why]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Wrap-Up: What You've Accomplished\n",
    "\n",
    "In just 45 minutes, you've learned to:\n",
    "\n",
    "✅ **Navigate dataset resources** - Kaggle and HuggingFace Hub  \n",
    "✅ **Load datasets programmatically** - Using the datasets library  \n",
    "✅ **Assess data quality** - Missing values, distribution, balance  \n",
    "✅ **Apply skills immediately** - Hands-on group exploration  \n",
    "✅ **Share findings** - Communicate dataset insights effectively  \n",
    "\n",
    "### 🚀 Next Steps for Your Projects:\n",
    "\n",
    "1. **Explore more datasets** - Browse HuggingFace Hub and Kaggle\n",
    "2. **Practice quality assessment** - Use our template on different data\n",
    "3. **Consider ethical implications** - Data bias, privacy, licensing\n",
    "4. **Document your choices** - Keep notes on why you selected specific datasets\n",
    "\n",
    "### 🔗 Useful Resources:\n",
    "\n",
    "- **HuggingFace Datasets Hub:** https://huggingface.co/datasets\n",
    "- **Kaggle Datasets:** https://www.kaggle.com/datasets\n",
    "- **Dataset Documentation:** Always read the dataset cards!\n",
    "- **Community Discussions:** Check comments and kernels for insights\n",
    "\n",
    "**Remember:** The quality of your data will largely determine the success of your ML project. Take time to choose and validate your datasets carefully!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}