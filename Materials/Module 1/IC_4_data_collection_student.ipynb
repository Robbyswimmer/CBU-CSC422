{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Quality Assessment (CSC 422)\n",
    "\n",
    "**Duration:** 45 minutes  \n",
    "**Format:** Live coding + group exercise  \n",
    "**Course:** CSC 422 - Machine and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of class, students will:\n",
    "- Navigate Kaggle effectively to discover relevant datasets\n",
    "- Use the HuggingFace datasets library to programmatically load data\n",
    "- Assess basic dataset quality metrics for ML projects\n",
    "- Apply dataset validation techniques hands-on\n",
    "- Select appropriate datasets for their own ML projects\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Timeline\n",
    "\n",
    "- **0‚Äì10 min** ‚Äî Hook: Dataset Quality + Kaggle Review\n",
    "- **10‚Äì25 min** ‚Äî HuggingFace Datasets Library Hands-on\n",
    "- **25‚Äì45 min** ‚Äî Group Exercise: Find, Load & Explore\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need the HuggingFace datasets library plus our standard data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets library if needed\n",
    "# !pip install datasets\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0‚Äì10 min: Hook - Why Dataset Quality Matters + Kaggle Review\n",
    "\n",
    "**Goal:** Understand the foundation of successful ML projects and review Kaggle as a data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ML Truth: \"Garbage In, Garbage Out\"\n",
    "\n",
    "The most sophisticated algorithm in the world won't save you from bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ THE DATA QUALITY IMPERATIVE:\")\n",
    "print(\"üìä 80% of ML project time = Data collection & cleaning\")\n",
    "print(\"ü§ñ 20% of ML project time = Model building & tuning\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è  COMMON DATA PROBLEMS:\")\n",
    "print(\"   ‚Ä¢ Missing values and inconsistent formats\")\n",
    "print(\"   ‚Ä¢ Biased or unrepresentative samples\")\n",
    "print(\"   ‚Ä¢ Insufficient data for reliable training\")\n",
    "print(\"   ‚Ä¢ Mislabeled or noisy target variables\")\n",
    "print(\"   ‚Ä¢ Data leakage (future info in features)\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ TODAY'S GOAL: Learn to find and validate quality datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: Your Data Goldmine üèÜ\n",
    "\n",
    "**What is Kaggle?**\n",
    "- World's largest data science community\n",
    "- 50,000+ public datasets across all domains\n",
    "- Competitions, kernels, and discussions\n",
    "- Real-world, industry-contributed data\n",
    "\n",
    "**Key Navigation Tips:**\n",
    "1. **Search by topic**: \"healthcare\", \"finance\", \"nlp\", \"computer vision\"\n",
    "2. **Filter by size**: Start with medium datasets (1K-100K rows)\n",
    "3. **Check usability score**: Look for 8.0+ ratings\n",
    "4. **Read the description**: Understand data collection methodology\n",
    "5. **Examine data card**: Check for missing values, data types\n",
    "\n",
    "**Quality Indicators to Look For:**\n",
    "- ‚úÖ Clear documentation and data dictionary\n",
    "- ‚úÖ Reasonable data size (not too small, not too massive)\n",
    "- ‚úÖ Recent updates and community engagement\n",
    "- ‚úÖ Proper licensing (usually CC0 or Open Database)\n",
    "- ‚úÖ Low percentage of missing values\n",
    "- ‚úÖ Balanced classes (for classification problems)\n",
    "\n",
    "**Red Flags to Avoid:**\n",
    "- ‚ùå No documentation or unclear data sources\n",
    "- ‚ùå Extremely imbalanced datasets (99% one class)\n",
    "- ‚ùå Too many missing values (>20% generally)\n",
    "- ‚ùå Suspicious perfect correlations\n",
    "- ‚ùå Data that seems \"too good to be true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10‚Äì25 min: HuggingFace Datasets Library Hands-on\n",
    "\n",
    "**Goal:** Learn to programmatically load and explore datasets using the HuggingFace datasets library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is HuggingFace Datasets?\n",
    "\n",
    "- **Unified interface** for loading 1000+ datasets\n",
    "- **Automatic caching** - download once, use forever\n",
    "- **Memory efficient** - works with datasets larger than RAM\n",
    "- **Consistent API** - same interface for all datasets\n",
    "- **Built-in preprocessing** - tokenization, feature extraction\n",
    "\n",
    "Think of it as \"scikit-learn for dataset loading\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Available Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since list_datasets() is deprecated, let's explore some popular datasets\n",
    "# Here are some well-known datasets available on HuggingFace:\n",
    "\n",
    "popular_datasets = [\n",
    "    \"imdb\", \"amazon_reviews_multi\", \"yelp_review_full\", \"squad\",\n",
    "    \"glue\", \"super_glue\", \"conll2003\", \"mnist\", \"fashion_mnist\", \n",
    "    \"cifar10\", \"cifar100\", \"food101\", \"beans\", \"common_voice\"\n",
    "]\n",
    "\n",
    "print(f\"Popular datasets examples ({len(popular_datasets)} shown):\")\n",
    "for i, dataset_name in enumerate(popular_datasets[:10]):\n",
    "    print(f\"  {i+1}. {dataset_name}\")\n",
    "    \n",
    "print(f\"\\nFor more datasets, visit: https://huggingface.co/datasets\")\n",
    "print(f\"üí° TIP: Look for datasets with many downloads and recent activity!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for datasets by domain/topic\n",
    "domains = {\n",
    "    'text_classification': ['imdb', 'amazon_reviews_multi', 'yelp_review_full'],\n",
    "    'question_answering': ['squad'],\n",
    "    'computer_vision': ['cifar10', 'mnist', 'fashion_mnist', 'beans'],\n",
    "    'audio': ['common_voice'],\n",
    "    'natural_language': ['glue', 'super_glue', 'conll2003']\n",
    "}\n",
    "\n",
    "print(\"üìÇ DATASETS BY DOMAIN:\")\n",
    "for domain, datasets in domains.items():\n",
    "    print(f\"\\n{domain.upper().replace('_', ' ')}:\")\n",
    "    for dataset in datasets:\n",
    "        print(f\"  ‚Ä¢ {dataset}\")\n",
    "        \n",
    "print(f\"\\nüí° TIP: Visit https://huggingface.co/datasets to search by keywords!\")\n",
    "print(f\"üîç SEARCH STRATEGY: Use filters for task type, language, size, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your First Dataset\n",
    "\n",
    "Let's start with a classic: the IMDB movie reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB movie reviews dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(\"‚úÖ Dataset loaded!\")\n",
    "print(f\"Dataset object type: {type(dataset)}\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training split\n",
    "train_data = dataset['train']\n",
    "print(f\"Training data size: {len(train_data)} examples\")\n",
    "print(f\"Features: {train_data.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first few examples\n",
    "print(\"First 3 examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}: {train_data[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Pandas for Analysis\n",
    "\n",
    "HuggingFace datasets play nicely with pandas for familiar data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = train_data.to_pandas()\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Dataset Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"üìä DATASET QUALITY REPORT:\")\n",
    "print(f\"   ‚Ä¢ Size: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"   ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Missing values: {df.isnull().sum().sum()} total\")\n",
    "print(f\"   ‚Ä¢ Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\nüìà DATA TYPES:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "print(\"üîç MISSING VALUES BY COLUMN:\")\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Percentage': missing_percent\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis (assuming 'label' is the target for IMDB)\n",
    "if 'label' in df.columns:\n",
    "    print(\"üéØ TARGET VARIABLE ANALYSIS:\")\n",
    "    print(f\"   ‚Ä¢ Unique values: {df['label'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Value range: {df['label'].min()} to {df['label'].max()}\")\n",
    "    print(\"   ‚Ä¢ Class meanings: 0=negative, 1=positive\")\n",
    "    print(\"\\n   ‚Ä¢ Distribution:\")\n",
    "    print(df['label'].value_counts().sort_index())\n",
    "    \n",
    "    # Quick visualization\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['label'].hist(bins=2, alpha=0.7)\n",
    "    plt.title('Target Distribution')\n",
    "    plt.xlabel('Sentiment (0=neg, 1=pos)')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show text length distribution\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['text_length'].hist(bins=30, alpha=0.7)\n",
    "    plt.title('Review Length Distribution')\n",
    "    plt.xlabel('Characters')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif 'quality' in df.columns:\n",
    "    print(\"üéØ TARGET VARIABLE ANALYSIS:\")\n",
    "    print(f\"   ‚Ä¢ Unique values: {df['quality'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Value range: {df['quality'].min()} to {df['quality'].max()}\")\n",
    "    print(\"\\n   ‚Ä¢ Distribution:\")\n",
    "    print(df['quality'].value_counts().sort_index())\n",
    "    \n",
    "    # Quick visualization\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['quality'].hist(bins=10, alpha=0.7)\n",
    "    plt.title('Target Distribution')\n",
    "    plt.xlabel('Quality Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df.boxplot(column='quality')\n",
    "    plt.title('Target Box Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"üéØ TARGET VARIABLE ANALYSIS:\")\n",
    "    print(\"   No obvious target column found. Common target names:\")\n",
    "    print(\"   ‚Ä¢ 'label', 'target', 'class', 'y'\")\n",
    "    print(\"   Check column names:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Template\n",
    "\n",
    "Here's a reusable template for loading and quickly assessing any dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_dataset_assessment(dataset_name, split='train'):\n",
    "    \"\"\"\n",
    "    Load a HuggingFace dataset and perform quick quality assessment\n",
    "    \"\"\"\n",
    "    print(f\"üì¶ Loading dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        data = dataset[split]\n",
    "        df = data.to_pandas()\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "        print(f\"   ‚Ä¢ Shape: {df.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Missing values: {df.isnull().sum().sum()}\")\n",
    "        print(f\"   ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing our assessment function:\")\n",
    "test_df = quick_dataset_assessment(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 25‚Äì45 min: Group Exercise - Find, Load & Explore\n",
    "\n",
    "**Goal:** Apply your new skills in small groups to discover and assess datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Your Mission: Dataset Discovery & Assessment\n",
    "\n",
    "**Time:** 20 minutes  \n",
    "**Format:** Groups of 2-3 students  \n",
    "**Deliverable:** Brief presentation of your findings  \n",
    "\n",
    "### Step 1: Choose Your Dataset (5 minutes)\n",
    "\n",
    "**Option A: Pick from our curated list (verified working datasets)**\n",
    "- `\"imdb\"` - Movie reviews (text classification)\n",
    "- `\"mnist\"` - Handwritten digits (image classification)\n",
    "- `\"fashion_mnist\"` - Clothing images (image classification)\n",
    "- `\"beans\"` - Bean leaf disease (image classification)\n",
    "- `\"squad\"` - Reading comprehension (question answering)\n",
    "- `\"yelp_review_full\"` - Restaurant reviews (text classification)\n",
    "\n",
    "**Option B: Find your own**\n",
    "- Browse the HuggingFace Hub: https://huggingface.co/datasets\n",
    "- Search for datasets in your domain of interest\n",
    "- Pick something that looks interesting!\n",
    "- **Tip:** Look for datasets with high download counts\n",
    "\n",
    "### Step 2: Load & Explore (10 minutes)\n",
    "\n",
    "Use the cells below to:\n",
    "1. Load your chosen dataset\n",
    "2. Explore its structure and content\n",
    "3. Assess its quality using our techniques\n",
    "4. Identify potential ML applications\n",
    "\n",
    "### Step 3: Prepare Your Report (3 minutes)\n",
    "\n",
    "Be ready to share:\n",
    "- **Dataset name and domain**\n",
    "- **Size and structure** (rows, columns, data types)\n",
    "- **Quality assessment** (missing values, balance, etc.)\n",
    "- **Potential ML problem** (classification, regression, clustering)\n",
    "- **One interesting finding** from your exploration\n",
    "\n",
    "### Step 4: Group Presentations (2 minutes each)\n",
    "\n",
    "Quick 1-2 minute presentations from each group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Your Group's Workspace\n",
    "\n",
    "**Group Members:** _[Write your names here]_  \n",
    "**Dataset Choice:** _[Write dataset name here]_  \n",
    "**Domain:** _[e.g., healthcare, NLP, computer vision, etc.]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your chosen dataset\n",
    "# Replace \"your_dataset_name\" with your actual choice\n",
    "\n",
    "dataset_name = \"_____________\"  # Fill in your dataset name\n",
    "print(f\"Loading dataset: {dataset_name}\")\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# dataset = load_dataset(dataset_name)\n",
    "# data = dataset['train']  # or appropriate split\n",
    "# df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Basic exploration\n",
    "# YOUR CODE HERE:\n",
    "# Explore shape, columns, data types, first few rows\n",
    "\n",
    "print(\"üìä BASIC INFO:\")\n",
    "# print(f\"Shape: {df.shape}\")\n",
    "# print(f\"Columns: {list(df.columns)}\")\n",
    "# print(f\"Data types: {df.dtypes.value_counts()}\")\n",
    "\n",
    "# Show first few examples\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Quality assessment\n",
    "# YOUR CODE HERE:\n",
    "# Check for missing values, duplicates, target distribution\n",
    "\n",
    "print(\"üîç QUALITY ASSESSMENT:\")\n",
    "# missing_values = df.isnull().sum()\n",
    "# print(f\"Missing values: {missing_values.sum()}\")\n",
    "# print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# If you have a target column, analyze its distribution\n",
    "# target_column = \"_____\"  # Fill in if applicable\n",
    "# df[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a visualization (optional but recommended)\n",
    "# YOUR CODE HERE:\n",
    "# Create a plot that shows something interesting about your data\n",
    "\n",
    "# Examples:\n",
    "# - Histogram of target variable\n",
    "# - Correlation heatmap\n",
    "# - Sample images (if image dataset)\n",
    "# - Word cloud (if text dataset)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Your visualization code here\n",
    "plt.title(\"Your Dataset Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Your Group's Findings\n",
    "\n",
    "**Complete this summary for your presentation:**\n",
    "\n",
    "**Dataset:** _[Name and brief description]_\n",
    "\n",
    "**Size & Structure:**\n",
    "- Rows: _[number]_\n",
    "- Columns: _[number]_\n",
    "- Data types: _[mix of numeric/text/categorical]_\n",
    "\n",
    "**Quality Assessment:**\n",
    "- Missing values: _[percentage or count]_\n",
    "- Data quality: _[Good/Fair/Poor and why]_\n",
    "- Balance: _[For classification: balanced/imbalanced]_\n",
    "\n",
    "**ML Application:**\n",
    "- Problem type: _[Classification/Regression/Other]_\n",
    "- Potential use case: _[What could you predict/classify?]_\n",
    "\n",
    "**Most Interesting Finding:**\n",
    "_[What surprised you or seemed most noteworthy?]_\n",
    "\n",
    "**Recommendation:**\n",
    "- Would you use this dataset for a project? _[Yes/No and why]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-Up: What You've Accomplished\n",
    "\n",
    "In just 45 minutes, you've learned to:\n",
    "\n",
    "‚úÖ **Navigate dataset resources** - Kaggle and HuggingFace Hub  \n",
    "‚úÖ **Load datasets programmatically** - Using the datasets library  \n",
    "‚úÖ **Assess data quality** - Missing values, distribution, balance  \n",
    "‚úÖ **Apply skills immediately** - Hands-on group exploration  \n",
    "‚úÖ **Share findings** - Communicate dataset insights effectively  \n",
    "\n",
    "### Next Steps for Your Projects:\n",
    "\n",
    "1. **Explore more datasets** - Browse HuggingFace Hub and Kaggle\n",
    "2. **Practice quality assessment** - Use our template on different data\n",
    "3. **Consider ethical implications** - Data bias, privacy, licensing\n",
    "4. **Document your choices** - Keep notes on why you selected specific datasets\n",
    "\n",
    "### Useful Resources:\n",
    "\n",
    "- **HuggingFace Datasets Hub:** https://huggingface.co/datasets\n",
    "- **Kaggle Datasets:** https://www.kaggle.com/datasets\n",
    "- **Dataset Documentation:** Always read the dataset cards!\n",
    "- **Community Discussions:** Check comments and kernels for insights\n",
    "\n",
    "**Remember:** The quality of your data will largely determine the success of your ML project. Take time to choose and validate your datasets carefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
