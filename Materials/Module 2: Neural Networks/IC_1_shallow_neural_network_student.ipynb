{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Building Your First Neural Network: Universal Approximation (CSC 422)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Format:** Live coding with student participation  \n",
    "**Course:** CSC 422 - Machine and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this session, students will:\n",
    "- Understand the Universal Approximation Theorem conceptually\n",
    "- Implement a shallow neural network from scratch using NumPy\n",
    "- See how adding neurons increases function approximation power\n",
    "- Connect neural networks to Module 1's gradient descent concepts\n",
    "- Build foundation for deeper neural network architectures\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Timeline\n",
    "\n",
    "- **0‚Äì5 min** ‚Äî Hook: The Universal Approximation Magic\n",
    "- **5‚Äì15 min** ‚Äî Build Neural Network from Scratch\n",
    "- **15‚Äì25 min** ‚Äî Add Neurons & Watch Approximation Improve\n",
    "- **25‚Äì30 min** ‚Äî Connect to Gradient Descent & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Ready to build neural networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hook-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0‚Äì5 min: Hook - The Universal Approximation Magic\n",
    "\n",
    "**Goal:** Show the amazing end result, then work backwards to understand how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hook-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The end result we're building towards\n",
    "print(\"üéØ TODAY'S GOAL: Build a neural network that can approximate ANY function!\")\n",
    "print(\"üìà Universal Approximation Theorem: A neural network with enough neurons\")\n",
    "print(\"   can approximate any continuous function to arbitrary accuracy.\")\n",
    "print(\"üöÄ We'll start simple and add complexity step by step!\")\n",
    "\n",
    "# Create a complex target function to approximate\n",
    "def complex_function(x):\n",
    "    \"\"\"A complex function we want our neural network to learn\"\"\"\n",
    "    return np.sin(2 * x) + 0.5 * np.cos(5 * x) + 0.3 * np.sin(10 * x)\n",
    "\n",
    "# Generate data\n",
    "x_demo = np.linspace(-2, 2, 200)\n",
    "y_demo = complex_function(x_demo)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target Function')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output (y)')\n",
    "plt.title('Complex Function We Want to Approximate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"ü§î How can we approximate this complex wiggling function?\")\n",
    "print(\"üí° Answer: Combine simple building blocks (neurons) intelligently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-1",
   "metadata": {},
   "source": [
    "**Discussion:** *Looking at this complex function, how might you break it down into simpler pieces?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network-basics",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5‚Äì15 min: Build Neural Network from Scratch\n",
    "\n",
    "**Goal:** Implement the simplest possible neural network and understand each component\n",
    "\n",
    "## From Linear Regression to Neural Networks\n",
    "\n",
    "Remember Module 1? We built: **y = ax + b**\n",
    "\n",
    "A single neuron is almost the same: **y = œÉ(wx + b)**\n",
    "\n",
    "The only difference: **œÉ** (sigma) - the activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key ingredient: Activation Functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: smooth S-curve, outputs between 0 and 1\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x) - simple but powerful\"\"\"\n",
    "    # TODO: Implement ReLU activation function\n",
    "    return _____________ \n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh activation: outputs between -1 and 1\"\"\"\n",
    "    # TODO: Implement tanh activation function\n",
    "    return _____________\n",
    "\n",
    "# Visualize activation functions\n",
    "x_act = np.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_act, sigmoid(x_act), 'b-', linewidth=2)\n",
    "plt.title('Sigmoid: œÉ(x)')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_act, relu(x_act), 'r-', linewidth=2)\n",
    "plt.title('ReLU: max(0, x)')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x_act, tanh(x_act), 'g-', linewidth=2)\n",
    "plt.title('Tanh: tanh(x)')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîë Key insight: Activation functions add non-linearity!\")\n",
    "print(\"   Without them, neural networks would just be linear regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-neuron-class",
   "metadata": {},
   "source": [
    "## Build a Single Neuron\n",
    "\n",
    "Let's implement our first neuron class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-neuron",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron:\n",
    "    \"\"\"A single neuron: the building block of neural networks\"\"\"\n",
    "    \n",
    "    def __init__(self, activation='tanh'):\n",
    "        \"\"\"Initialize neuron with random weights\"\"\"\n",
    "        # TODO: Initialize weight and bias with random values\n",
    "        self.weight = _____________  # Random weight\n",
    "        self.bias = _____________    # Random bias\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "        else:\n",
    "            self.activation = tanh\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: compute neuron output\"\"\"\n",
    "        # Step 1: Linear combination (just like Module 1!)\n",
    "        linear_output = _____________  # TODO: Compute w*x + b\n",
    "        \n",
    "        # Step 2: Apply activation function (the new part!)\n",
    "        return self.activation(linear_output)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Neuron(w={self.weight:.3f}, b={self.bias:.3f})\"\n",
    "\n",
    "# Create and test a single neuron\n",
    "neuron = SingleNeuron(activation='tanh')\n",
    "print(f\"Created neuron: {neuron}\")\n",
    "\n",
    "# Test it on some data\n",
    "x_test = np.linspace(-3, 3, 100)\n",
    "y_neuron = neuron.forward(x_test)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_test, y_neuron, 'r-', linewidth=2, label=f'Single Neuron Output')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output (y)')\n",
    "plt.title('What One Neuron Can Do')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üß† One neuron creates a smooth curve!\")\n",
    "print(f\"   But can it approximate our complex target function? Let's see...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-neuron-limits",
   "metadata": {},
   "source": [
    "## Test Single Neuron vs. Target Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-neuron-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single neuron to our target function\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target Function', alpha=0.8)\n",
    "plt.plot(x_demo, neuron.forward(x_demo), 'r--', linewidth=2, label='Single Neuron')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output (y)')\n",
    "plt.title('Single Neuron vs. Complex Target Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observation: One neuron can't capture the complexity!\")\n",
    "print(\"üí° Solution: Add more neurons and combine their outputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-2",
   "metadata": {},
   "source": [
    "**Discussion:** *What do you think will happen if we add more neurons and combine them?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shallow-network-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 15‚Äì25 min: Add Neurons & Watch Approximation Improve\n",
    "\n",
    "**Goal:** Build a shallow neural network and see the Universal Approximation Theorem in action\n",
    "\n",
    "## Shallow Neural Network Architecture\n",
    "\n",
    "A shallow neural network combines multiple neurons:\n",
    "**y = w‚ÇÅœÉ(w‚ÇÅ‚ÇÅx + b‚ÇÅ) + w‚ÇÇœÉ(w‚ÇÅ‚ÇÇx + b‚ÇÇ) + ... + w‚ÇôœÉ(w‚ÇÅ‚Çôx + b‚Çô)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shallow-network-class",
   "metadata": {},
   "outputs": [],
   "source": "class ShallowNeuralNetwork:\n    \"\"\"A shallow neural network with multiple neurons in one hidden layer\"\"\"\n    \n    def __init__(self, num_neurons=3, activation='tanh'):\n        \"\"\"Initialize network with specified number of neurons\"\"\"\n        self.num_neurons = num_neurons\n        \n        # Hidden layer weights and biases (input to hidden)\n        # TODO: Create arrays of random weights and biases for hidden layer\n        self.hidden_weights = _____________\n        self.hidden_biases = _____________\n        \n        # Output layer weights (hidden to output)\n        # TODO: Create array of random weights for output layer\n        self.output_weights = _____________\n        \n        # Choose activation function\n        if activation == 'sigmoid':\n            self.activation = sigmoid\n            self.activation_derivative = lambda x: sigmoid(x) * (1 - sigmoid(x))\n        elif activation == 'relu':\n            self.activation = relu\n            self.activation_derivative = lambda x: (x > 0).astype(float)\n        else:\n            self.activation = tanh\n            self.activation_derivative = lambda x: 1 - tanh(x)**2\n    \n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        # Step 1: Compute hidden layer outputs\n        # Each neuron: œÉ(w_i * x + b_i)\n        hidden_linear = self.hidden_weights[:, np.newaxis] * x + self.hidden_biases[:, np.newaxis]\n        hidden_outputs = self.activation(hidden_linear)\n        \n        # Step 2: Combine hidden outputs with output weights\n        # Final output: sum of weighted hidden outputs\n        # TODO: Compute final output as weighted sum\n        final_output = _____________\n        \n        return final_output, hidden_outputs, hidden_linear\n    \n    def predict(self, x):\n        \"\"\"Simple prediction without returning hidden outputs\"\"\"\n        output, _, _ = self.forward(x)\n        return output\n    \n    def train(self, x_train, y_train, learning_rate=0.01, epochs=1000, verbose=True):\n        \"\"\"Train the network using gradient descent\n        \n        Args:\n            x_train: Training inputs\n            y_train: Training targets\n            learning_rate: Step size for gradient descent\n            epochs: Number of training iterations\n            verbose: Whether to print training progress\n            \n        Returns:\n            loss_history: List of loss values during training\n        \"\"\"\n        loss_history = []\n        \n        for epoch in range(epochs):\n            # Forward pass\n            output, hidden_outputs, hidden_linear = self.forward(x_train)\n            \n            # Calculate loss (Mean Squared Error)\n            # TODO: Compute mean squared error\n            loss = _____________\n            loss_history.append(loss)\n            \n            # Backward pass - compute gradients\n            # Output layer gradients\n            output_error = output - y_train\n            # TODO: Compute gradients for output weights\n            output_weights_grad = _____________\n            \n            # Hidden layer gradients (chain rule!)\n            hidden_error = output_error * self.output_weights[:, np.newaxis]\n            hidden_activation_grad = self.activation_derivative(hidden_linear)\n            # TODO: Compute gradients for hidden weights and biases\n            hidden_weights_grad = _____________\n            hidden_biases_grad = _____________\n            \n            # Update parameters using gradient descent\n            # TODO: Update all parameters (weights and biases)\n            self.output_weights -= _____________\n            self.hidden_weights -= _____________\n            self.hidden_biases -= _____________\n            \n            # Print progress\n            if verbose and (epoch + 1) % (epochs // 5) == 0:\n                print(f\"   Epoch {epoch + 1:4d}/{epochs}: Loss = {loss:.6f}\")\n        \n        if verbose:\n            print(f\"‚úÖ Training complete! Final loss: {loss:.6f}\")\n        \n        return loss_history\n    \n    def __repr__(self):\n        return f\"ShallowNN({self.num_neurons} neurons)\"\n\n# Test with different numbers of neurons\nneuron_counts = [1, 3, 5, 10]\nnetworks = {}\n\nfor n in neuron_counts:\n    networks[n] = ShallowNeuralNetwork(num_neurons=n, activation='tanh')\n    print(f\"Created network with {n} neurons\")\n\nprint(\"\\n‚úÖ Neural networks ready to test!\")"
  },
  {
   "cell_type": "markdown",
   "id": "approximation-comparison",
   "metadata": {},
   "source": [
    "## Watch Approximation Power Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare networks with different numbers of neurons\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, n_neurons in enumerate(neuron_counts):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Get network prediction\n",
    "    network = networks[n_neurons]\n",
    "    y_pred = network.predict(x_demo)\n",
    "    \n",
    "    # Plot target vs prediction\n",
    "    plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target Function', alpha=0.8)\n",
    "    plt.plot(x_demo, y_pred, 'r--', linewidth=2, label=f'Network ({n_neurons} neurons)')\n",
    "    \n",
    "    # Calculate and display error\n",
    "    # TODO: Compute mean squared error\n",
    "    mse = _____________\n",
    "    plt.title(f'{n_neurons} Neurons - MSE: {mse:.4f}')\n",
    "    plt.xlabel('Input (x)')\n",
    "    plt.ylabel('Output (y)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà UNIVERSAL APPROXIMATION IN ACTION:\")\n",
    "print(\"   ‚Üí More neurons = better approximation\")\n",
    "print(\"   ‚Üí With enough neurons, we can approximate ANY function!\")\n",
    "print(\"   ‚Üí This is the theoretical foundation of neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wk29k70pnec",
   "source": "## The Power of Training: Before vs After\n\n**Goal:** See how training transforms random networks into function approximators!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kkqx5vxaq",
   "source": "# Create training data\nx_train = np.linspace(-2, 2, 100)\ny_train = complex_function(x_train)\n\n# Compare untrained vs trained networks\nplt.figure(figsize=(15, 10))\n\nneuron_counts_train = [3, 5, 10]\nfor i, n_neurons in enumerate(neuron_counts_train):\n    # Create two identical networks\n    untrained_network = ShallowNeuralNetwork(num_neurons=n_neurons, activation='tanh')\n    trained_network = ShallowNeuralNetwork(num_neurons=n_neurons, activation='tanh')\n    \n    # Copy weights to make them identical initially\n    trained_network.hidden_weights = untrained_network.hidden_weights.copy()\n    trained_network.hidden_biases = untrained_network.hidden_biases.copy()\n    trained_network.output_weights = untrained_network.output_weights.copy()\n    \n    # Get untrained prediction\n    y_untrained = untrained_network.predict(x_demo)\n    \n    # Train the network\n    print(f\"\\\\nüéØ Training {n_neurons}-neuron network...\")\n    # TODO: Train the network on our data\n    loss_history = _____________\n    \n    # Get trained prediction\n    y_trained = trained_network.predict(x_demo)\n    \n    # Plot results\n    plt.subplot(2, 3, i+1)\n    plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target', alpha=0.8)\n    plt.plot(x_demo, y_untrained, 'r--', linewidth=2, label='Untrained', alpha=0.7)\n    plt.title(f'{n_neurons} Neurons - BEFORE Training')\n    plt.xlabel('Input (x)')\n    plt.ylabel('Output (y)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(2, 3, i+4)\n    plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target', alpha=0.8)\n    plt.plot(x_demo, y_trained, 'g-', linewidth=2, label='Trained', alpha=0.8)\n    \n    # TODO: Calculate final MSE after training\n    final_mse = _____________\n    plt.title(f'{n_neurons} Neurons - AFTER Training\\\\nMSE: {final_mse:.4f}')\n    plt.xlabel('Input (x)')\n    plt.ylabel('Output (y)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\nüöÄ TRAINING RESULTS:\")\nprint(\"   ‚Üí Random networks: Poor approximation\")\nprint(\"   ‚Üí Trained networks: Excellent approximation!\")\nprint(\"   ‚Üí This is the magic of gradient descent!\")\nprint(\"   ‚Üí More neurons + training = Universal Approximation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4cpswj4nt33",
   "source": "## Watch the Learning Process",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "204ugzrsoi4h",
   "source": "# Demonstrate the learning process with different network sizes\nplt.figure(figsize=(12, 8))\n\n# Train networks and collect learning curves\nlearning_curves = {}\nfinal_performances = {}\n\nfor n_neurons in [1, 3, 5, 10]:\n    print(f\"Training {n_neurons}-neuron network...\")\n    \n    # Create and train network\n    network = ShallowNeuralNetwork(num_neurons=n_neurons, activation='tanh')\n    # TODO: Train the network and collect loss history\n    loss_history = _____________\n    \n    learning_curves[n_neurons] = loss_history\n    final_performances[n_neurons] = loss_history[-1]\n\n# Plot learning curves\nplt.subplot(2, 2, 1)\nfor n_neurons, losses in learning_curves.items():\n    plt.plot(losses, label=f'{n_neurons} neurons', linewidth=2)\nplt.xlabel('Training Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('Learning Curves: Loss vs Training Time')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\n# Plot final performance vs network size\nplt.subplot(2, 2, 2)\nneurons = list(final_performances.keys())\nfinal_losses = list(final_performances.values())\nplt.bar(neurons, final_losses, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\nplt.xlabel('Number of Neurons')\nplt.ylabel('Final Loss (MSE)')\nplt.title('Final Performance vs Network Size')\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\n# Show parameter count vs performance\nplt.subplot(2, 2, 3)\nparam_counts = [2*n + n for n in neurons]  # 2*n weights + n biases\nplt.scatter(param_counts, final_losses, c=['red', 'orange', 'green', 'blue'], s=100)\nfor i, n in enumerate(neurons):\n    plt.annotate(f'{n}n', (param_counts[i], final_losses[i]), xytext=(5, 5), \n                textcoords='offset points')\nplt.xlabel('Total Parameters')\nplt.ylabel('Final Loss (MSE)')\nplt.title('Parameters vs Performance')\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\n# Show convergence comparison\nplt.subplot(2, 2, 4)\n# Plot only the first 200 epochs for clarity\nfor n_neurons, losses in learning_curves.items():\n    epochs_to_show = min(200, len(losses))\n    plt.plot(range(epochs_to_show), losses[:epochs_to_show], \n             label=f'{n_neurons} neurons', linewidth=2)\nplt.xlabel('Training Epoch (First 200)')\nplt.ylabel('Loss (MSE)')\nplt.title('Early Training: Convergence Speed')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä LEARNING INSIGHTS:\")\nprint(\"   ‚úì More neurons ‚Üí Lower final loss\")\nprint(\"   ‚úì More parameters ‚Üí Better approximation\")\nprint(\"   ‚úì All networks learn, but at different rates\")\nprint(\"   ‚úì Diminishing returns: 10 neurons vs 5 neurons\")\nprint(\"\\\\nüéØ KEY TAKEAWAY: Universal Approximation Theorem in action!\")\nprint(\"   With enough neurons and proper training, we can approximate any function!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "individual-neuron-contributions",
   "metadata": {},
   "source": [
    "## Visualize Individual Neuron Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neuron-contributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how individual neurons contribute to the final output\n",
    "network_5 = networks[5]  # Use 5-neuron network\n",
    "final_output, hidden_outputs = network_5.forward(x_demo)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Top plot: Individual neuron outputs\n",
    "plt.subplot(2, 1, 1)\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "for i in range(5):\n",
    "    # TODO: Compute weighted output for each neuron\n",
    "    weighted_output = _____________\n",
    "    plt.plot(x_demo, weighted_output, color=colors[i], linewidth=2, \n",
    "             label=f'Neuron {i+1} (w={network_5.output_weights[i]:.2f})', alpha=0.7)\n",
    "\n",
    "plt.title('Individual Weighted Neuron Contributions')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Neuron Output')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom plot: Sum of all neurons vs target\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x_demo, y_demo, 'b-', linewidth=3, label='Target Function')\n",
    "plt.plot(x_demo, final_output, 'r--', linewidth=2, label='Sum of All Neurons')\n",
    "plt.title('Final Network Output (Sum of Individual Contributions)')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output (y)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Key Insight: Neural networks work by combining simple building blocks!\")\n",
    "print(\"   Each neuron captures a different 'feature' of the target function.\")\n",
    "print(\"   The final output is just a weighted sum of these features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student-question-3",
   "metadata": {},
   "source": [
    "**Discussion:** *How is this similar to building with LEGO blocks? What happens if we want even more accuracy?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameter-scaling",
   "metadata": {},
   "source": [
    "## Parameter Scaling Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how parameter count scales with neurons\n",
    "def count_parameters(num_neurons):\n",
    "    \"\"\"Count total parameters in our shallow network\"\"\"\n",
    "    # TODO: Calculate total number of parameters\n",
    "    # Hidden layer: weights + biases\n",
    "    # Output layer: weights\n",
    "    hidden_params = _____________  # weights + biases for hidden layer\n",
    "    output_params = _____________  # output weights\n",
    "    return hidden_params + output_params\n",
    "\n",
    "# Calculate parameters for different network sizes\n",
    "neuron_range = np.arange(1, 21)\n",
    "param_counts = [count_parameters(n) for n in neuron_range]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(neuron_range, param_counts, 'bo-', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Total Parameters')\n",
    "plt.title('Parameter Count vs. Network Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show approximation quality vs parameters\n",
    "plt.subplot(1, 2, 2)\n",
    "test_neurons = [1, 3, 5, 10, 15]\n",
    "mse_scores = []\n",
    "\n",
    "for n in test_neurons:\n",
    "    net = ShallowNeuralNetwork(num_neurons=n, activation='tanh')\n",
    "    pred = net.predict(x_demo)\n",
    "    mse = np.mean((y_demo - pred)**2)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "plt.plot([count_parameters(n) for n in test_neurons], mse_scores, 'ro-', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Approximation Quality vs. Parameters')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä PARAMETER SCALING INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ 1 neuron:  {count_parameters(1)} parameters\")\n",
    "print(f\"   ‚Ä¢ 10 neurons: {count_parameters(10)} parameters\")\n",
    "print(f\"   ‚Ä¢ 100 neurons: {count_parameters(100)} parameters\")\n",
    "print(\"   ‚Ä¢ More parameters = more flexibility = better approximation\")\n",
    "print(\"   ‚Ä¢ Modern neural networks have MILLIONS or BILLIONS of parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-descent-connection",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 25‚Äì30 min: Connect to Gradient Descent & Next Steps\n",
    "\n",
    "**Goal:** Bridge today's implementation to Module 1 concepts and preview what's coming\n",
    "\n",
    "## The Optimization Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Module 1: Same optimization, more parameters\n",
    "print(\"üîó CONNECTION TO MODULE 1:\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(\"üìà Linear Regression (Module 1):\")\n",
    "print(\"   ‚Ä¢ Function: y = ax + b\")\n",
    "print(\"   ‚Ä¢ Parameters: 2 (a, b)\")\n",
    "print(\"   ‚Ä¢ Optimization: Gradient descent\")\n",
    "print(\"\")\n",
    "print(\"üß† Neural Network (Today):\")\n",
    "print(\"   ‚Ä¢ Function: y = Œ£ w·µ¢œÉ(w·µ¢·µ¢x + b·µ¢)\")\n",
    "print(f\"   ‚Ä¢ Parameters: {count_parameters(5)} (for 5 neurons)\")\n",
    "print(\"   ‚Ä¢ Optimization: SAME gradient descent!\")\n",
    "print(\"\")\n",
    "print(\"üéØ Key insight: Neural networks are just more complex functions\")\n",
    "print(\"   that we optimize using the same mathematical principles!\")\n",
    "\n",
    "# Show parameter landscape visualization\n",
    "print(\"\\nüóª OPTIMIZATION LANDSCAPE:\")\n",
    "print(\"   ‚Ä¢ Linear regression: 2D parameter space (easy to visualize)\")\n",
    "print(\"   ‚Ä¢ Neural networks: High-dimensional space (harder but same idea)\")\n",
    "print(\"   ‚Ä¢ Gradient descent: Still walks downhill to find minimum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whats-next",
   "metadata": {},
   "source": [
    "## What's Coming Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-next-topics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview upcoming neural network topics\n",
    "print(\"üöÄ COMING SOON IN MODULE 2:\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"üéØ Backpropagation:\")\n",
    "print(\"   ‚Ä¢ How to efficiently compute gradients for ALL parameters\")\n",
    "print(\"   ‚Ä¢ The algorithm that makes neural network training practical\")\n",
    "print(\"\")\n",
    "print(\"üèóÔ∏è Deep Networks:\")\n",
    "print(\"   ‚Ä¢ Multiple hidden layers: shallow ‚Üí deep\")\n",
    "print(\"   ‚Ä¢ Why depth matters for complex problems\")\n",
    "print(\"\")\n",
    "print(\"üìä Real Applications:\")\n",
    "print(\"   ‚Ä¢ Image classification\")\n",
    "print(\"   ‚Ä¢ Natural language processing\")\n",
    "print(\"   ‚Ä¢ Game playing (like AlphaGo)\")\n",
    "print(\"\")\n",
    "print(\"üõ†Ô∏è Practical Tools:\")\n",
    "print(\"   ‚Ä¢ PyTorch and TensorFlow\")\n",
    "print(\"   ‚Ä¢ GPU acceleration\")\n",
    "print(\"   ‚Ä¢ Best practices for training\")\n",
    "print(\"\")\n",
    "print(\"üí° Remember: Today's simple network is the foundation for ALL of this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways & Skills Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skills-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TODAY'S NEURAL NETWORK SKILLS ‚úÖ ===\")\n",
    "print(\"‚úì Universal Approximation Theorem - understand the theory\")\n",
    "print(\"‚úì Activation functions - the source of non-linearity\")\n",
    "print(\"‚úì Shallow network implementation - from scratch with NumPy\")\n",
    "print(\"‚úì Parameter scaling - more neurons = more approximation power\")\n",
    "print(\"‚úì Function decomposition - complex functions from simple parts\")\n",
    "print(\"‚úì Connection to optimization - same gradient descent principles\")\n",
    "print(\"\")\n",
    "print(\"üéì YOU NOW UNDERSTAND:\")\n",
    "print(\"   ‚Ä¢ Why neural networks are powerful (Universal Approximation)\")\n",
    "print(\"   ‚Ä¢ How they work internally (weighted combinations of neurons)\")\n",
    "print(\"   ‚Ä¢ Why we need many parameters (flexibility for complex functions)\")\n",
    "print(\"   ‚Ä¢ How they connect to classical ML (gradient descent optimization)\")\n",
    "print(\"\")\n",
    "print(\"üöÄ READY FOR: Backpropagation, deep networks, and real applications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-challenge",
   "metadata": {},
   "source": [
    "## Mini Challenge (Your Turn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-challenge-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Try different activation functions and see the effect\n",
    "print(\"üéØ YOUR TURN - MINI CHALLENGE:\")\n",
    "print(\"Try creating networks with different activation functions\")\n",
    "print(\"and see how they affect the approximation quality!\")\n",
    "print(\"\")\n",
    "print(\"Experiment with:\")\n",
    "print(\"‚Ä¢ sigmoid vs tanh vs relu\")\n",
    "print(\"‚Ä¢ Different numbers of neurons\")\n",
    "print(\"‚Ä¢ Different target functions\")\n",
    "print(\"\")\n",
    "print(\"Which combination works best for different types of functions?\")\n",
    "\n",
    "# TODO: Create and test different networks\n",
    "# Example:\n",
    "# relu_network = ShallowNeuralNetwork(num_neurons=5, activation='relu')\n",
    "# sigmoid_network = ShallowNeuralNetwork(num_neurons=5, activation='sigmoid')\n",
    "# Compare their outputs!\n",
    "\n",
    "# YOUR CODE HERE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdet990z2y",
   "source": "---\n\n# Extension: Multi-Input, Multi-Output Networks\n\n**Goal:** Build realistic neural networks that handle multiple inputs and outputs\n\n## From 1D to Multi-Dimensional\n\nReal neural networks typically have:\n- **Multiple inputs**: Features like age, income, temperature, pixel values\n- **Multiple outputs**: Classifications, predictions, or multi-task objectives\n\n**Architecture:** 2 inputs ‚Üí 3 hidden neurons ‚Üí 2 outputs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bvl4gs1vbmm",
   "source": "class MultiInputOutputNetwork:\n    \"\"\"A shallow neural network with multiple inputs and outputs\"\"\"\n    \n    def __init__(self, input_size=2, hidden_size=3, output_size=2, activation='tanh'):\n        \"\"\"Initialize network with specified architecture\"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Hidden layer: input_size ‚Üí hidden_size\n        # Each hidden neuron connects to ALL inputs\n        # TODO: Create weight matrix and bias vector for hidden layer\n        self.W1 = _____________  # Shape: (hidden_size, input_size)\n        self.b1 = _____________  # Shape: (hidden_size, 1)\n        \n        # Output layer: hidden_size ‚Üí output_size\n        # Each output connects to ALL hidden neurons\n        # TODO: Create weight matrix and bias vector for output layer\n        self.W2 = _____________  # Shape: (output_size, hidden_size)\n        self.b2 = _____________  # Shape: (output_size, 1)\n        \n        # Choose activation function\n        if activation == 'sigmoid':\n            self.activation = sigmoid\n        elif activation == 'relu':\n            self.activation = relu\n        else:\n            self.activation = tanh\n    \n    def forward(self, X):\n        \"\"\"Forward pass through the network\n        \n        Args:\n            X: Input matrix of shape (input_size, num_samples)\n        \n        Returns:\n            output: Network output of shape (output_size, num_samples)\n            hidden: Hidden activations of shape (hidden_size, num_samples)\n        \"\"\"\n        # Ensure X is 2D\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        \n        # Hidden layer: Z1 = W1 @ X + b1, A1 = œÉ(Z1)\n        # TODO: Compute hidden layer linear combination\n        Z1 = _____________\n        A1 = self.activation(Z1)\n        \n        # Output layer: Z2 = W2 @ A1 + b2\n        # TODO: Compute output layer linear combination\n        Z2 = _____________\n        \n        return Z2, A1\n    \n    def predict(self, X):\n        \\\"\\\"\\\"Simple prediction interface\\\"\\\"\\\"\n        output, _ = self.forward(X)\n        return output\n    \n    def count_parameters(self):\n        \\\"\\\"\\\"Count total number of trainable parameters\\\"\\\"\\\"\n        # TODO: Calculate total parameters\n        w1_params = _____________\n        w2_params = _____________\n        return w1_params + w2_params\n    \n    def __repr__(self):\n        return f\\\"MultiNN({self.input_size}‚Üí{self.hidden_size}‚Üí{self.output_size})\\\"\n\n# Create our 2‚Üí3‚Üí2 network (will work once TODOs are completed)\n# multi_network = MultiInputOutputNetwork(input_size=2, hidden_size=3, output_size=2)\n# print(f\\\"Created network: {multi_network}\\\")\nprint(\\\"üí° Complete the TODOs above to create the multi-input/output network!\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "isl3o67495q",
   "source": "## Your Challenge: Real-World Applications\n\n**Your Task:** Think about how our 2‚Üí3‚Üí2 network maps to real-world problems!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z7qc12hqq3n",
   "source": "# Real-world application examples\nprint(\\\"üåç REAL-WORLD APPLICATIONS:\\\")\nprint(\\\"Our 2‚Üí3‚Üí2 network architecture appears everywhere in practice!\\\")\nprint(\\\"\\\")\n\n# TODO: Complete these application examples\napplications = [\n    {\n        'domain': 'Medical Diagnosis',\n        'inputs': ['Blood Pressure', '_____________'],  # TODO: Add second input\n        'outputs': ['_____________', 'Treatment Urgency'],  # TODO: Add first output\n        'hidden': 'Patient Health Patterns'\n    },\n    {\n        'domain': 'Stock Trading',\n        'inputs': ['_____________', 'Volume'],  # TODO: Add first input\n        'outputs': ['Buy Signal', '_____________'],  # TODO: Add second output\n        'hidden': 'Market Trend Features'\n    },\n    {\n        'domain': 'Weather Prediction',\n        'inputs': ['Temperature', '_____________'],  # TODO: Add second input\n        'outputs': ['_____________', 'Wind Speed'],  # TODO: Add first output\n        'hidden': 'Climate Dynamics'\n    }\n]\n\nfor i, app in enumerate(applications, 1):\n    print(f\\\"{i}. {app['domain']}:\\\")\n    print(f\\\"   üì• Inputs: {', '.join(app['inputs'])}\\\")\n    print(f\\\"   üß† Hidden: {app['hidden']}\\\")\n    print(f\\\"   üì§ Outputs: {', '.join(app['outputs'])}\\\")\n    print()\n\nprint(\\\"üéØ YOUR TURN:\\\")\nprint(\\\"Think of your own 2-input, 2-output application!\\\")\nprint(\\\"What problem could you solve with this architecture?\\\")\n\n# TODO: Create your own application example\nmy_application = {\n    'domain': '_____________',  # Your application domain\n    'inputs': ['_____________', '_____________'],  # Your two inputs\n    'outputs': ['_____________', '_____________'],  # Your two outputs\n    'description': '_____________'  # Brief description\n}\n\nprint(f\\\"\\\\nüöÄ YOUR APPLICATION:\\\")\nprint(f\\\"Domain: {my_application['domain']}\\\")\nprint(f\\\"Inputs: {', '.join(my_application['inputs'])}\\\")\nprint(f\\\"Outputs: {', '.join(my_application['outputs'])}\\\")\nprint(f\\\"Description: {my_application['description']}\\\")\n\nprint(\\\"\\\\nüí° KEY INSIGHTS:\\\")\nprint(\\\"‚úì Neural networks scale from simple demos to real applications\\\")\nprint(\\\"‚úì Same mathematical principles work across all domains\\\")\nprint(\\\"‚úì Matrix operations enable efficient computation\\\")\nprint(\\\"‚úì Understanding fundamentals prepares you for any framework\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}