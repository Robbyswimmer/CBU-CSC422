{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Building Your First Neural Network: Universal Approximation (CSC 422)\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Format:** Live coding with student participation  \n",
    "**Course:** CSC 422 - Machine and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this session, students will:\n",
    "- Understand the Universal Approximation Theorem conceptually\n",
    "- Implement a shallow neural network from scratch using NumPy\n",
    "- See how adding neurons increases function approximation power\n",
    "- Connect neural networks to Module 1's gradient descent concepts\n",
    "- Build foundation for deeper neural network architectures\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱Timeline\n",
    "\n",
    "- **0–5 min** — Hook: The Universal Approximation Magic\n",
    "- **5–15 min** — Build Neural Network from Scratch\n",
    "- **15–25 min** — Add Neurons & Watch Approximation Improve\n",
    "- **25–30 min** — Connect to Gradient Descent & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"✅ Ready to build neural networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hook-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0–5 min: Hook - The Universal Approximation Magic\n",
    "\n",
    "**Goal:** Show the amazing end result, then work backwards to understand how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hook-demonstration",
   "metadata": {},
   "outputs": [],
   "source": "# The end result we're building towards\nprint(\"TODAY'S GOAL: Build a neural network that can approximate ANY function!\")\nprint(\"Universal Approximation Theorem: A neural network with enough neurons\")\nprint(\"   can approximate any continuous function to arbitrary accuracy.\")\nprint(\"We'll start simple and add complexity step by step!\")\n\n# Create a complex target function to approximate\ndef complex_function(x):\n    \"\"\"A complex function we want our neural network to learn\"\"\"\n    return np.sin(2 * x) + 0.5 * np.cos(5 * x) + 0.3 * np.sin(10 * x)\n\n# Generate data\nx_demo = np.linspace(-2, 2, 200)\ny_demo = complex_function(x_demo)\n\nplt.figure(figsize=(10, 4))\nplt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output (y)')\nplt.title('Complex Function We Want to Approximate')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\nprint(\"How can we approximate this complex wiggling function?\")\nprint(\"Answer: Combine simple building blocks (neurons) intelligently!\")"
  },
  {
   "cell_type": "markdown",
   "id": "student-question-1",
   "metadata": {},
   "source": [
    "**Discussion:** *Looking at this complex function, how might you break it down into simpler pieces?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network-basics",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5–15 min: Build Neural Network from Scratch\n",
    "\n",
    "**Goal:** Implement the simplest possible neural network and understand each component\n",
    "\n",
    "## From Linear Regression to Neural Networks\n",
    "\n",
    "Remember Module 1? We built: **y = ax + b**\n",
    "\n",
    "A single neuron is almost the same: **y = σ(wx + b)**\n",
    "\n",
    "The only difference: **σ** (sigma) - the activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-functions",
   "metadata": {},
   "outputs": [],
   "source": "# The key ingredient: Activation Functions\ndef sigmoid(x):\n    \"\"\"Sigmoid activation: smooth S-curve, outputs between 0 and 1\"\"\"\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n\ndef relu(x):\n    \"\"\"ReLU activation: max(0, x) - simple but powerful\"\"\"\n    # TODO: Implement ReLU activation function\n    return _____________ \n\ndef tanh(x):\n    \"\"\"Tanh activation: outputs between -1 and 1\"\"\"\n    # TODO: Implement tanh activation function\n    return _____________\n\n# Visualize activation functions\nx_act = np.linspace(-5, 5, 100)\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(x_act, sigmoid(x_act), 'b-', linewidth=2)\nplt.title('Sigmoid: σ(x)')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 2)\nplt.plot(x_act, relu(x_act), 'r-', linewidth=2)\nplt.title('ReLU: max(0, x)')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 3)\nplt.plot(x_act, tanh(x_act), 'g-', linewidth=2)\nplt.title('Tanh: tanh(x)')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Activation functions add non-linearity!\")\nprint(\"   Without them, neural networks would just be linear regression.\")"
  },
  {
   "cell_type": "markdown",
   "id": "single-neuron-class",
   "metadata": {},
   "source": [
    "## Build a Single Neuron\n",
    "\n",
    "Let's implement our first neuron class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-neuron",
   "metadata": {},
   "outputs": [],
   "source": "class SingleNeuron:\n    \"\"\"A single neuron: the building block of neural networks\"\"\"\n    \n    def __init__(self, activation='tanh'):\n        \"\"\"Initialize neuron with random weights\"\"\"\n        # TODO: Initialize weight and bias with random values\n        self.weight = _____________  # Random weight\n        self.bias = _____________    # Random bias\n        \n        # Choose activation function\n        if activation == 'sigmoid':\n            self.activation = sigmoid\n        elif activation == 'relu':\n            self.activation = relu\n        else:\n            self.activation = tanh\n    \n    def forward(self, x):\n        \"\"\"Forward pass: compute neuron output\"\"\"\n        # Step 1: Linear combination (just like Module 1!)\n        linear_output = _____________  # TODO: Compute w*x + b\n        \n        # Step 2: Apply activation function (the new part!)\n        return self.activation(linear_output)\n    \n    def __repr__(self):\n        return f\"Neuron(w={self.weight:.3f}, b={self.bias:.3f})\"\n\n# Create and test a single neuron\nneuron = SingleNeuron(activation='tanh')\nprint(f\"Created neuron: {neuron}\")\n\n# Test it on some data\nx_test = np.linspace(-3, 3, 100)\ny_neuron = neuron.forward(x_test)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x_test, y_neuron, 'r-', linewidth=2, label=f'Single Neuron Output')\nplt.xlabel('Input (x)')\nplt.ylabel('Output (y)')\nplt.title('What One Neuron Can Do')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\nprint(f\"One neuron creates a smooth curve!\")\nprint(f\"   But can it approximate our complex target function? Let's see...\")"
  },
  {
   "cell_type": "markdown",
   "id": "single-neuron-limits",
   "metadata": {},
   "source": [
    "## Test Single Neuron vs. Target Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-neuron-test",
   "metadata": {},
   "outputs": [],
   "source": "# Compare single neuron to our target function\nplt.figure(figsize=(10, 5))\nplt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target Function', alpha=0.8)\nplt.plot(x_demo, neuron.forward(x_demo), 'r--', linewidth=2, label='Single Neuron')\nplt.xlabel('Input (x)')\nplt.ylabel('Output (y)')\nplt.title('Single Neuron vs. Complex Target Function')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Observation: One neuron can't capture the complexity!\")\nprint(\"Solution: Add more neurons and combine their outputs!\")"
  },
  {
   "cell_type": "markdown",
   "id": "student-question-2",
   "metadata": {},
   "source": [
    "**Discussion:** *What do you think will happen if we add more neurons and combine them?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shallow-network-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 15–25 min: Add Neurons & Watch Approximation Improve\n",
    "\n",
    "**Goal:** Build a shallow neural network and see the Universal Approximation Theorem in action\n",
    "\n",
    "## Shallow Neural Network Architecture\n",
    "\n",
    "A shallow neural network combines multiple neurons:\n",
    "**y = w₁σ(w₁₁x + b₁) + w₂σ(w₁₂x + b₂) + ... + wₙσ(w₁ₙx + bₙ)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shallow-network-class",
   "metadata": {},
   "outputs": [],
   "source": "class ShallowNeuralNetwork:\n    \"\"\"A shallow neural network with multiple neurons in one hidden layer\"\"\"\n    \n    def __init__(self, num_neurons=3, activation='tanh'):\n        \"\"\"Initialize network with specified number of neurons\"\"\"\n        self.num_neurons = num_neurons\n        \n        # Hidden layer weights and biases (input to hidden)\n        # TODO: Create arrays of random weights and biases for hidden layer\n        self.hidden_weights = _____________\n        self.hidden_biases = _____________\n        \n        # Output layer weights (hidden to output)\n        # TODO: Create array of random weights for output layer\n        self.output_weights = _____________\n        \n        # Choose activation function\n        if activation == 'sigmoid':\n            self.activation = sigmoid\n            self.activation_derivative = lambda x: sigmoid(x) * (1 - sigmoid(x))\n        elif activation == 'relu':\n            self.activation = relu\n            self.activation_derivative = lambda x: (x > 0).astype(float)\n        else:\n            self.activation = tanh\n            self.activation_derivative = lambda x: 1 - np.tanh(x)**2\n    \n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        # Step 1: Compute hidden layer outputs\n        # Each neuron: σ(w_i * x + b_i)\n        hidden_linear = self.hidden_weights[:, np.newaxis] * x + self.hidden_biases[:, np.newaxis]\n        hidden_outputs = self.activation(hidden_linear)\n        \n        # Step 2: Combine hidden outputs with output weights\n        # Final output: sum of weighted hidden outputs\n        # TODO: Compute final output as weighted sum\n        final_output = _____________\n        \n        return final_output, hidden_outputs, hidden_linear\n    \n    def predict(self, x):\n        \"\"\"Simple prediction without returning hidden outputs\"\"\"\n        output, _, _ = self.forward(x)\n        return output\n    \n    def train(self, x_train, y_train, learning_rate=0.01, epochs=1000, verbose=True):\n        \"\"\"Train the network using gradient descent\n        \n        Args:\n            x_train: Training inputs\n            y_train: Training targets\n            learning_rate: Step size for gradient descent\n            epochs: Number of training iterations\n            verbose: Whether to print training progress\n            \n        Returns:\n            loss_history: List of loss values during training\n        \"\"\"\n        loss_history = []\n        \n        for epoch in range(epochs):\n            # Forward pass\n            output, hidden_outputs, hidden_linear = self.forward(x_train)\n            \n            # Calculate loss (Mean Squared Error)\n            # TODO: Compute mean squared error\n            loss = _____________\n            loss_history.append(loss)\n            \n            # Backward pass - compute gradients\n            # Output layer gradients\n            output_error = output - y_train\n            # TODO: Compute gradients for output weights\n            output_weights_grad = _____________\n            \n            # Hidden layer gradients (chain rule!)\n            hidden_error = output_error[np.newaxis, :] * self.output_weights[:, np.newaxis]\n            hidden_activation_grad = self.activation_derivative(hidden_linear)\n            # TODO: Compute gradients for hidden weights and biases\n            hidden_weights_grad = _____________\n            hidden_biases_grad = _____________\n            \n            # Update parameters using gradient descent\n            # TODO: Update all parameters (weights and biases)\n            self.output_weights -= _____________\n            self.hidden_weights -= _____________\n            self.hidden_biases -= _____________\n            \n            # Print progress\n            if verbose and (epoch + 1) % (epochs // 5) == 0:\n                print(f\"   Epoch {epoch + 1:4d}/{epochs}: Loss = {loss:.6f}\")\n        \n        if verbose:\n            print(f\"Training complete! Final loss: {loss:.6f}\")\n        \n        return loss_history\n    \n    def __repr__(self):\n        return f\"ShallowNN({self.num_neurons} neurons)\"\n\n# Test with different numbers of neurons\nneuron_counts = [1, 3, 5, 10]\nnetworks = {}\n\nfor n in neuron_counts:\n    networks[n] = ShallowNeuralNetwork(num_neurons=n, activation='tanh')\n    print(f\"Created network with {n} neurons\")\n\nprint(\"\\nNeural networks ready to test!\")"
  },
  {
   "cell_type": "markdown",
   "id": "approximation-comparison",
   "metadata": {},
   "source": [
    "## Watch Approximation Power Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximation-demo",
   "metadata": {},
   "outputs": [],
   "source": "# Compare networks with different numbers of neurons\nplt.figure(figsize=(15, 10))\n\nfor i, n_neurons in enumerate(neuron_counts):\n    plt.subplot(2, 2, i+1)\n    \n    # Get network prediction\n    network = networks[n_neurons]\n    y_pred = network.predict(x_demo)\n    \n    # Plot target vs prediction\n    plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target Function', alpha=0.8)\n    plt.plot(x_demo, y_pred, 'r--', linewidth=2, label=f'Network ({n_neurons} neurons)')\n    \n    # Calculate and display error\n    # TODO: Compute mean squared error\n    mse = _____________\n    plt.title(f'{n_neurons} Neurons - MSE: {mse:.4f}')\n    plt.xlabel('Input (x)')\n    plt.ylabel('Output (y)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"UNIVERSAL APPROXIMATION IN ACTION:\")\nprint(\"   → More neurons = better approximation\")\nprint(\"   → With enough neurons, we can approximate ANY function!\")\nprint(\"   → This is the theoretical foundation of neural networks\")"
  },
  {
   "cell_type": "markdown",
   "id": "wk29k70pnec",
   "source": "## The Power of Training: Before vs After\n\n**Goal:** See how training transforms random networks into function approximators!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kkqx5vxaq",
   "source": "# Create training data\nx_train = np.linspace(-2, 2, 100)\ny_train = complex_function(x_train)\n\n# Compare untrained vs trained networks\nplt.figure(figsize=(15, 10))\n\nneuron_counts_train = [3, 5, 10]\nfor i, n_neurons in enumerate(neuron_counts_train):\n    # Create two identical networks\n    untrained_network = ShallowNeuralNetwork(num_neurons=n_neurons, activation='tanh')\n    trained_network = ShallowNeuralNetwork(num_neurons=n_neurons, activation='tanh')\n    \n    # Copy weights to make them identical initially\n    trained_network.hidden_weights = untrained_network.hidden_weights.copy()\n    trained_network.hidden_biases = untrained_network.hidden_biases.copy()\n    trained_network.output_weights = untrained_network.output_weights.copy()\n    \n    # Get untrained prediction\n    y_untrained = untrained_network.predict(x_demo)\n    \n    # Train the network\n    print(f\"\\nTraining {n_neurons}-neuron network...\")\n    # TODO: Train the network on our data\n    loss_history = _____________\n    \n    # Get trained prediction\n    y_trained = trained_network.predict(x_demo)\n    \n    # Plot results\n    plt.subplot(2, 3, i+1)\n    plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target', alpha=0.8)\n    plt.plot(x_demo, y_untrained, 'r--', linewidth=2, label='Untrained', alpha=0.7)\n    plt.title(f'{n_neurons} Neurons - BEFORE Training')\n    plt.xlabel('Input (x)')\n    plt.ylabel('Output (y)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(2, 3, i+4)\n    plt.plot(x_demo, y_demo, 'b-', linewidth=2, label='Target', alpha=0.8)\n    plt.plot(x_demo, y_trained, 'g-', linewidth=2, label='Trained', alpha=0.8)\n    \n    # TODO: Calculate final MSE after training\n    final_mse = _____________\n    plt.title(f'{n_neurons} Neurons - AFTER Training\\nMSE: {final_mse:.4f}')\n    plt.xlabel('Input (x)')\n    plt.ylabel('Output (y)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTRAINING RESULTS:\")\nprint(\"   → Random networks: Poor approximation\")\nprint(\"   → Trained networks: Excellent approximation!\")\nprint(\"   → This is the magic of gradient descent!\")\nprint(\"   → More neurons + training = Universal Approximation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4cpswj4nt33",
   "source": "## Watch the Learning Process",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "204ugzrsoi4h",
   "source": "# Demonstrate the learning process with different network sizes\nplt.figure(figsize=(12, 8))\n\n# Train networks and collect learning curves\nlearning_curves = {}\nfinal_performances = {}\n\nfor n_neurons in [1, 3, 5, 10]:\n    print(f\"Training {n_neurons}-neuron network...\")\n    \n    # Create and train network\n    network = ShallowNeuralNetwork(num_neurons=n_neurons, activation='tanh')\n    # TODO: Train the network and collect loss history\n    loss_history = _____________\n    \n    learning_curves[n_neurons] = loss_history\n    final_performances[n_neurons] = loss_history[-1]\n\n# Plot learning curves\nplt.subplot(2, 2, 1)\nfor n_neurons, losses in learning_curves.items():\n    plt.plot(losses, label=f'{n_neurons} neurons', linewidth=2)\nplt.xlabel('Training Epoch')\nplt.ylabel('Loss (MSE)')\nplt.title('Learning Curves: Loss vs Training Time')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\n# Plot final performance vs network size\nplt.subplot(2, 2, 2)\nneurons = list(final_performances.keys())\nfinal_losses = list(final_performances.values())\nplt.bar(neurons, final_losses, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\nplt.xlabel('Number of Neurons')\nplt.ylabel('Final Loss (MSE)')\nplt.title('Final Performance vs Network Size')\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\n# Show parameter count vs performance\nplt.subplot(2, 2, 3)\nparam_counts = [2*n + n for n in neurons]  # 2*n weights + n biases\nplt.scatter(param_counts, final_losses, c=['red', 'orange', 'green', 'blue'], s=100)\nfor i, n in enumerate(neurons):\n    plt.annotate(f'{n}n', (param_counts[i], final_losses[i]), xytext=(5, 5), \n                textcoords='offset points')\nplt.xlabel('Total Parameters')\nplt.ylabel('Final Loss (MSE)')\nplt.title('Parameters vs Performance')\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\n# Show convergence comparison\nplt.subplot(2, 2, 4)\n# Plot only the first 200 epochs for clarity\nfor n_neurons, losses in learning_curves.items():\n    epochs_to_show = min(200, len(losses))\n    plt.plot(range(epochs_to_show), losses[:epochs_to_show], \n             label=f'{n_neurons} neurons', linewidth=2)\nplt.xlabel('Training Epoch (First 200)')\nplt.ylabel('Loss (MSE)')\nplt.title('Early Training: Convergence Speed')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"LEARNING INSIGHTS:\")\nprint(\"   More neurons → Lower final loss\")\nprint(\"   More parameters → Better approximation\")\nprint(\"   All networks learn, but at different rates\")\nprint(\"   Diminishing returns: 10 neurons vs 5 neurons\")\nprint(\"\\nKEY TAKEAWAY: Universal Approximation Theorem in action!\")\nprint(\"   With enough neurons and proper training, we can approximate any function!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "individual-neuron-contributions",
   "metadata": {},
   "source": [
    "## Visualize Individual Neuron Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neuron-contributions",
   "metadata": {},
   "outputs": [],
   "source": "# Show how individual neurons contribute to the final output\nnetwork_5 = networks[5]  # Use 5-neuron network\nfinal_output, hidden_outputs, _ = network_5.forward(x_demo)  # Fix: unpack all 3 values\n\nplt.figure(figsize=(12, 8))\n\n# Top plot: Individual neuron outputs\nplt.subplot(2, 1, 1)\ncolors = ['red', 'green', 'blue', 'orange', 'purple']\nfor i in range(5):\n    # TODO: Compute weighted output for each neuron\n    weighted_output = _____________\n    plt.plot(x_demo, weighted_output, color=colors[i], linewidth=2, \n             label=f'Neuron {i+1} (w={network_5.output_weights[i]:.2f})', alpha=0.7)\n\nplt.title('Individual Weighted Neuron Contributions')\nplt.xlabel('Input (x)')\nplt.ylabel('Neuron Output')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Bottom plot: Sum of all neurons vs target\nplt.subplot(2, 1, 2)\nplt.plot(x_demo, y_demo, 'b-', linewidth=3, label='Target Function')\nplt.plot(x_demo, final_output, 'r--', linewidth=2, label='Sum of All Neurons')\nplt.title('Final Network Output (Sum of Individual Contributions)')\nplt.xlabel('Input (x)')\nplt.ylabel('Output (y)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Insight: Neural networks work by combining simple building blocks!\")\nprint(\"   Each neuron captures a different 'feature' of the target function.\")\nprint(\"   The final output is just a weighted sum of these features.\")"
  },
  {
   "cell_type": "markdown",
   "id": "student-question-3",
   "metadata": {},
   "source": [
    "**Discussion:** *How is this similar to building with LEGO blocks? What happens if we want even more accuracy?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameter-scaling",
   "metadata": {},
   "source": [
    "## Parameter Scaling Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter-count",
   "metadata": {},
   "outputs": [],
   "source": "# Show how parameter count scales with neurons\ndef count_parameters(num_neurons):\n    \"\"\"Count total parameters in our shallow network\"\"\"\n    # TODO: Calculate total number of parameters\n    # Hidden layer: weights + biases\n    # Output layer: weights\n    hidden_params = _____________  # weights + biases for hidden layer\n    output_params = _____________  # output weights\n    return hidden_params + output_params\n\n# Calculate parameters for different network sizes\nneuron_range = np.arange(1, 21)\nparam_counts = [count_parameters(n) for n in neuron_range]\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(neuron_range, param_counts, 'bo-', linewidth=2, markersize=6)\nplt.xlabel('Number of Neurons')\nplt.ylabel('Total Parameters')\nplt.title('Parameter Count vs. Network Size')\nplt.grid(True, alpha=0.3)\n\n# Show approximation quality vs parameters\nplt.subplot(1, 2, 2)\ntest_neurons = [1, 3, 5, 10, 15]\nmse_scores = []\n\nfor n in test_neurons:\n    net = ShallowNeuralNetwork(num_neurons=n, activation='tanh')\n    pred = net.predict(x_demo)\n    mse = np.mean((y_demo - pred)**2)\n    mse_scores.append(mse)\n\nplt.plot([count_parameters(n) for n in test_neurons], mse_scores, 'ro-', linewidth=2, markersize=6)\nplt.xlabel('Number of Parameters')\nplt.ylabel('Mean Squared Error')\nplt.title('Approximation Quality vs. Parameters')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"PARAMETER SCALING INSIGHTS:\")\nprint(f\"   • 1 neuron:  {count_parameters(1)} parameters\")\nprint(f\"   • 10 neurons: {count_parameters(10)} parameters\")\nprint(f\"   • 100 neurons: {count_parameters(100)} parameters\")\nprint(\"   • More parameters = more flexibility = better approximation\")\nprint(\"   • Modern neural networks have MILLIONS or BILLIONS of parameters!\")"
  },
  {
   "cell_type": "markdown",
   "id": "gradient-descent-connection",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 25–30 min: Connect to Gradient Descent & Next Steps\n",
    "\n",
    "**Goal:** Bridge today's implementation to Module 1 concepts and preview what's coming\n",
    "\n",
    "## The Optimization Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-preview",
   "metadata": {},
   "outputs": [],
   "source": "# Connect to Module 1: Same optimization, more parameters\nprint(\"CONNECTION TO MODULE 1:\")\nprint(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\nprint(\"Linear Regression (Module 1):\")\nprint(\"   • Function: y = ax + b\")\nprint(\"   • Parameters: 2 (a, b)\")\nprint(\"   • Optimization: Gradient descent\")\nprint(\"\")\nprint(\"Neural Network (Today):\")\nprint(\"   • Function: y = Σ wᵢσ(wᵢᵢx + bᵢ)\")\nprint(f\"   • Parameters: {count_parameters(5)} (for 5 neurons)\")\nprint(\"   • Optimization: SAME gradient descent!\")\nprint(\"\")\nprint(\"Key insight: Neural networks are just more complex functions\")\nprint(\"   that we optimize using the same mathematical principles!\")\n\n# Show parameter landscape visualization\nprint(\"\\nOPTIMIZATION LANDSCAPE:\")\nprint(\"   • Linear regression: 2D parameter space (easy to visualize)\")\nprint(\"   • Neural networks: High-dimensional space (harder but same idea)\")\nprint(\"   • Gradient descent: Still walks downhill to find minimum!\")"
  },
  {
   "cell_type": "markdown",
   "id": "whats-next",
   "metadata": {},
   "source": [
    "## What's Coming Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-next-topics",
   "metadata": {},
   "outputs": [],
   "source": "# Preview upcoming neural network topics\nprint(\"COMING SOON IN MODULE 2:\")\nprint(\"═══════════════════════════════\")\nprint(\"Backpropagation:\")\nprint(\"   • How to efficiently compute gradients for ALL parameters\")\nprint(\"   • The algorithm that makes neural network training practical\")\nprint(\"\")\nprint(\"Deep Networks:\")\nprint(\"   • Multiple hidden layers: shallow → deep\")\nprint(\"   • Why depth matters for complex problems\")\nprint(\"\")\nprint(\"Real Applications:\")\nprint(\"   • Image classification\")\nprint(\"   • Natural language processing\")\nprint(\"   • Game playing (like AlphaGo)\")\nprint(\"\")\nprint(\"Practical Tools:\")\nprint(\"   • PyTorch and TensorFlow\")\nprint(\"   • GPU acceleration\")\nprint(\"   • Best practices for training\")\nprint(\"\")\nprint(\"Remember: Today's simple network is the foundation for ALL of this!\")"
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways & Skills Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skills-checklist",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== TODAY'S NEURAL NETWORK SKILLS ===\")\nprint(\"✓ Universal Approximation Theorem - understand the theory\")\nprint(\"✓ Activation functions - the source of non-linearity\")\nprint(\"✓ Shallow network implementation - from scratch with NumPy\")\nprint(\"✓ Parameter scaling - more neurons = more approximation power\")\nprint(\"✓ Function decomposition - complex functions from simple parts\")\nprint(\"✓ Connection to optimization - same gradient descent principles\")\nprint(\"\")\nprint(\"YOU NOW UNDERSTAND:\")\nprint(\"   • Why neural networks are powerful (Universal Approximation)\")\nprint(\"   • How they work internally (weighted combinations of neurons)\")\nprint(\"   • Why we need many parameters (flexibility for complex functions)\")\nprint(\"   • How they connect to classical ML (gradient descent optimization)\")\nprint(\"\")\nprint(\"READY FOR: Backpropagation, deep networks, and real applications!\")"
  },
  {
   "cell_type": "markdown",
   "id": "final-challenge",
   "metadata": {},
   "source": [
    "## Mini Challenge (Your Turn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-challenge-code",
   "metadata": {},
   "outputs": [],
   "source": "# Challenge: Try different activation functions and see the effect\nprint(\"YOUR TURN - MINI CHALLENGE:\")\nprint(\"Try creating networks with different activation functions\")\nprint(\"and see how they affect the approximation quality!\")\nprint(\"\")\nprint(\"Experiment with:\")\nprint(\"• sigmoid vs tanh vs relu\")\nprint(\"• Different numbers of neurons\")\nprint(\"• Different target functions\")\nprint(\"\")\nprint(\"Which combination works best for different types of functions?\")\n\n# TODO: Create and test different networks\n# Example:\n# relu_network = ShallowNeuralNetwork(num_neurons=5, activation='relu')\n# sigmoid_network = ShallowNeuralNetwork(num_neurons=5, activation='sigmoid')\n# Compare their outputs!\n\n# YOUR CODE HERE:"
  },
  {
   "cell_type": "markdown",
   "id": "pdet990z2y",
   "source": "---\n\n# Extension: Multi-Input, Multi-Output Networks\n\n**Goal:** Build realistic neural networks that handle multiple inputs and outputs\n\n## From 1D to Multi-Dimensional\n\nReal neural networks typically have:\n- **Multiple inputs**: Features like age, income, temperature, pixel values\n- **Multiple outputs**: Classifications, predictions, or multi-task objectives\n\n**Architecture:** 2 inputs → 3 hidden neurons → 2 outputs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bvl4gs1vbmm",
   "source": "class MultiInputOutputNetwork:\n    \"\"\"A shallow neural network with multiple inputs and outputs\"\"\"\n    \n    def __init__(self, input_size=2, hidden_size=3, output_size=2, activation='tanh'):\n        \"\"\"Initialize network with specified architecture\"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Hidden layer: input_size → hidden_size\n        # Each hidden neuron connects to ALL inputs\n        # TODO: Create weight matrix and bias vector for hidden layer\n        self.W1 = _____________  # Shape: (hidden_size, input_size)\n        self.b1 = _____________  # Shape: (hidden_size, 1)\n        \n        # Output layer: hidden_size → output_size\n        # Each output connects to ALL hidden neurons\n        # TODO: Create weight matrix and bias vector for output layer\n        self.W2 = _____________  # Shape: (output_size, hidden_size)\n        self.b2 = _____________  # Shape: (output_size, 1)\n        \n        # Choose activation function\n        if activation == 'sigmoid':\n            self.activation = sigmoid\n        elif activation == 'relu':\n            self.activation = relu\n        else:\n            self.activation = tanh\n    \n    def forward(self, X):\n        \"\"\"Forward pass through the network\n        \n        Args:\n            X: Input matrix of shape (input_size, num_samples)\n        \n        Returns:\n            output: Network output of shape (output_size, num_samples)\n            hidden: Hidden activations of shape (hidden_size, num_samples)\n        \"\"\"\n        # Ensure X is 2D\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        \n        # Hidden layer: Z1 = W1 @ X + b1, A1 = σ(Z1)\n        # TODO: Compute hidden layer linear combination\n        Z1 = _____________\n        A1 = self.activation(Z1)\n        \n        # Output layer: Z2 = W2 @ A1 + b2\n        # TODO: Compute output layer linear combination\n        Z2 = _____________\n        \n        return Z2, A1\n    \n    def predict(self, X):\n        \"\"\"Simple prediction interface\"\"\"\n        output, _ = self.forward(X)\n        return output\n    \n    def count_parameters(self):\n        \"\"\"Count total number of trainable parameters\"\"\"\n        # TODO: Calculate total parameters\n        w1_params = _____________\n        w2_params = _____________\n        return w1_params + w2_params\n    \n    def __repr__(self):\n        return f\"MultiNN({self.input_size}→{self.hidden_size}→{self.output_size})\"\n\n# Create our 2→3→2 network (will work once TODOs are completed)\n# multi_network = MultiInputOutputNetwork(input_size=2, hidden_size=3, output_size=2)\n# print(f\"Created network: {multi_network}\")\nprint(\"Complete the TODOs above to create the multi-input/output network!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "isl3o67495q",
   "source": "## Your Challenge: Real-World Applications\n\n**Your Task:** Think about how our 2→3→2 network maps to real-world problems!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z7qc12hqq3n",
   "source": "# Real-world application examples\nprint(\"REAL-WORLD APPLICATIONS:\")\nprint(\"Our 2→3→2 network architecture appears everywhere in practice!\")\nprint(\"\")\n\n# TODO: Complete these application examples\napplications = [\n    {\n        'domain': 'Medical Diagnosis',\n        'inputs': ['Blood Pressure', '_____________'],  # TODO: Add second input\n        'outputs': ['_____________', 'Treatment Urgency'],  # TODO: Add first output\n        'hidden': 'Patient Health Patterns'\n    },\n    {\n        'domain': 'Stock Trading',\n        'inputs': ['_____________', 'Volume'],  # TODO: Add first input\n        'outputs': ['Buy Signal', '_____________'],  # TODO: Add second output\n        'hidden': 'Market Trend Features'\n    },\n    {\n        'domain': 'Weather Prediction',\n        'inputs': ['Temperature', '_____________'],  # TODO: Add second input\n        'outputs': ['_____________', 'Wind Speed'],  # TODO: Add first output\n        'hidden': 'Climate Dynamics'\n    }\n]\n\nfor i, app in enumerate(applications, 1):\n    print(f\"{i}. {app['domain']}:\")\n    print(f\"   Inputs: {', '.join(app['inputs'])}\")\n    print(f\"   Hidden: {app['hidden']}\")\n    print(f\"   Outputs: {', '.join(app['outputs'])}\")\n    print()\n\nprint(\"YOUR TURN:\")\nprint(\"Think of your own 2-input, 2-output application!\")\nprint(\"What problem could you solve with this architecture?\")\n\n# TODO: Create your own application example\nmy_application = {\n    'domain': '_____________',  # Your application domain\n    'inputs': ['_____________', '_____________'],  # Your two inputs\n    'outputs': ['_____________', '_____________'],  # Your two outputs\n    'description': '_____________'  # Brief description\n}\n\nprint(f\"\\nYOUR APPLICATION:\")\nprint(f\"Domain: {my_application['domain']}\")\nprint(f\"Inputs: {', '.join(my_application['inputs'])}\")\nprint(f\"Outputs: {', '.join(my_application['outputs'])}\")\nprint(f\"Description: {my_application['description']}\")\n\nprint(\"\\nKEY INSIGHTS:\")\nprint(\"✓ Neural networks scale from simple demos to real applications\")\nprint(\"✓ Same mathematical principles work across all domains\")\nprint(\"✓ Matrix operations enable efficient computation\")\nprint(\"✓ Understanding fundamentals prepares you for any framework\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}