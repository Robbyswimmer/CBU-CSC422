{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Coding Assignment 2: Classification Models\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Student ID:** [Your Student ID]  \n",
    "**Date:** [Today's Date]  \n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to your second machine learning assignment! In this notebook, you'll explore classification algorithms by implementing three fundamental approaches: K-Nearest Neighbors, Logistic Regression, and Naive Bayes. You'll apply these to predict Titanic passenger survival using real historical data.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand classification vs regression\n",
    "- Implement and compare classification algorithms\n",
    "- Apply data preprocessing for classification tasks\n",
    "- Interpret model results and performance metrics\n",
    "- Reflect on algorithm strengths and weaknesses\n",
    "\n",
    "**Estimated Time:** 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundation (30 minutes)\n",
    "\n",
    "Before implementing algorithms, let's understand the mathematics behind classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 1.1 Understanding Classification\n",
    "\n",
    "**Classification** predicts discrete categories or classes, unlike regression which predicts continuous values.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Decision Boundary**: Line/surface separating different classes\n",
    "- **Probability**: Many classifiers output probabilities for each class\n",
    "- **Binary vs Multiclass**: Two classes vs multiple classes\n",
    "\n",
    "**Examples:**\n",
    "- Email spam detection (spam/not spam)\n",
    "- Medical diagnosis (disease/healthy)\n",
    "- Image recognition (cat/dog/bird)\n",
    "- **Our task**: Titanic survival (survived/died)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple visualization of classification vs regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "X_sample = np.random.rand(100, 1) * 10\n",
    "y_regression = 2 * X_sample.flatten() + np.random.normal(0, 1, 100)\n",
    "y_classification = (X_sample.flatten() + np.random.normal(0, 1, 100) > 5).astype(int)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Regression plot\n",
    "ax1.scatter(X_sample, y_regression, alpha=0.6, color='blue')\n",
    "ax1.set_xlabel('X (Feature)')\n",
    "ax1.set_ylabel('y (Continuous Target)')\n",
    "ax1.set_title('Regression: Predicting Continuous Values')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Classification plot\n",
    "colors = ['red' if y == 0 else 'green' for y in y_classification]\n",
    "ax2.scatter(X_sample, y_classification, alpha=0.6, c=colors)\n",
    "ax2.set_xlabel('X (Feature)')\n",
    "ax2.set_ylabel('y (Class: 0 or 1)')\n",
    "ax2.set_title('Classification: Predicting Categories')\n",
    "ax2.set_yticks([0, 1])\n",
    "ax2.set_yticklabels(['Class 0', 'Class 1'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Differences:\")\n",
    "print(\"Regression: Predicts continuous values (prices, temperatures, etc.)\")\n",
    "print(\"Classification: Predicts discrete categories (classes, labels, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 1.2 Distance Metrics for K-Nearest Neighbors\n",
    "\n",
    "KNN classifies points based on the class of their nearest neighbors. The key is measuring **distance**.\n",
    "\n",
    "**Euclidean Distance** between points A and B:\n",
    "$$d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + ... + (z_1 - z_2)^2}$$\n",
    "\n",
    "For n-dimensional space:\n",
    "$$d = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two points.\n",
    "    \n",
    "    Parameters:\n",
    "    point1 (array): First point coordinates\n",
    "    point2 (array): Second point coordinates\n",
    "    \n",
    "    Returns:\n",
    "    distance (float): Euclidean distance\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the Euclidean distance\n",
    "    # Hint: Use np.sqrt() and np.sum() with squared differences\n",
    "    distance = None  # Replace None with your code\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Test the distance function\n",
    "# TODO: Uncomment the lines below after implementing the function\n",
    "# point_a = np.array([1, 2])\n",
    "# point_b = np.array([4, 6])\n",
    "# test_distance = euclidean_distance(point_a, point_b)\n",
    "# print(f\"Distance between {point_a} and {point_b}: {test_distance:.2f}\")\n",
    "# print(f\"Expected: 5.00 (3² + 4² = 9 + 16 = 25, √25 = 5)\")\n",
    "\n",
    "print(\"TODO: Implement the euclidean_distance function above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression Mathematics\n",
    "\n",
    "Unlike linear regression, logistic regression predicts probabilities using the **sigmoid function**.\n",
    "\n",
    "**Sigmoid Function:**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where $z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "**Key Properties:**\n",
    "- Output is always between 0 and 1 (perfect for probabilities)\n",
    "- S-shaped curve\n",
    "- Coefficients (β) represent log-odds changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    z (array): Input values\n",
    "    \n",
    "    Returns:\n",
    "    sigmoid_z (array): Sigmoid of input\n",
    "    \"\"\"\n",
    "    # TODO: Implement the sigmoid function: 1 / (1 + exp(-z))\n",
    "    # Hint: Use np.exp() for exponential\n",
    "    sigmoid_z = None  # Replace None with your code\n",
    "    \n",
    "    return sigmoid_z\n",
    "\n",
    "# TODO: Uncomment the visualization code below after implementing sigmoid\n",
    "# z_values = np.linspace(-10, 10, 100)\n",
    "# sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(z_values, sigmoid_values, 'b-', linewidth=2, label='Sigmoid Function')\n",
    "# plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Boundary (0.5)')\n",
    "# plt.axvline(x=0, color='r', linestyle='--', alpha=0.7)\n",
    "# plt.xlabel('z (Linear Combination)')\n",
    "# plt.ylabel('σ(z) (Probability)')\n",
    "# plt.title('Sigmoid Function: Converting Linear Output to Probabilities')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.ylim(-0.1, 1.1)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Key Points:\")\n",
    "# print(f\"σ(-∞) ≈ {sigmoid(-100):.6f} (approaches 0)\")\n",
    "# print(f\"σ(0) = {sigmoid(0):.6f} (exactly 0.5)\")\n",
    "# print(f\"σ(+∞) ≈ {sigmoid(100):.6f} (approaches 1)\")\n",
    "\n",
    "print(\"TODO: Implement the sigmoid function above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 1.4 Naive Bayes Foundation\n",
    "\n",
    "Naive Bayes applies **Bayes' Theorem** with a \"naive\" assumption of feature independence.\n",
    "\n",
    "**Bayes' Theorem:**\n",
    "$$P(Class|Features) = \\frac{P(Features|Class) \\times P(Class)}{P(Features)}$$\n",
    "\n",
    "**Naive Assumption:**\n",
    "All features are independent, so:\n",
    "$$P(x_1, x_2, ..., x_n|Class) = P(x_1|Class) \\times P(x_2|Class) \\times ... \\times P(x_n|Class)$$\n",
    "\n",
    "**Example**: For Titanic survival:\n",
    "- P(Survived | Age=25, Class=1st, Gender=Female)\n",
    "- = P(Age=25 | Survived) × P(Class=1st | Survived) × P(Gender=Female | Survived) × P(Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 2: Dataset & Exploration (20 minutes)\n",
    "\n",
    "Let's load and explore the famous Titanic dataset to understand our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "try:\n",
    "    # Try loading from seaborn first\n",
    "    titanic = sns.load_dataset('titanic')\n",
    "    print(\"Loaded Titanic dataset from seaborn\")\n",
    "except:\n",
    "    # Fallback: create a sample dataset if seaborn data is not available\n",
    "    print(\"Creating sample Titanic dataset...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 891\n",
    "    \n",
    "    # Create synthetic Titanic-like data\n",
    "    titanic = pd.DataFrame({\n",
    "        'survived': np.random.choice([0, 1], n_samples, p=[0.62, 0.38]),\n",
    "        'pclass': np.random.choice([1, 2, 3], n_samples, p=[0.24, 0.21, 0.55]),\n",
    "        'sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),\n",
    "        'age': np.random.normal(30, 12, n_samples).clip(0.5, 80),\n",
    "        'sibsp': np.random.poisson(0.5, n_samples).clip(0, 8),\n",
    "        'parch': np.random.poisson(0.4, n_samples).clip(0, 6),\n",
    "        'fare': np.random.lognormal(3, 1, n_samples).clip(0, 500),\n",
    "        'embarked': np.random.choice(['S', 'C', 'Q'], n_samples, p=[0.72, 0.19, 0.09])\n",
    "    })\n",
    "    \n",
    "    # Add some missing values to make it realistic\n",
    "    titanic.loc[np.random.choice(titanic.index, 177, replace=False), 'age'] = np.nan\n",
    "    titanic.loc[np.random.choice(titanic.index, 2, replace=False), 'embarked'] = np.nan\n",
    "\n",
    "print(f\"\\nDataset shape: {titanic.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(titanic.head())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 2.1 Understanding Our Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable: survival\n",
    "print(\"Survival Statistics:\")\n",
    "survival_counts = titanic['survived'].value_counts().sort_index()\n",
    "print(survival_counts)\n",
    "print(f\"\\nSurvival Rate: {titanic['survived'].mean():.3f} ({titanic['survived'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Visualize survival distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "survival_counts.plot(kind='bar', color=['red', 'green'], alpha=0.7)\n",
    "plt.title('Survival Distribution')\n",
    "plt.xlabel('Survived (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Died', 'Survived'], rotation=0)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pie(survival_counts.values, labels=['Died', 'Survived'], colors=['red', 'green'], alpha=0.7, autopct='%1.1f%%')\n",
    "plt.title('Survival Proportions')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Survival by passenger class\n",
    "survival_by_class = titanic.groupby('pclass')['survived'].mean()\n",
    "survival_by_class.plot(kind='bar', color='skyblue', alpha=0.7)\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSurvival by Class:\")\n",
    "for pclass in [1, 2, 3]:\n",
    "    rate = survival_by_class[pclass]\n",
    "    print(f\"Class {pclass}: {rate:.3f} ({rate*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 2.2 Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore key relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Survival by Gender\n",
    "axes[0, 0].set_title('Survival by Gender')\n",
    "survival_gender = pd.crosstab(titanic['sex'], titanic['survived'])\n",
    "survival_gender.div(survival_gender.sum(axis=1), axis=0).plot(kind='bar', ax=axes[0, 0], color=['red', 'green'])\n",
    "axes[0, 0].set_ylabel('Proportion')\n",
    "axes[0, 0].legend(['Died', 'Survived'])\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Survival by Age\n",
    "axes[0, 1].set_title('Age Distribution by Survival')\n",
    "titanic[titanic['survived']==0]['age'].hist(alpha=0.7, color='red', label='Died', bins=20, ax=axes[0, 1])\n",
    "titanic[titanic['survived']==1]['age'].hist(alpha=0.7, color='green', label='Survived', bins=20, ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Survival by Fare\n",
    "axes[1, 0].set_title('Fare Distribution by Survival')\n",
    "titanic[titanic['survived']==0]['fare'].hist(alpha=0.7, color='red', label='Died', bins=30, ax=axes[1, 0])\n",
    "titanic[titanic['survived']==1]['fare'].hist(alpha=0.7, color='green', label='Survived', bins=30, ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Fare')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_xlim(0, 200)  # Limit x-axis for better visualization\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Family size vs Survival\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "family_survival = titanic.groupby('family_size')['survived'].mean()\n",
    "axes[1, 1].bar(family_survival.index, family_survival.values, alpha=0.7, color='skyblue')\n",
    "axes[1, 1].set_title('Survival Rate by Family Size')\n",
    "axes[1, 1].set_xlabel('Family Size')\n",
    "axes[1, 1].set_ylabel('Survival Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some insights\n",
    "print(\"Key Insights:\")\n",
    "print(f\"Female survival rate: {titanic[titanic['sex']=='female']['survived'].mean():.3f}\")\n",
    "print(f\"Male survival rate: {titanic[titanic['sex']=='male']['survived'].mean():.3f}\")\n",
    "print(f\"Average age of survivors: {titanic[titanic['survived']==1]['age'].mean():.1f}\")\n",
    "print(f\"Average age of non-survivors: {titanic[titanic['survived']==0]['age'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing (15 minutes)\n",
    "\n",
    "Before training models, we need to prepare our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 3.1 Handle Missing Values and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values by column:\")\n",
    "missing_values = titanic.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Create a working copy\n",
    "df = titanic.copy()\n",
    "\n",
    "# Handle missing age values by filling with median\n",
    "df['age'].fillna(df['age'].median(), inplace=True)\n",
    "\n",
    "# Handle missing embarked values by filling with mode (most common)\n",
    "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Select features for our models\n",
    "# We'll use: pclass, sex, age, sibsp, parch, fare, embarked\n",
    "feature_columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "target_column = 'survived'\n",
    "\n",
    "print(f\"\\nSelected features: {feature_columns}\")\n",
    "print(f\"Target variable: {target_column}\")\n",
    "print(f\"\\nNo missing values after preprocessing: {df[feature_columns + [target_column]].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 3.2 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target arrays\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target_column].copy()\n",
    "\n",
    "print(\"Before encoding:\")\n",
    "print(X.dtypes)\n",
    "print(f\"\\nSample of categorical data:\")\n",
    "print(X[['sex', 'embarked']].head())\n",
    "\n",
    "# TODO: Encode the 'sex' column (male=0, female=1)\n",
    "# Hint: You can use pd.get_dummies() or manual mapping\n",
    "X['sex_encoded'] = None  # Replace None with your code to encode sex\n",
    "\n",
    "# TODO: Encode the 'embarked' column using one-hot encoding\n",
    "# Hint: Use pd.get_dummies(X['embarked'], prefix='embarked')\n",
    "embarked_encoded = None  # Replace None with your code\n",
    "\n",
    "# Add encoded columns and remove original categorical columns\n",
    "# TODO: Uncomment these lines after implementing the encoding above\n",
    "# X = pd.concat([X, embarked_encoded], axis=1)\n",
    "# X = X.drop(['sex', 'embarked'], axis=1)\n",
    "\n",
    "print(\"\\nTODO: Implement the encoding steps above\")\n",
    "# TODO: Uncomment the lines below after implementing encoding\n",
    "# print(f\"\\nAfter encoding:\")\n",
    "# print(f\"Shape: {X.shape}\")\n",
    "# print(f\"Columns: {list(X.columns)}\")\n",
    "# print(f\"\\nFirst few rows:\")\n",
    "# print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### 3.3 Train-Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: After implementing encoding above, uncomment this entire cell\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "# print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "# print(f\"Training survival rate: {y_train.mean():.3f}\")\n",
    "# print(f\"Test survival rate: {y_test.mean():.3f}\")\n",
    "\n",
    "# # TODO: Apply feature scaling using StandardScaler\n",
    "# # Initialize the scaler\n",
    "# scaler = None  # Replace None with StandardScaler()\n",
    "\n",
    "# # TODO: Fit the scaler on training data and transform both train and test\n",
    "# # Hint: Use fit_transform for training data, transform for test data\n",
    "# X_train_scaled = None  # Replace None with your code\n",
    "# X_test_scaled = None   # Replace None with your code\n",
    "\n",
    "# print(f\"\\nOriginal feature ranges:\")\n",
    "# print(f\"Age: {X_train['age'].min():.1f} to {X_train['age'].max():.1f}\")\n",
    "# print(f\"Fare: {X_train['fare'].min():.1f} to {X_train['fare'].max():.1f}\")\n",
    "\n",
    "# # TODO: Uncomment after implementing scaling\n",
    "# # print(f\"\\nAfter scaling:\")\n",
    "# # print(f\"Mean: {X_train_scaled.mean():.6f}\")\n",
    "# # print(f\"Std: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "print(\"TODO: Implement encoding first, then uncomment this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Part 4: Model Implementations (60 minutes)\n",
    "\n",
    "Now let's implement and compare three classification algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 4.1 K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete this section after preprocessing is done\n",
    "\n",
    "# print(\"=== K-Nearest Neighbors Classification ===\")\n",
    "\n",
    "# # TODO: Create a KNN classifier with k=5\n",
    "# # Hint: Use KNeighborsClassifier from sklearn\n",
    "# knn_model = None  # Replace None with KNeighborsClassifier(n_neighbors=?)\n",
    "\n",
    "# # TODO: Train the model\n",
    "# # Hint: Use the fit method with scaled training data\n",
    "# # knn_model.fit(?, ?)\n",
    "\n",
    "# # TODO: Make predictions on test set\n",
    "# y_pred_knn = None  # Replace None with predictions\n",
    "\n",
    "# # TODO: Calculate accuracy\n",
    "# knn_accuracy = None  # Replace None with accuracy calculation\n",
    "\n",
    "# print(f\"KNN Accuracy (k=5): {knn_accuracy:.4f}\")\n",
    "\n",
    "# # Test different values of k\n",
    "# print(f\"\\nTesting different k values:\")\n",
    "# k_values = [1, 3, 5, 7, 10, 15, 20]\n",
    "# k_accuracies = []\n",
    "\n",
    "# for k in k_values:\n",
    "#     # TODO: Create, train, and evaluate KNN with different k values\n",
    "#     knn_temp = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn_temp.fit(X_train_scaled, y_train)\n",
    "#     y_pred_temp = knn_temp.predict(X_test_scaled)\n",
    "#     accuracy = accuracy_score(y_test, y_pred_temp)\n",
    "#     k_accuracies.append(accuracy)\n",
    "#     print(f\"k={k}: {accuracy:.4f}\")\n",
    "\n",
    "# # Plot k vs accuracy\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(k_values, k_accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "# plt.xlabel('k (Number of Neighbors)')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('KNN Performance vs k Value')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.xticks(k_values)\n",
    "# plt.show()\n",
    "\n",
    "# best_k = k_values[np.argmax(k_accuracies)]\n",
    "# print(f\"\\nBest k value: {best_k} with accuracy: {max(k_accuracies):.4f}\")\n",
    "\n",
    "print(\"TODO: Complete preprocessing first, then implement KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete this section after preprocessing is done\n",
    "\n",
    "# print(\"=== Logistic Regression Classification ===\")\n",
    "\n",
    "# # TODO: Create a Logistic Regression classifier\n",
    "# # Hint: Use LogisticRegression from sklearn with random_state=42\n",
    "# lr_model = None  # Replace None with LogisticRegression\n",
    "\n",
    "# # TODO: Train the model\n",
    "# # lr_model.fit(?, ?)\n",
    "\n",
    "# # TODO: Make predictions\n",
    "# y_pred_lr = None  # Replace None with predictions\n",
    "# y_proba_lr = None  # Replace None with predict_proba for probabilities\n",
    "\n",
    "# # TODO: Calculate accuracy\n",
    "# lr_accuracy = None  # Replace None with accuracy calculation\n",
    "\n",
    "# print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# # Analyze coefficients\n",
    "# print(f\"\\nLogistic Regression Coefficients:\")\n",
    "# feature_names = X.columns\n",
    "# coefficients = lr_model.coef_[0]\n",
    "\n",
    "# # TODO: Sort features by absolute coefficient value and print\n",
    "# coef_df = pd.DataFrame({\n",
    "#     'Feature': feature_names,\n",
    "#     'Coefficient': coefficients,\n",
    "#     'Abs_Coefficient': np.abs(coefficients)\n",
    "# })\n",
    "# coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# print(coef_df)\n",
    "\n",
    "# # Visualize coefficients\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# colors = ['red' if x < 0 else 'green' for x in coef_df['Coefficient']]\n",
    "# plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, alpha=0.7)\n",
    "# plt.xlabel('Coefficient Value')\n",
    "# plt.title('Logistic Regression Coefficients\\n(Positive = Increases Survival Probability)')\n",
    "# plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "# plt.grid(True, alpha=0.3, axis='x')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"TODO: Complete preprocessing first, then implement Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### 4.3 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete this section after preprocessing is done\n",
    "\n",
    "# print(\"=== Naive Bayes Classification ===\")\n",
    "\n",
    "# # TODO: Create a Gaussian Naive Bayes classifier\n",
    "# # Hint: Use GaussianNB from sklearn\n",
    "# nb_model = None  # Replace None with GaussianNB()\n",
    "\n",
    "# # TODO: Train the model\n",
    "# # Note: Naive Bayes can work with or without scaling, but let's use scaled data for consistency\n",
    "# # nb_model.fit(?, ?)\n",
    "\n",
    "# # TODO: Make predictions\n",
    "# y_pred_nb = None  # Replace None with predictions\n",
    "# y_proba_nb = None  # Replace None with predict_proba for probabilities\n",
    "\n",
    "# # TODO: Calculate accuracy\n",
    "# nb_accuracy = None  # Replace None with accuracy calculation\n",
    "\n",
    "# print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "\n",
    "# # Show some example predictions with probabilities\n",
    "# print(f\"\\nExample Predictions (first 10 test samples):\")\n",
    "# print(f\"{'Actual':<8} {'Predicted':<10} {'Prob_Died':<12} {'Prob_Survived':<12} {'Correct':<8}\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# for i in range(min(10, len(y_test))):\n",
    "#     actual = y_test.iloc[i]\n",
    "#     predicted = y_pred_nb[i]\n",
    "#     prob_died = y_proba_nb[i][0]\n",
    "#     prob_survived = y_proba_nb[i][1]\n",
    "#     correct = \"✓\" if actual == predicted else \"✗\"\n",
    "    \n",
    "#     print(f\"{actual:<8} {predicted:<10} {prob_died:<12.3f} {prob_survived:<12.3f} {correct:<8}\")\n",
    "\n",
    "print(\"TODO: Complete preprocessing first, then implement Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Part 5: Model Comparison (20 minutes)\n",
    "\n",
    "Let's compare all three models systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete this section after implementing all models\n",
    "\n",
    "# print(\"=== Model Comparison ===\")\n",
    "\n",
    "# # Collect all results\n",
    "# models = {\n",
    "#     'K-Nearest Neighbors': {\n",
    "#         'model': None,  # TODO: Use your best KNN model\n",
    "#         'predictions': None,  # TODO: Add KNN predictions\n",
    "#         'accuracy': None  # TODO: Add KNN accuracy\n",
    "#     },\n",
    "#     'Logistic Regression': {\n",
    "#         'model': None,  # TODO: Add your LR model\n",
    "#         'predictions': None,  # TODO: Add LR predictions  \n",
    "#         'accuracy': None  # TODO: Add LR accuracy\n",
    "#     },\n",
    "#     'Naive Bayes': {\n",
    "#         'model': None,  # TODO: Add your NB model\n",
    "#         'predictions': None,  # TODO: Add NB predictions\n",
    "#         'accuracy': None  # TODO: Add NB accuracy\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # TODO: Create a comparison table\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Model': models.keys(),\n",
    "#     'Accuracy': [models[model]['accuracy'] for model in models.keys()]\n",
    "# })\n",
    "# comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "# print(comparison_df)\n",
    "\n",
    "# # Visualize model comparison\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Subplot 1: Accuracy comparison\n",
    "# plt.subplot(2, 2, 1)\n",
    "# colors = ['gold', 'silver', 'bronze'][:len(comparison_df)]\n",
    "# bars = plt.bar(comparison_df['Model'], comparison_df['Accuracy'], color=colors, alpha=0.7)\n",
    "# plt.title('Model Accuracy Comparison')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xticks(rotation=15)\n",
    "# plt.ylim(0.7, 0.9)  # Focus on the relevant range\n",
    "\n",
    "# # Add accuracy values on top of bars\n",
    "# for bar, acc in zip(bars, comparison_df['Accuracy']):\n",
    "#     plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "#              f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# # TODO: Add confusion matrices for each model (subplots 2, 3, 4)\n",
    "# for i, (model_name, model_data) in enumerate(models.items(), 2):\n",
    "#     plt.subplot(2, 2, i)\n",
    "#     cm = confusion_matrix(y_test, model_data['predictions'])\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "#                 xticklabels=['Died', 'Survived'], \n",
    "#                 yticklabels=['Died', 'Survived'])\n",
    "#     plt.title(f'{model_name}\\nConfusion Matrix')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('Actual')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Detailed classification reports\n",
    "# print(\"\\n=== Detailed Classification Reports ===\")\n",
    "# for model_name, model_data in models.items():\n",
    "#     print(f\"\\n{model_name}:\")\n",
    "#     print(classification_report(y_test, model_data['predictions'], \n",
    "#                               target_names=['Died', 'Survived']))\n",
    "\n",
    "print(\"TODO: Complete all model implementations first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### 5.1 Cross-Validation Analysis (Optional Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bonus section - implement cross-validation\n",
    "# Uncomment after completing main implementations\n",
    "\n",
    "# print(\"=== Cross-Validation Analysis ===\")\n",
    "\n",
    "# # TODO: Perform 5-fold cross-validation for each model\n",
    "# cv_results = {}\n",
    "\n",
    "# for model_name, model_data in models.items():\n",
    "#     model = model_data['model']\n",
    "#     \n",
    "#     # TODO: Use cross_val_score with cv=5\n",
    "#     cv_scores = None  # Replace with cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "#     \n",
    "#     cv_results[model_name] = {\n",
    "#         'scores': cv_scores,\n",
    "#         'mean': cv_scores.mean(),\n",
    "#         'std': cv_scores.std()\n",
    "#     }\n",
    "#     \n",
    "#     print(f\"{model_name}:\")\n",
    "#     print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "#     print(f\"  Individual Scores: {cv_scores}\")\n",
    "\n",
    "# # Visualize cross-validation results\n",
    "# cv_means = [cv_results[model]['mean'] for model in cv_results.keys()]\n",
    "# cv_stds = [cv_results[model]['std'] for model in cv_results.keys()]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(cv_results.keys(), cv_means, yerr=cv_stds, capsize=10, alpha=0.7)\n",
    "# plt.title('Cross-Validation Results\\n(Mean ± Standard Deviation)')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xticks(rotation=15)\n",
    "# plt.grid(True, alpha=0.3, axis='y')\n",
    "# plt.show()\n",
    "\n",
    "print(\"TODO: Bonus - implement after completing main models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Part 6: Analysis Questions (15 minutes)\n",
    "\n",
    "Now let's analyze your results and reflect on what you've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### 6.1 Model Performance Analysis\n",
    "\n",
    "**TODO: Answer these questions based on your results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "**1. Which model performed best on the Titanic survival prediction? Why might this be the case?**\n",
    "\n",
    "[TODO: Write your answer here. Consider the nature of the data, feature relationships, and algorithm characteristics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "**2. How does changing k in KNN affect the results? What did you observe when testing k=1, 5, 10, 20?**\n",
    "\n",
    "[TODO: Write your observations here. Discuss overfitting vs underfitting, bias-variance tradeoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "**3. What do the logistic regression coefficients tell you about the factors affecting Titanic survival?**\n",
    "\n",
    "[TODO: Analyze the coefficients. Which features have positive vs negative coefficients? What does this mean for survival probability?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "**4. Compare the confusion matrices. Which model makes fewer false positives vs false negatives?**\n",
    "\n",
    "[TODO: Analyze the confusion matrices. Discuss the trade-offs between different types of errors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### 6.2 Algorithm Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "**5. When would you choose each algorithm in practice?**\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "[TODO: Describe scenarios where KNN would be preferred]\n",
    "\n",
    "**Logistic Regression:**\n",
    "[TODO: Describe scenarios where Logistic Regression would be preferred]\n",
    "\n",
    "**Naive Bayes:**\n",
    "[TODO: Describe scenarios where Naive Bayes would be preferred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "**6. What assumptions does each algorithm make? How might these affect performance?**\n",
    "\n",
    "[TODO: Discuss the key assumptions of each algorithm and their implications]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "## Part 7: Critical Reflection (10 minutes)\n",
    "\n",
    "Let's step back and think about the bigger picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "### 7.1 Classification vs Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "**1. How does classification differ from the regression you implemented in CA.01?**\n",
    "\n",
    "**Key Differences:**\n",
    "[TODO: Compare the two problem types, evaluation metrics, output types, etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "**2. Which evaluation metrics are important for classification vs regression?**\n",
    "\n",
    "**Classification Metrics:**\n",
    "[TODO: List and explain classification metrics like accuracy, precision, recall]\n",
    "\n",
    "**Regression Metrics (from CA.01):**\n",
    "[TODO: Recall regression metrics like MSE, R²]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "### 7.2 Real-World Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "**3. Give examples of real-world classification problems for each algorithm:**\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "[TODO: Provide 2-3 real-world examples where KNN would be used]\n",
    "\n",
    "**Logistic Regression:**\n",
    "[TODO: Provide 2-3 real-world examples where Logistic Regression would be used]\n",
    "\n",
    "**Naive Bayes:**\n",
    "[TODO: Provide 2-3 real-world examples where Naive Bayes would be used]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "**4. What ethical considerations should we keep in mind when applying these models to real problems?**\n",
    "\n",
    "[TODO: Discuss bias, fairness, interpretability, and other ethical concerns in classification]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "### 7.3 Limitations and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "**5. What limitations did you encounter with each algorithm?**\n",
    "\n",
    "[TODO: Reflect on challenges, assumptions violations, or performance issues you observed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "**6. How could you improve the Titanic survival predictions?**\n",
    "\n",
    "[TODO: Suggest feature engineering, different algorithms, ensemble methods, etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## Bonus: Advanced Experiments (Optional)\n",
    "\n",
    "If you have extra time, try these experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 1: Feature Engineering\n",
    "# TODO: Create new features like 'Title' from name, 'IsAlone' from family size, etc.\n",
    "\n",
    "print(\"Bonus experiments:\")\n",
    "print(\"1. Extract title from passenger names (Mr, Mrs, Miss, etc.)\")\n",
    "print(\"2. Create 'IsAlone' feature (family_size == 1)\")\n",
    "print(\"3. Create age groups (Child, Adult, Senior)\")\n",
    "print(\"4. Try ensemble methods (combining multiple models)\")\n",
    "print(\"5. Experiment with different train/test splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2: Hyperparameter Tuning\n",
    "# TODO: Use GridSearchCV to find optimal parameters for each model\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# \n",
    "# # Example for KNN\n",
    "# param_grid_knn = {'n_neighbors': range(1, 31)}\n",
    "# grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)\n",
    "# grid_knn.fit(X_train_scaled, y_train)\n",
    "# print(f\"Best KNN parameters: {grid_knn.best_params_}\")\n",
    "\n",
    "print(\"TODO: Implement hyperparameter tuning if you have extra time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-56",
   "metadata": {},
   "source": [
    "## Summary and Submission\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! In this assignment, you have:\n",
    "\n",
    "**Understood classification fundamentals** and how they differ from regression  \n",
    "**Implemented three classification algorithms**: KNN, Logistic Regression, and Naive Bayes  \n",
    "**Applied data preprocessing** including encoding and scaling  \n",
    "**Compared model performance** using multiple metrics  \n",
    "**Interpreted results** and analyzed algorithm characteristics  \n",
    "**Reflected on real-world applications** and ethical considerations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**TODO: Write 2-3 key insights from this assignment:**\n",
    "\n",
    "1. [TODO: Your first key takeaway about classification algorithms]\n",
    "2. [TODO: Your second key takeaway about model comparison or data preprocessing]  \n",
    "3. [TODO: Your third key takeaway about practical applications or limitations]\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "**TODO: Write a brief (100-150 words) final reflection on your experience with classification:**\n",
    "\n",
    "[TODO: Your final reflection here - discuss what surprised you, what was challenging, what you'd like to explore further]\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Complete!** \n",
    "\n",
    "Make sure to:\n",
    "1. Fill in all TODO sections\n",
    "2. Answer all analysis questions\n",
    "3. Save your notebook\n",
    "4. Export as HTML\n",
    "5. Submit both .ipynb and .html files\n",
    "6. Include your name and student ID at the top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
