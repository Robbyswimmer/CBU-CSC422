{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Coding Assignment 3: Neural Networks\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Student ID:** [Your Student ID]  \n",
    "**Date:** [Today's Date]  \n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to your first deep learning assignment! In this notebook, you'll journey from linear models to neural networks by implementing a Multi-Layer Perceptron (MLP) from scratch using NumPy. You'll then compare your implementation with PyTorch, the industry-standard deep learning framework.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand neural network architecture and forward propagation\n",
    "- Implement backpropagation and gradient descent from scratch\n",
    "- Compare different activation functions and architectures\n",
    "- Apply neural networks to real classification problems\n",
    "- Transition from NumPy to PyTorch implementations\n",
    "- Reflect on when to use neural networks vs simpler models\n",
    "\n",
    "**Estimated Time:** 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundation (30 minutes)\n",
    "\n",
    "Neural networks extend the linear models you've implemented by adding layers and non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports (we'll use these later)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - you can install it with: pip install torch\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 1.1 From Linear to Neural Networks\n",
    "\n",
    "**Linear Model (CA1 & CA2):**\n",
    "- Single layer: `y = Wx + b`\n",
    "- Linear decision boundaries\n",
    "- Limited representation power\n",
    "\n",
    "**Neural Network:**\n",
    "- Multiple layers with non-linear activations\n",
    "- Can learn complex, non-linear patterns\n",
    "- Universal approximation capability\n",
    "\n",
    "**Multi-Layer Perceptron (MLP) Architecture:**\n",
    "```\n",
    "Input → Hidden Layer 1 → Hidden Layer 2 → ... → Output\n",
    "  x   →   σ(W₁x + b₁)  →  σ(W₂h₁ + b₂) → ... →  y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the difference between linear and non-linear decision boundaries\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a non-linearly separable dataset (XOR-like problem)\n",
    "n_samples = 200\n",
    "X_nonlinear = np.random.randn(n_samples, 2)\n",
    "y_nonlinear = ((X_nonlinear[:, 0] * X_nonlinear[:, 1]) > 0).astype(int)\n",
    "\n",
    "# Create a linearly separable dataset\n",
    "X_linear = np.random.randn(n_samples, 2)\n",
    "X_linear[y_nonlinear == 1] += [1.5, 1.5]  # Shift one class\n",
    "y_linear = y_nonlinear.copy()\n",
    "\n",
    "# Plot both datasets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Linear problem\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_linear == i\n",
    "    ax1.scatter(X_linear[mask, 0], X_linear[mask, 1], \n",
    "               c=colors[i], alpha=0.6, label=f'Class {i}')\n",
    "ax1.set_title('Linearly Separable Problem\\n(Good for Linear Models)')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Non-linear problem\n",
    "for i in range(2):\n",
    "    mask = y_nonlinear == i\n",
    "    ax2.scatter(X_nonlinear[mask, 0], X_nonlinear[mask, 1], \n",
    "               c=colors[i], alpha=0.6, label=f'Class {i}')\n",
    "ax2.set_title('Non-Linearly Separable Problem\\n(Needs Neural Networks)')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Neural networks can solve problems that linear models cannot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 1.2 Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity, enabling neural networks to learn complex patterns.\n",
    "\n",
    "**Common Activation Functions:**\n",
    "- **Sigmoid**: `σ(x) = 1/(1 + e^(-x))` → Output range [0, 1]\n",
    "- **Tanh**: `tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))` → Output range [-1, 1]\n",
    "- **ReLU**: `ReLU(x) = max(0, x)` → Output range [0, ∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array): Input values\n",
    "    \n",
    "    Returns:\n",
    "    array: Sigmoid of input\n",
    "    \"\"\"\n",
    "    # TODO: Implement sigmoid function: 1 / (1 + exp(-x))\n",
    "    # Hint: Use np.exp() and clip x to prevent overflow\n",
    "    x_clipped = np.clip(x, -500, 500)  # Prevent overflow\n",
    "    return None  # Replace None with your implementation\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array): Input values (pre-activation)\n",
    "    \n",
    "    Returns:\n",
    "    array: Derivative of sigmoid\n",
    "    \"\"\"\n",
    "    # TODO: Implement sigmoid derivative: sigmoid(x) * (1 - sigmoid(x))\n",
    "    s = sigmoid(x)\n",
    "    return None  # Replace None with your implementation\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array): Input values\n",
    "    \n",
    "    Returns:\n",
    "    array: Tanh of input\n",
    "    \"\"\"\n",
    "    # TODO: Implement tanh function\n",
    "    # Hint: Use np.tanh() or (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "    return None  # Replace None with your implementation\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of tanh function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array): Input values (pre-activation)\n",
    "    \n",
    "    Returns:\n",
    "    array: Derivative of tanh\n",
    "    \"\"\"\n",
    "    # TODO: Implement tanh derivative: 1 - tanh(x)^2\n",
    "    t = tanh(x)\n",
    "    return None  # Replace None with your implementation\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array): Input values\n",
    "    \n",
    "    Returns:\n",
    "    array: ReLU of input\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU function: max(0, x)\n",
    "    # Hint: Use np.maximum(0, x)\n",
    "    return None  # Replace None with your implementation\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array): Input values (pre-activation)\n",
    "    \n",
    "    Returns:\n",
    "    array: Derivative of ReLU\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU derivative: 1 if x > 0, else 0\n",
    "    # Hint: Use (x > 0).astype(float)\n",
    "    return None  # Replace None with your implementation\n",
    "\n",
    "print(\"TODO: Implement the activation functions above\")\n",
    "\n",
    "# TODO: Uncomment the visualization code below after implementing the functions\n",
    "# # Visualize activation functions\n",
    "# x_range = np.linspace(-5, 5, 100)\n",
    "# \n",
    "# fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "# \n",
    "# # Activation functions\n",
    "# axes[0, 0].plot(x_range, sigmoid(x_range), 'b-', linewidth=2, label='Sigmoid')\n",
    "# axes[0, 0].set_title('Sigmoid Activation')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "# axes[0, 0].set_ylabel('Output')\n",
    "# \n",
    "# axes[0, 1].plot(x_range, tanh(x_range), 'r-', linewidth=2, label='Tanh')\n",
    "# axes[0, 1].set_title('Tanh Activation')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "# \n",
    "# axes[0, 2].plot(x_range, relu(x_range), 'g-', linewidth=2, label='ReLU')\n",
    "# axes[0, 2].set_title('ReLU Activation')\n",
    "# axes[0, 2].grid(True, alpha=0.3)\n",
    "# \n",
    "# # Derivatives\n",
    "# axes[1, 0].plot(x_range, sigmoid_derivative(x_range), 'b--', linewidth=2, label='Sigmoid Derivative')\n",
    "# axes[1, 0].set_title('Sigmoid Derivative')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "# axes[1, 0].set_xlabel('Input')\n",
    "# axes[1, 0].set_ylabel('Derivative')\n",
    "# \n",
    "# axes[1, 1].plot(x_range, tanh_derivative(x_range), 'r--', linewidth=2, label='Tanh Derivative')\n",
    "# axes[1, 1].set_title('Tanh Derivative')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "# axes[1, 1].set_xlabel('Input')\n",
    "# \n",
    "# axes[1, 2].plot(x_range, relu_derivative(x_range), 'g--', linewidth=2, label='ReLU Derivative')\n",
    "# axes[1, 2].set_title('ReLU Derivative')\n",
    "# axes[1, 2].grid(True, alpha=0.3)\n",
    "# axes[1, 2].set_xlabel('Input')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"Key Properties:\")\n",
    "# print(\"• Sigmoid: Smooth, bounded [0,1], but can cause vanishing gradients\")\n",
    "# print(\"• Tanh: Smooth, bounded [-1,1], zero-centered\")\n",
    "# print(\"• ReLU: Simple, unbounded, helps with vanishing gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 1.3 Forward Propagation\n",
    "\n",
    "Forward propagation computes the output of a neural network given an input.\n",
    "\n",
    "**For a 2-layer network:**\n",
    "1. **Hidden Layer**: `h = σ(W₁x + b₁)`\n",
    "2. **Output Layer**: `y = σ(W₂h + b₂)`\n",
    "\n",
    "**Matrix Dimensions:**\n",
    "- Input: `(batch_size, input_features)`\n",
    "- W₁: `(input_features, hidden_size)`\n",
    "- b₁: `(hidden_size,)`\n",
    "- W₂: `(hidden_size, output_size)`\n",
    "- b₂: `(output_size,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_example():\n",
    "    \"\"\"\n",
    "    Example of forward propagation through a simple 2-layer network.\n",
    "    \"\"\"\n",
    "    # Network architecture: 2 inputs → 3 hidden → 1 output\n",
    "    \n",
    "    # Sample input (batch_size=4, input_features=2)\n",
    "    X = np.array([[1.0, 2.0],\n",
    "                  [2.0, 1.0],\n",
    "                  [0.5, 1.5],\n",
    "                  [1.5, 0.5]])\n",
    "    \n",
    "    # Initialize random weights and biases\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(2, 3) * 0.5  # (input_size, hidden_size)\n",
    "    b1 = np.random.randn(3) * 0.5      # (hidden_size,)\n",
    "    W2 = np.random.randn(3, 1) * 0.5   # (hidden_size, output_size)\n",
    "    b2 = np.random.randn(1) * 0.5      # (output_size,)\n",
    "    \n",
    "    print(f\"Input X shape: {X.shape}\")\n",
    "    print(f\"W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "    print(f\"W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "    \n",
    "    # TODO: Implement forward pass\n",
    "    # Step 1: Compute hidden layer pre-activation\n",
    "    z1 = None  # Replace None with X @ W1 + b1\n",
    "    \n",
    "    # Step 2: Apply activation function to get hidden layer output\n",
    "    # Use tanh activation (implement tanh first above!)\n",
    "    h1 = None  # Replace None with activation function\n",
    "    \n",
    "    # Step 3: Compute output layer pre-activation\n",
    "    z2 = None  # Replace None with h1 @ W2 + b2\n",
    "    \n",
    "    # Step 4: Apply activation function to get final output\n",
    "    # Use sigmoid for binary classification\n",
    "    output = None  # Replace None with activation function\n",
    "    \n",
    "    # TODO: Uncomment the print statements after implementation\n",
    "    # print(f\"\\nForward pass results:\")\n",
    "    # print(f\"Hidden layer output shape: {h1.shape}\")\n",
    "    # print(f\"Final output shape: {output.shape}\")\n",
    "    # print(f\"Sample outputs: {output.flatten()[:3]}\")\n",
    "    \n",
    "    return X, (W1, b1, W2, b2), (z1, h1, z2, output)\n",
    "\n",
    "# Run the example\n",
    "print(\"Forward Propagation Example:\")\n",
    "example_data = forward_pass_example()\n",
    "print(\"TODO: Complete the forward pass implementation above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 2: From-Scratch MLP Implementation (45 minutes)\n",
    "\n",
    "Now let's build a complete Multi-Layer Perceptron class with forward propagation, backpropagation, and training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the MLP.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of input features\n",
    "        hidden_sizes (list): List of hidden layer sizes [h1, h2, ...]\n",
    "        output_size (int): Number of output classes\n",
    "        activation (str): Activation function ('relu', 'tanh', 'sigmoid')\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Set activation functions\n",
    "        self._set_activation_functions()\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def _set_activation_functions(self):\n",
    "        \"\"\"Set activation function and its derivative.\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            self.activation_func = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif self.activation == 'tanh':\n",
    "            self.activation_func = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.activation_func = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        # Create layer sizes list: [input, hidden1, hidden2, ..., output]\n",
    "        layer_sizes = [self.input_size] + self.hidden_sizes + [self.output_size]\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # TODO: Initialize weights using Xavier initialization\n",
    "            # Xavier initialization: weights ~ Normal(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i + 1]\n",
    "            \n",
    "            # Replace None with proper weight initialization\n",
    "            weight = None  # np.random.normal(0, np.sqrt(2 / (fan_in + fan_out)), (fan_in, fan_out))\n",
    "            bias = None    # np.zeros(fan_out)\n",
    "            \n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (final_output, activations, pre_activations)\n",
    "        \"\"\"\n",
    "        activations = [X]  # Store all activations for backprop\n",
    "        pre_activations = []  # Store pre-activation values for backprop\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        # Forward pass through all layers\n",
    "        for i, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "            # TODO: Compute pre-activation (linear transformation)\n",
    "            z = None  # Replace None with current_input @ weight + bias\n",
    "            pre_activations.append(z)\n",
    "            \n",
    "            # Apply activation function\n",
    "            if i < len(self.weights) - 1:  # Hidden layers\n",
    "                a = self.activation_func(z)\n",
    "            else:  # Output layer (use sigmoid for binary classification)\n",
    "                a = sigmoid(z)\n",
    "            \n",
    "            activations.append(a)\n",
    "            current_input = a\n",
    "        \n",
    "        return activations[-1], activations, pre_activations\n",
    "    \n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        \"\"\"\n",
    "        Backward propagation (compute gradients).\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Input data\n",
    "        y (array): True labels\n",
    "        activations (list): Activations from forward pass\n",
    "        pre_activations (list): Pre-activations from forward pass\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (weight_gradients, bias_gradients)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # batch size\n",
    "        weight_grads = []\n",
    "        bias_grads = []\n",
    "        \n",
    "        # Start with output layer error\n",
    "        # For binary cross-entropy with sigmoid: dL/dz = (y_pred - y_true)\n",
    "        delta = activations[-1] - y\n",
    "        \n",
    "        # Backpropagate through all layers (reverse order)\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # TODO: Compute gradients for current layer\n",
    "            # Weight gradient: dL/dW = (1/m) * a_prev^T @ delta\n",
    "            dW = None  # Replace None with gradient computation\n",
    "            \n",
    "            # Bias gradient: dL/db = (1/m) * sum(delta, axis=0)\n",
    "            db = None  # Replace None with gradient computation\n",
    "            \n",
    "            weight_grads.append(dW)\n",
    "            bias_grads.append(db)\n",
    "            \n",
    "            # Compute delta for previous layer (if not input layer)\n",
    "            if i > 0:\n",
    "                # TODO: Backpropagate error to previous layer\n",
    "                # delta_prev = (delta @ W^T) * activation_derivative(z_prev)\n",
    "                delta_prev = None  # Replace None with backpropagation computation\n",
    "                delta = delta_prev\n",
    "        \n",
    "        # Reverse gradients to match forward order\n",
    "        weight_grads.reverse()\n",
    "        bias_grads.reverse()\n",
    "        \n",
    "        return weight_grads, bias_grads\n",
    "    \n",
    "    def update_parameters(self, weight_grads, bias_grads, learning_rate):\n",
    "        \"\"\"Update weights and biases using gradients.\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            # TODO: Update weights and biases\n",
    "            # weight -= learning_rate * gradient\n",
    "            self.weights[i] -= None  # Replace None with update rule\n",
    "            self.biases[i] -= None   # Replace None with update rule\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (array): True labels\n",
    "        y_pred (array): Predicted probabilities\n",
    "        \n",
    "        Returns:\n",
    "        float: Average loss\n",
    "        \"\"\"\n",
    "        # TODO: Implement binary cross-entropy loss\n",
    "        # BCE = -[y*log(p) + (1-y)*log(1-p)]\n",
    "        # Clip predictions to prevent log(0)\n",
    "        epsilon = 1e-7\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = None  # Replace None with loss computation\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, learning_rate=0.01, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Training data\n",
    "        y (array): Training labels\n",
    "        epochs (int): Number of training epochs\n",
    "        learning_rate (float): Learning rate\n",
    "        batch_size (int): Mini-batch size\n",
    "        verbose (bool): Print training progress\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data for each epoch\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_X = X_shuffled[i:i+batch_size]\n",
    "                batch_y = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                output, activations, pre_activations = self.forward(batch_X)\n",
    "                \n",
    "                # Compute loss\n",
    "                batch_loss = self.compute_loss(batch_y, output)\n",
    "                epoch_loss += batch_loss * len(batch_X)\n",
    "                \n",
    "                # Backward pass\n",
    "                weight_grads, bias_grads = self.backward(batch_X, batch_y, activations, pre_activations)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(weight_grads, bias_grads, learning_rate)\n",
    "            \n",
    "            # Record average epoch loss\n",
    "            avg_loss = epoch_loss / n_samples\n",
    "            self.loss_history.append(avg_loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % (epochs // 10) == 0:\n",
    "                print(f\"Epoch {epoch + 1:4d}/{epochs}: Loss = {avg_loss:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Input data\n",
    "        \n",
    "        Returns:\n",
    "        array: Predicted class probabilities\n",
    "        \"\"\"\n",
    "        output, _, _ = self.forward(X)\n",
    "        return output\n",
    "    \n",
    "    def predict_classes(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Input data\n",
    "        threshold (float): Classification threshold\n",
    "        \n",
    "        Returns:\n",
    "        array: Predicted class labels\n",
    "        \"\"\"\n",
    "        probabilities = self.predict(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "\n",
    "print(\"MLP class defined!\")\n",
    "print(\"TODO: Complete the missing implementations in the methods above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 2.1 Test Your MLP Implementation\n",
    "\n",
    "Let's test your implementation on the non-linearly separable dataset we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run this code after implementing the MLP class\n",
    "\n",
    "# # Test on the XOR-like problem\n",
    "# print(\"Testing MLP on Non-Linear Problem:\")\n",
    "# \n",
    "# # Prepare data\n",
    "# X_test = X_nonlinear\n",
    "# y_test = y_nonlinear.reshape(-1, 1)  # Reshape for compatibility\n",
    "# \n",
    "# # Split data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=42)\n",
    "# \n",
    "# # Scale features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# \n",
    "# print(f\"Training set: {X_train_scaled.shape}, Validation set: {X_val_scaled.shape}\")\n",
    "# \n",
    "# # Create and train MLP\n",
    "# mlp = MLP(input_size=2, hidden_sizes=[8, 4], output_size=1, activation='relu')\n",
    "# print(f\"\\nMLP Architecture: 2 → {mlp.hidden_sizes} → 1\")\n",
    "# \n",
    "# # Train the model\n",
    "# mlp.fit(X_train_scaled, y_train, epochs=1000, learning_rate=0.1, batch_size=16, verbose=True)\n",
    "# \n",
    "# # Make predictions\n",
    "# train_pred = mlp.predict_classes(X_train_scaled)\n",
    "# val_pred = mlp.predict_classes(X_val_scaled)\n",
    "# \n",
    "# # Calculate accuracy\n",
    "# train_accuracy = accuracy_score(y_train, train_pred)\n",
    "# val_accuracy = accuracy_score(y_val, val_pred)\n",
    "# \n",
    "# print(f\"\\nResults:\")\n",
    "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"TODO: Complete the MLP implementation first, then uncomment the code above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment this visualization after implementing and testing the MLP\n",
    "\n",
    "# # Visualize training progress and decision boundary\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# \n",
    "# # Plot 1: Loss curve\n",
    "# axes[0].plot(mlp.loss_history, 'b-', linewidth=2)\n",
    "# axes[0].set_title('Training Loss')\n",
    "# axes[0].set_xlabel('Epoch')\n",
    "# axes[0].set_ylabel('Loss')\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "# \n",
    "# # Plot 2: Decision boundary\n",
    "# def plot_decision_boundary(model, X, y, scaler, ax, title):\n",
    "#     \"\"\"Plot decision boundary for 2D data.\"\"\"\n",
    "#     h = 0.01\n",
    "#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#                          np.arange(y_min, y_max, h))\n",
    "#     \n",
    "#     grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "#     grid_points_scaled = scaler.transform(grid_points)\n",
    "#     Z = model.predict(grid_points_scaled)\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     \n",
    "#     ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "#     colors = ['red', 'blue']\n",
    "#     for i in range(2):\n",
    "#         mask = y.flatten() == i\n",
    "#         ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.8, label=f'Class {i}')\n",
    "#     ax.set_title(title)\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "# \n",
    "# plot_decision_boundary(mlp, X_test, y_test, scaler, axes[1], 'MLP Decision Boundary')\n",
    "# \n",
    "# # Plot 3: Confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_val, val_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[2])\n",
    "# axes[2].set_title('Confusion Matrix')\n",
    "# axes[2].set_xlabel('Predicted')\n",
    "# axes[2].set_ylabel('Actual')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"TODO: Visualization will be available after MLP implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 3: Real-World Application (30 minutes)\n",
    "\n",
    "Now let's apply your neural network to a real dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real dataset\n",
    "print(\"Loading Wine Dataset...\")\n",
    "wine_data = load_wine()\n",
    "X_wine = wine_data.data\n",
    "y_wine = wine_data.target\n",
    "\n",
    "print(f\"Dataset shape: {X_wine.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_wine))}\")\n",
    "print(f\"Feature names: {wine_data.feature_names[:5]}... (showing first 5)\")\n",
    "print(f\"Class distribution: {np.bincount(y_wine)}\")\n",
    "\n",
    "# For binary classification, let's use classes 0 vs 1\n",
    "binary_mask = y_wine != 2  # Remove class 2\n",
    "X_binary = X_wine[binary_mask]\n",
    "y_binary = y_wine[binary_mask]\n",
    "\n",
    "print(f\"\\nBinary classification dataset:\")\n",
    "print(f\"Shape: {X_binary.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_binary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and splitting\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "wine_scaler = StandardScaler()\n",
    "X_train_wine_scaled = wine_scaler.fit_transform(X_train_wine)\n",
    "X_test_wine_scaled = wine_scaler.transform(X_test_wine)\n",
    "\n",
    "# Reshape targets for compatibility\n",
    "y_train_wine = y_train_wine.reshape(-1, 1)\n",
    "y_test_wine = y_test_wine.reshape(-1, 1)\n",
    "\n",
    "print(f\"Training set: {X_train_wine_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_wine_scaled.shape}\")\n",
    "print(f\"Feature ranges after scaling:\")\n",
    "print(f\"  Min: {X_train_wine_scaled.min():.3f}\")\n",
    "print(f\"  Max: {X_train_wine_scaled.max():.3f}\")\n",
    "print(f\"  Mean: {X_train_wine_scaled.mean():.3f}\")\n",
    "print(f\"  Std: {X_train_wine_scaled.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 3.1 Experiment with Different Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete this section after implementing the MLP\n",
    "\n",
    "# # Test different architectures\n",
    "# architectures = {\n",
    "#     'Shallow': [16],\n",
    "#     'Deep Narrow': [8, 8],\n",
    "#     'Wide': [32],\n",
    "#     'Deep Wide': [32, 16, 8]\n",
    "# }\n",
    "# \n",
    "# results = {}\n",
    "# \n",
    "# print(\"Testing Different Architectures:\")\n",
    "# print(\"=\" * 50)\n",
    "# \n",
    "# for name, hidden_sizes in architectures.items():\n",
    "#     print(f\"\\nTraining {name} network: {X_train_wine_scaled.shape[1]} → {hidden_sizes} → 1\")\n",
    "#     \n",
    "#     # TODO: Create and train MLP with current architecture\n",
    "#     mlp_arch = MLP(\n",
    "#         input_size=X_train_wine_scaled.shape[1],\n",
    "#         hidden_sizes=hidden_sizes,\n",
    "#         output_size=1,\n",
    "#         activation='relu'\n",
    "#     )\n",
    "#     \n",
    "#     # Train the model\n",
    "#     mlp_arch.fit(X_train_wine_scaled, y_train_wine, \n",
    "#                  epochs=500, learning_rate=0.01, batch_size=16, verbose=False)\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     train_pred = mlp_arch.predict_classes(X_train_wine_scaled)\n",
    "#     test_pred = mlp_arch.predict_classes(X_test_wine_scaled)\n",
    "#     \n",
    "#     train_acc = accuracy_score(y_train_wine, train_pred)\n",
    "#     test_acc = accuracy_score(y_test_wine, test_pred)\n",
    "#     \n",
    "#     results[name] = {\n",
    "#         'train_accuracy': train_acc,\n",
    "#         'test_accuracy': test_acc,\n",
    "#         'final_loss': mlp_arch.loss_history[-1],\n",
    "#         'parameters': sum(w.size for w in mlp_arch.weights) + sum(b.size for b in mlp_arch.biases)\n",
    "#     }\n",
    "#     \n",
    "#     print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "#     print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "#     print(f\"  Parameters:     {results[name]['parameters']}\")\n",
    "# \n",
    "# # TODO: Create comparison visualization\n",
    "# # Uncomment the visualization code below\n",
    "\n",
    "print(\"TODO: Complete MLP implementation first, then uncomment architecture comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment this visualization after running the architecture comparison\n",
    "\n",
    "# # Visualize architecture comparison\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# \n",
    "# names = list(results.keys())\n",
    "# train_accs = [results[name]['train_accuracy'] for name in names]\n",
    "# test_accs = [results[name]['test_accuracy'] for name in names]\n",
    "# param_counts = [results[name]['parameters'] for name in names]\n",
    "# \n",
    "# # Plot 1: Accuracy comparison\n",
    "# x = np.arange(len(names))\n",
    "# width = 0.35\n",
    "# \n",
    "# axes[0].bar(x - width/2, train_accs, width, label='Train', alpha=0.8)\n",
    "# axes[0].bar(x + width/2, test_accs, width, label='Test', alpha=0.8)\n",
    "# axes[0].set_xlabel('Architecture')\n",
    "# axes[0].set_ylabel('Accuracy')\n",
    "# axes[0].set_title('Accuracy by Architecture')\n",
    "# axes[0].set_xticks(x)\n",
    "# axes[0].set_xticklabels(names, rotation=45)\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "# \n",
    "# # Plot 2: Parameters vs Performance\n",
    "# axes[1].scatter(param_counts, test_accs, s=100, alpha=0.7)\n",
    "# for i, name in enumerate(names):\n",
    "#     axes[1].annotate(name, (param_counts[i], test_accs[i]), \n",
    "#                     xytext=(5, 5), textcoords='offset points')\n",
    "# axes[1].set_xlabel('Number of Parameters')\n",
    "# axes[1].set_ylabel('Test Accuracy')\n",
    "# axes[1].set_title('Model Complexity vs Performance')\n",
    "# axes[1].grid(True, alpha=0.3)\n",
    "# \n",
    "# # Plot 3: Overfitting analysis\n",
    "# overfitting = [train_accs[i] - test_accs[i] for i in range(len(names))]\n",
    "# colors = ['green' if x < 0.05 else 'orange' if x < 0.1 else 'red' for x in overfitting]\n",
    "# axes[2].bar(names, overfitting, color=colors, alpha=0.7)\n",
    "# axes[2].set_xlabel('Architecture')\n",
    "# axes[2].set_ylabel('Train - Test Accuracy')\n",
    "# axes[2].set_title('Overfitting Analysis')\n",
    "# axes[2].set_xticklabels(names, rotation=45)\n",
    "# axes[2].grid(True, alpha=0.3)\n",
    "# axes[2].axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Moderate Overfitting')\n",
    "# axes[2].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='High Overfitting')\n",
    "# axes[2].legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # Summary\n",
    "# best_arch = max(results.keys(), key=lambda x: results[x]['test_accuracy'])\n",
    "# print(f\"\\nBest Architecture: {best_arch}\")\n",
    "# print(f\"Test Accuracy: {results[best_arch]['test_accuracy']:.4f}\")\n",
    "# print(f\"Parameters: {results[best_arch]['parameters']}\")\n",
    "\n",
    "print(\"TODO: Architecture visualization will be available after implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### 3.2 Activation Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete this section after implementing the MLP\n",
    "\n",
    "# # Compare different activation functions\n",
    "# activations = ['relu', 'tanh', 'sigmoid']\n",
    "# activation_results = {}\n",
    "# \n",
    "# print(\"Comparing Activation Functions:\")\n",
    "# print(\"=\" * 40)\n",
    "# \n",
    "# for activation in activations:\n",
    "#     print(f\"\\nTesting {activation.upper()} activation...\")\n",
    "#     \n",
    "#     # TODO: Create MLP with current activation function\n",
    "#     mlp_act = MLP(\n",
    "#         input_size=X_train_wine_scaled.shape[1],\n",
    "#         hidden_sizes=[16, 8],  # Use consistent architecture\n",
    "#         output_size=1,\n",
    "#         activation=activation\n",
    "#     )\n",
    "#     \n",
    "#     # Train the model\n",
    "#     mlp_act.fit(X_train_wine_scaled, y_train_wine,\n",
    "#                 epochs=500, learning_rate=0.01, batch_size=16, verbose=False)\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     test_pred = mlp_act.predict_classes(X_test_wine_scaled)\n",
    "#     test_acc = accuracy_score(y_test_wine, test_pred)\n",
    "#     \n",
    "#     activation_results[activation] = {\n",
    "#         'accuracy': test_acc,\n",
    "#         'loss_history': mlp_act.loss_history\n",
    "#     }\n",
    "#     \n",
    "#     print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "#     print(f\"  Final Loss: {mlp_act.loss_history[-1]:.6f}\")\n",
    "# \n",
    "# # TODO: Visualize activation function comparison\n",
    "# # Uncomment the visualization code below\n",
    "\n",
    "print(\"TODO: Complete MLP implementation first, then uncomment activation comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment this visualization after running the activation comparison\n",
    "\n",
    "# # Visualize activation function comparison\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# \n",
    "# # Plot 1: Learning curves\n",
    "# colors = ['red', 'green', 'blue']\n",
    "# for i, (activation, data) in enumerate(activation_results.items()):\n",
    "#     ax1.plot(data['loss_history'], color=colors[i], linewidth=2, \n",
    "#             label=f'{activation.upper()} (Acc: {data[\"accuracy\"]:.3f})', alpha=0.8)\n",
    "# \n",
    "# ax1.set_xlabel('Epoch')\n",
    "# ax1.set_ylabel('Loss')\n",
    "# ax1.set_title('Learning Curves by Activation Function')\n",
    "# ax1.legend()\n",
    "# ax1.grid(True, alpha=0.3)\n",
    "# ax1.set_yscale('log')\n",
    "# \n",
    "# # Plot 2: Final accuracy comparison\n",
    "# activations_list = list(activation_results.keys())\n",
    "# accuracies = [activation_results[act]['accuracy'] for act in activations_list]\n",
    "# \n",
    "# bars = ax2.bar(activations_list, accuracies, color=colors, alpha=0.7)\n",
    "# ax2.set_xlabel('Activation Function')\n",
    "# ax2.set_ylabel('Test Accuracy')\n",
    "# ax2.set_title('Test Accuracy by Activation Function')\n",
    "# ax2.grid(True, alpha=0.3)\n",
    "# \n",
    "# # Add accuracy values on top of bars\n",
    "# for bar, acc in zip(bars, accuracies):\n",
    "#     ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "#             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # Find best activation\n",
    "# best_activation = max(activation_results.keys(), key=lambda x: activation_results[x]['accuracy'])\n",
    "# print(f\"\\nBest Activation Function: {best_activation.upper()}\")\n",
    "# print(f\"Test Accuracy: {activation_results[best_activation]['accuracy']:.4f}\")\n",
    "\n",
    "print(\"TODO: Activation function visualization will be available after implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Part 4: PyTorch Implementation (20 minutes)\n",
    "\n",
    "Now let's see how easy it is to implement the same network using PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PYTORCH_AVAILABLE:\n",
    "    print(\"PyTorch not available. Please install it with: pip install torch\")\n",
    "    print(\"Skipping PyTorch section...\")\n",
    "else:\n",
    "    print(\"PyTorch is available! Let's implement the same MLP.\")\n",
    "    \n",
    "    class PyTorchMLP(nn.Module):\n",
    "        \"\"\"\n",
    "        PyTorch implementation of MLP.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, input_size, hidden_sizes, output_size, activation='relu'):\n",
    "            super(PyTorchMLP, self).__init__()\n",
    "            \n",
    "            # TODO: Create layers using nn.ModuleList()\n",
    "            layers = []\n",
    "            \n",
    "            # Input to first hidden layer\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(len(hidden_sizes) - 1):\n",
    "                layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            \n",
    "            # Last hidden to output\n",
    "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "            \n",
    "            self.layers = nn.ModuleList(layers)\n",
    "            \n",
    "            # Set activation function\n",
    "            if activation == 'relu':\n",
    "                self.activation = F.relu\n",
    "            elif activation == 'tanh':\n",
    "                self.activation = torch.tanh\n",
    "            elif activation == 'sigmoid':\n",
    "                self.activation = torch.sigmoid\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            Forward pass through the network.\n",
    "            \n",
    "            Parameters:\n",
    "            x (tensor): Input data\n",
    "            \n",
    "            Returns:\n",
    "            tensor: Output predictions\n",
    "            \"\"\"\n",
    "            # TODO: Implement forward pass\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x = layer(x)\n",
    "                \n",
    "                # Apply activation function (except for output layer)\n",
    "                if i < len(self.layers) - 1:  # Hidden layers\n",
    "                    x = self.activation(x)\n",
    "                else:  # Output layer\n",
    "                    x = torch.sigmoid(x)  # Sigmoid for binary classification\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    print(\"PyTorch MLP class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_torch = torch.FloatTensor(X_train_wine_scaled)\n",
    "    y_train_torch = torch.FloatTensor(y_train_wine)\n",
    "    X_test_torch = torch.FloatTensor(X_test_wine_scaled)\n",
    "    y_test_torch = torch.FloatTensor(y_test_wine)\n",
    "    \n",
    "    print(f\"Converted to PyTorch tensors:\")\n",
    "    print(f\"X_train shape: {X_train_torch.shape}\")\n",
    "    print(f\"y_train shape: {y_train_torch.shape}\")\n",
    "    \n",
    "    # Create PyTorch model\n",
    "    pytorch_mlp = PyTorchMLP(\n",
    "        input_size=X_train_torch.shape[1],\n",
    "        hidden_sizes=[16, 8],\n",
    "        output_size=1,\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPyTorch Model Architecture:\")\n",
    "    print(pytorch_mlp)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in pytorch_mlp.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params}\")\n",
    "    \n",
    "    # TODO: Set up training components\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = optim.Adam(pytorch_mlp.parameters(), lr=0.01)\n",
    "    \n",
    "    print(f\"\\nTraining setup:\")\n",
    "    print(f\"Loss function: {criterion}\")\n",
    "    print(f\"Optimizer: {optimizer}\")\n",
    "else:\n",
    "    print(\"Skipping PyTorch implementation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Train PyTorch model\n",
    "    print(\"Training PyTorch MLP...\")\n",
    "    \n",
    "    epochs = 500\n",
    "    batch_size = 16\n",
    "    pytorch_losses = []\n",
    "    \n",
    "    # Create data loader\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    pytorch_mlp.train()  # Set to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # TODO: Implement training loop\n",
    "            # 1. Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. Forward pass\n",
    "            outputs = pytorch_mlp(batch_X)\n",
    "            \n",
    "            # 3. Compute loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # 4. Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        # Record average epoch loss\n",
    "        avg_loss = epoch_loss / len(train_dataset)\n",
    "        pytorch_losses.append(avg_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % (epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch + 1:4d}/{epochs}: Loss = {avg_loss:.6f}\")\n",
    "    \n",
    "    print(\"\\nPyTorch training completed!\")\n",
    "else:\n",
    "    print(\"Skipping PyTorch training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Evaluate PyTorch model\n",
    "    pytorch_mlp.eval()  # Set to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        train_pred_torch = pytorch_mlp(X_train_torch)\n",
    "        test_pred_torch = pytorch_mlp(X_test_torch)\n",
    "        \n",
    "        # Convert to class predictions\n",
    "        train_pred_classes = (train_pred_torch >= 0.5).float()\n",
    "        test_pred_classes = (test_pred_torch >= 0.5).float()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        train_acc_torch = (train_pred_classes == y_train_torch).float().mean().item()\n",
    "        test_acc_torch = (test_pred_classes == y_test_torch).float().mean().item()\n",
    "    \n",
    "    print(f\"PyTorch Results:\")\n",
    "    print(f\"Training Accuracy: {train_acc_torch:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc_torch:.4f}\")\n",
    "    print(f\"Final Loss: {pytorch_losses[-1]:.6f}\")\n",
    "    \n",
    "    # TODO: Compare with your NumPy implementation\n",
    "    # Uncomment after implementing your MLP\n",
    "    # print(f\"\\nComparison with NumPy Implementation:\")\n",
    "    # print(f\"NumPy Test Accuracy: {test_accuracy:.4f}\")\n",
    "    # print(f\"PyTorch Test Accuracy: {test_acc_torch:.4f}\")\n",
    "    # print(f\"Difference: {abs(test_accuracy - test_acc_torch):.4f}\")\n",
    "else:\n",
    "    print(\"PyTorch not available for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Visualize PyTorch vs NumPy comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Learning curves comparison\n",
    "    # TODO: Add your NumPy loss history for comparison\n",
    "    ax1.plot(pytorch_losses, 'b-', linewidth=2, label='PyTorch', alpha=0.8)\n",
    "    # ax1.plot(your_mlp.loss_history, 'r--', linewidth=2, label='NumPy', alpha=0.8)  # Uncomment after implementing\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Learning Curves: PyTorch vs NumPy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot 2: Implementation comparison\n",
    "    implementations = ['PyTorch']\n",
    "    accuracies_comp = [test_acc_torch]\n",
    "    \n",
    "    # TODO: Add NumPy results when available\n",
    "    # implementations.append('NumPy')\n",
    "    # accuracies_comp.append(your_test_accuracy)\n",
    "    \n",
    "    bars = ax2.bar(implementations, accuracies_comp, \n",
    "                   color=['blue', 'red'][:len(implementations)], alpha=0.7)\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.set_title('Implementation Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, accuracies_comp):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Advantages of PyTorch:\")\n",
    "    print(\"• Automatic differentiation (no manual backprop implementation)\")\n",
    "    print(\"• GPU acceleration support\")\n",
    "    print(\"• Optimized operations and memory management\")\n",
    "    print(\"• Rich ecosystem of pre-built components\")\n",
    "    print(\"• Production-ready deployment tools\")\n",
    "else:\n",
    "    print(\"Install PyTorch to see the comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Part 5: Critical Reflection (15 minutes)\n",
    "\n",
    "Now let's reflect on what you've learned about neural networks and their practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### 5.1 Architecture and Design Choices\n",
    "\n",
    "**TODO: Answer these questions based on your experiments:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "**1. How did different network architectures (shallow vs deep, narrow vs wide) affect performance?**\n",
    "\n",
    "[TODO: Analyze your architecture comparison results. Discuss trade-offs between model complexity and performance.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "**2. Which activation function worked best for your dataset? Why do you think this was the case?**\n",
    "\n",
    "[TODO: Compare the activation functions you tested. Consider convergence speed, final accuracy, and potential issues like vanishing gradients.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "**3. What signs of overfitting (if any) did you observe? How could you address them?**\n",
    "\n",
    "[TODO: Look at the difference between training and test accuracy. Suggest techniques like regularization, dropout, or early stopping.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### 5.2 Neural Networks vs Previous Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "**4. How do neural networks compare to the linear regression (CA1) and classification models (CA2) you implemented?**\n",
    "\n",
    "**Advantages of Neural Networks:**\n",
    "[TODO: List advantages like non-linear modeling, universal approximation, feature learning]\n",
    "\n",
    "**Disadvantages of Neural Networks:**\n",
    "[TODO: List disadvantages like complexity, interpretability, training time, overfitting risk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "**5. When would you choose a neural network over simpler models like logistic regression or decision trees?**\n",
    "\n",
    "[TODO: Consider factors like data size, problem complexity, interpretability requirements, computational resources]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "### 5.3 Implementation Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "**6. What was the most challenging part of implementing backpropagation from scratch?**\n",
    "\n",
    "[TODO: Reflect on difficulties with gradient computation, matrix operations, or debugging]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "**7. How does the PyTorch implementation compare to your NumPy version?**\n",
    "\n",
    "**Ease of Implementation:**\n",
    "[TODO: Compare the complexity and lines of code]\n",
    "\n",
    "**Performance:**\n",
    "[TODO: Compare training speed and final accuracy]\n",
    "\n",
    "**Debugging and Development:**\n",
    "[TODO: Discuss which was easier to debug and modify]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### 5.4 Real-World Applications and Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "**8. Give three real-world applications where neural networks would be appropriate:**\n",
    "\n",
    "**Application 1:**\n",
    "[TODO: Describe a specific use case, why neural networks are suitable, and what type of data would be involved]\n",
    "\n",
    "**Application 2:**\n",
    "[TODO: Describe another use case with different characteristics]\n",
    "\n",
    "**Application 3:**\n",
    "[TODO: Describe a third use case, perhaps in a different domain]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "**9. What ethical considerations should be kept in mind when deploying neural networks?**\n",
    "\n",
    "[TODO: Discuss issues like bias, fairness, transparency, privacy, and accountability. Consider the \"black box\" nature of neural networks.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "**10. How might the \"black box\" nature of neural networks be problematic in certain applications?**\n",
    "\n",
    "[TODO: Consider applications like medical diagnosis, loan approval, criminal justice where explainability is crucial]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## Bonus: Advanced Experiments (Optional)\n",
    "\n",
    "If you have extra time, try these advanced experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 1: Implement different weight initialization strategies\n",
    "def xavier_init(fan_in, fan_out):\n",
    "    \"\"\"Xavier/Glorot initialization.\"\"\"\n",
    "    limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "    return np.random.uniform(-limit, limit, (fan_in, fan_out))\n",
    "\n",
    "def he_init(fan_in, fan_out):\n",
    "    \"\"\"He initialization (good for ReLU).\"\"\"\n",
    "    std = np.sqrt(2 / fan_in)\n",
    "    return np.random.normal(0, std, (fan_in, fan_out))\n",
    "\n",
    "print(\"Bonus: Try different initialization strategies in your MLP class!\")\n",
    "print(\"1. Xavier initialization: Good for tanh and sigmoid\")\n",
    "print(\"2. He initialization: Good for ReLU\")\n",
    "print(\"3. Compare convergence speed and final performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2: Implement learning rate scheduling\n",
    "def step_decay(initial_lr, epoch, drop_rate=0.5, epochs_drop=100):\n",
    "    \"\"\"Step decay learning rate schedule.\"\"\"\n",
    "    return initial_lr * (drop_rate ** (epoch // epochs_drop))\n",
    "\n",
    "def exponential_decay(initial_lr, epoch, decay_rate=0.95):\n",
    "    \"\"\"Exponential decay learning rate schedule.\"\"\"\n",
    "    return initial_lr * (decay_rate ** epoch)\n",
    "\n",
    "print(\"Bonus: Implement adaptive learning rates!\")\n",
    "print(\"1. Start with higher learning rate for faster initial convergence\")\n",
    "print(\"2. Reduce learning rate over time for fine-tuning\")\n",
    "print(\"3. Compare with constant learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 3: Implement early stopping\n",
    "def early_stopping_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate early stopping to prevent overfitting.\n",
    "    \"\"\"\n",
    "    print(\"Bonus: Implement early stopping!\")\n",
    "    print(\"1. Monitor validation loss during training\")\n",
    "    print(\"2. Stop training when validation loss stops improving\")\n",
    "    print(\"3. Save the best model weights\")\n",
    "    print(\"4. Compare with and without early stopping\")\n",
    "    \n",
    "    # TODO: Modify your MLP class to include validation monitoring\n",
    "    # and early stopping logic\n",
    "\n",
    "early_stopping_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "## Summary and Submission\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! In this assignment, you have:\n",
    "\n",
    "**Understood neural network fundamentals** including forward/backward propagation  \n",
    "**Implemented a complete MLP from scratch** using only NumPy  \n",
    "**Mastered backpropagation** and gradient descent for neural networks  \n",
    "**Compared different architectures** and activation functions  \n",
    "**Applied neural networks** to real-world classification problems  \n",
    "**Transitioned to PyTorch** for modern deep learning development  \n",
    "**Reflected critically** on when and how to use neural networks  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**TODO: Write 3-4 key insights from this assignment:**\n",
    "\n",
    "1. [TODO: Your first key takeaway about neural network capabilities vs limitations]\n",
    "2. [TODO: Your second key takeaway about implementation challenges or surprises]\n",
    "3. [TODO: Your third key takeaway about practical considerations (overfitting, architecture choice, etc.)]\n",
    "4. [TODO: Your fourth key takeaway about PyTorch vs manual implementation]\n",
    "\n",
    "### Neural Networks vs Previous Models\n",
    "\n",
    "**TODO: Compare neural networks to the models from CA1 and CA2:**\n",
    "\n",
    "**When to use Neural Networks:**\n",
    "[TODO: List scenarios where neural networks are the best choice]\n",
    "\n",
    "**When to use Simpler Models:**\n",
    "[TODO: List scenarios where linear/logistic regression or other simple models are better]\n",
    "\n",
    "### Looking Forward\n",
    "\n",
    "**TODO: What aspects of neural networks would you like to explore further?**\n",
    "\n",
    "[TODO: Mention topics like convolutional networks, recurrent networks, attention mechanisms, or specific applications]\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "**TODO: Write a brief (150-200 words) reflection on your experience implementing neural networks from scratch:**\n",
    "\n",
    "[TODO: Your final reflection here - discuss what was challenging, what was surprising, how it changed your understanding of neural networks, and what you'd like to learn next]\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Complete!**\n",
    "\n",
    "Make sure to:\n",
    "1. Complete all TODO sections\n",
    "2. Test your MLP implementation thoroughly\n",
    "3. Answer all reflection questions\n",
    "4. Save your notebook\n",
    "5. Export as HTML\n",
    "6. Submit both .ipynb and .html files\n",
    "7. Include your name and student ID at the top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}