{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment 5: Transformer Architecture Fundamentals\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Student ID:** [Your Student ID]  \n",
    "**Date:** [Today's Date]  \n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the fascinating world of transformers! In this assignment, you'll build a transformer architecture from scratch, implementing the revolutionary attention mechanism that powers modern AI systems like ChatGPT, BERT, and GPT-4.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand transformer architecture and attention mechanisms\n",
    "- Implement tokenization, embeddings, and positional encoding\n",
    "- Build self-attention and multi-head attention layers\n",
    "- Assemble complete transformer blocks with layer normalization\n",
    "- Train a mini-transformer on sentiment analysis\n",
    "- Analyze transformer capabilities and visualize attention patterns\n",
    "\n",
    "**Estimated Time:** 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Progress tracking and utilities\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Transformers (20 minutes)\n",
    "\n",
    "**Goal:** Learn why transformers revolutionized AI and understand their core concepts\n",
    "\n",
    "## 1.1 The Transformer Revolution\n",
    "\n",
    "Before transformers, processing sequential data (like text) required **recurrent neural networks (RNNs)** that processed words one by one. This was slow and suffered from vanishing gradients. Transformers changed everything with a simple but powerful idea: **\"Attention is All You Need\"**.\n",
    "\n",
    "**Key Innovations:**\n",
    "- **Self-Attention**: Allow each word to \"attend\" to all other words simultaneously\n",
    "- **Parallelization**: Process all words at once, not sequentially\n",
    "- **Position Encoding**: Add position information since attention is position-agnostic\n",
    "- **Scalability**: Architecture scales beautifully to massive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference between RNN and Transformer processing\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# RNN Sequential Processing\n",
    "words = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "positions = range(len(words))\n",
    "\n",
    "# Show RNN sequential dependencies\n",
    "ax1.scatter(positions, [1]*len(words), s=200, alpha=0.7, color='lightblue')\n",
    "for i, word in enumerate(words):\n",
    "    ax1.text(i, 1, word, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw sequential arrows\n",
    "for i in range(len(words)-1):\n",
    "    ax1.arrow(i+0.1, 1, 0.8, 0, head_width=0.05, head_length=0.05, \n",
    "              fc='red', ec='red', linewidth=2)\n",
    "\n",
    "ax1.set_xlim(-0.5, len(words)-0.5)\n",
    "ax1.set_ylim(0.5, 1.5)\n",
    "ax1.set_title('RNN: Sequential Processing\\n(Each word depends on previous)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Time Steps (Sequential)')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Transformer Parallel Processing\n",
    "ax2.scatter(positions, [1]*len(words), s=200, alpha=0.7, color='lightgreen')\n",
    "for i, word in enumerate(words):\n",
    "    ax2.text(i, 1, word, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw attention connections (every word to every word)\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        if i != j:\n",
    "            # Curved arrows to show all-to-all connections\n",
    "            ax2.annotate('', xy=(j, 1), xytext=(i, 1),\n",
    "                        arrowprops=dict(arrowstyle='->', color='blue', alpha=0.3,\n",
    "                                      connectionstyle='arc3,rad=0.2', linewidth=1))\n",
    "\n",
    "ax2.set_xlim(-0.5, len(words)-0.5)\n",
    "ax2.set_ylim(0.5, 1.5)\n",
    "ax2.set_title('Transformer: Parallel Processing\\n(Each word attends to all words)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Parallel Processing')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîë Key Differences:\")\n",
    "print(\"   RNN: Sequential, slow, vanishing gradients\")\n",
    "print(\"   Transformer: Parallel, fast, better long-range dependencies\")\n",
    "print(\"\\n‚ö° Why Transformers Won:\")\n",
    "print(\"   ‚Ä¢ Faster training (parallelization)\")\n",
    "print(\"   ‚Ä¢ Better at long sequences\")\n",
    "print(\"   ‚Ä¢ More expressive attention patterns\")\n",
    "print(\"   ‚Ä¢ Scales to billions of parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 High-Level Transformer Architecture\n",
    "\n",
    "Let's understand the overall transformer architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of transformer architecture\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "\n",
    "# Define the components and their positions\n",
    "components = [\n",
    "    {'name': 'Input Tokens', 'y': 0.1, 'color': 'lightblue', 'desc': '[\"The\", \"cat\", \"sat\"]'},\n",
    "    {'name': 'Token Embeddings', 'y': 0.2, 'color': 'lightgreen', 'desc': 'Convert words to vectors'},\n",
    "    {'name': 'Position Embeddings', 'y': 0.3, 'color': 'lightyellow', 'desc': 'Add position information'},\n",
    "    {'name': 'Multi-Head Attention', 'y': 0.5, 'color': 'lightcoral', 'desc': 'Words attend to each other'},\n",
    "    {'name': 'Add & Norm', 'y': 0.6, 'color': 'lightgray', 'desc': 'Residual + Layer Norm'},\n",
    "    {'name': 'Feed Forward', 'y': 0.7, 'color': 'lightpink', 'desc': 'Position-wise MLP'},\n",
    "    {'name': 'Add & Norm', 'y': 0.8, 'color': 'lightgray', 'desc': 'Residual + Layer Norm'},\n",
    "    {'name': 'Classification Head', 'y': 0.95, 'color': 'lightsteelblue', 'desc': 'Final predictions'}\n",
    "]\n",
    "\n",
    "# Draw the components\n",
    "for comp in components:\n",
    "    # Main box\n",
    "    rect = plt.Rectangle((0.2, comp['y']-0.04), 0.6, 0.06, \n",
    "                        facecolor=comp['color'], edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Component name\n",
    "    ax.text(0.5, comp['y'], comp['name'], ha='center', va='center', \n",
    "           fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Description\n",
    "    ax.text(0.85, comp['y'], comp['desc'], ha='left', va='center', \n",
    "           fontsize=10, style='italic')\n",
    "\n",
    "# Draw arrows between components\n",
    "for i in range(len(components)-1):\n",
    "    ax.arrow(0.5, components[i]['y']+0.03, 0, 0.04, \n",
    "            head_width=0.02, head_length=0.01, fc='blue', ec='blue')\n",
    "\n",
    "# Add special notation for transformer block repetition\n",
    "ax.annotate('', xy=(0.15, 0.8), xytext=(0.15, 0.5),\n",
    "           arrowprops=dict(arrowstyle='<->', color='red', linewidth=3))\n",
    "ax.text(0.05, 0.65, 'Transformer\\nBlock\\n(Repeat N times)', \n",
    "       ha='center', va='center', fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "ax.set_xlim(0, 1.4)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_title('Transformer Architecture Overview', fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üèóÔ∏è Transformer Building Blocks:\")\n",
    "print(\"   1. Embeddings: Convert tokens to vectors\")\n",
    "print(\"   2. Attention: Let words 'talk' to each other\")\n",
    "print(\"   3. Feed-Forward: Process each position independently\")\n",
    "print(\"   4. Residuals: Add input to output (helps training)\")\n",
    "print(\"   5. Layer Norm: Normalize activations (stabilizes training)\")\n",
    "print(\"\\nüí° The magic is in the attention mechanism!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Why Position Matters\n",
    "\n",
    "Unlike RNNs, attention has no inherent sense of position. We need to add position information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why position encoding is needed\n",
    "sentence1 = \"The cat sat on the mat\"\n",
    "sentence2 = \"The mat sat on the cat\"\n",
    "\n",
    "print(\"ü§î Without position encoding:\")\n",
    "print(f\"   Sentence 1: {sentence1}\")\n",
    "print(f\"   Sentence 2: {sentence2}\")\n",
    "print(\"\\n   Both sentences have the same words!\")\n",
    "print(\"   Pure attention (without position) couldn't tell them apart!\")\n",
    "\n",
    "print(\"\\n‚úÖ With position encoding:\")\n",
    "words1 = sentence1.split()\n",
    "words2 = sentence2.split()\n",
    "\n",
    "print(\"   Sentence 1 with positions:\")\n",
    "for i, word in enumerate(words1):\n",
    "    print(f\"      Position {i}: {word}\")\n",
    "    \n",
    "print(\"\\n   Sentence 2 with positions:\")\n",
    "for i, word in enumerate(words2):\n",
    "    print(f\"      Position {i}: {word}\")\n",
    "    \n",
    "print(\"\\n   Now the transformer can distinguish between them!\")\n",
    "print(\"\\nüéØ Position encoding gives transformers a sense of word order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Tokenization & Embeddings (20 minutes)\n",
    "\n",
    "**Goal:** Convert text into numerical representations that transformers can process\n",
    "\n",
    "## 2.1 Text Tokenization\n",
    "\n",
    "First, we need to convert text into tokens (discrete units like words or subwords):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple word-level tokenizer for learning purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=5000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.inverse_vocab = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.cls_token = '<CLS>'  # For classification\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        \n",
    "        # TODO: Count word frequencies across all texts\n",
    "        # HINT: Use Counter to count words after tokenizing\n",
    "        # HINT: Convert to lowercase and split on whitespace\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            # TODO: Tokenize text into words\n",
    "            # HINT: Clean text, convert to lowercase, split into words\n",
    "            words = None  # Your code here\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # TODO: Build vocabulary with most frequent words\n",
    "        # HINT: Start with special tokens, then add most common words\n",
    "        \n",
    "        # Add special tokens first\n",
    "        self.vocab[self.pad_token] = 0\n",
    "        self.vocab[self.unk_token] = 1 \n",
    "        self.vocab[self.cls_token] = 2\n",
    "        \n",
    "        # TODO: Add most common words up to vocab_size\n",
    "        # HINT: Use word_counts.most_common(self.vocab_size - 3)\n",
    "        # HINT: Reserve 3 slots for special tokens\n",
    "        most_common = None  # Your code here\n",
    "        \n",
    "        for i, (word, count) in enumerate(most_common):\n",
    "            self.vocab[word] = i + 3  # +3 for special tokens\n",
    "        \n",
    "        # Create inverse mapping\n",
    "        self.inverse_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "        \n",
    "        print(f\"‚úÖ Vocabulary built with {len(self.vocab)} words\")\n",
    "        print(f\"   Most common words: {list(self.vocab.keys())[3:8]}\")\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # TODO: Implement text cleaning\n",
    "        # HINT: Remove extra whitespace, convert to lowercase\n",
    "        # HINT: Optionally remove punctuation or keep it\n",
    "        text = text.lower().strip()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "        \n",
    "    def tokenize(self, text, max_length=128):\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        \n",
    "        # TODO: Clean and tokenize text\n",
    "        # HINT: Use clean_text, then split into words\n",
    "        words = None  # Your code here\n",
    "        \n",
    "        # TODO: Add CLS token at the beginning\n",
    "        # HINT: tokens = [self.cls_token] + words\n",
    "        tokens = None  # Your code here\n",
    "        \n",
    "        # TODO: Truncate to max_length\n",
    "        # HINT: tokens = tokens[:max_length]\n",
    "        tokens = None  # Your code here\n",
    "        \n",
    "        # TODO: Convert words to indices\n",
    "        # HINT: Use self.vocab.get(word, self.vocab[self.unk_token])\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            idx = None  # Your code here - get index or UNK\n",
    "            indices.append(idx)\n",
    "        \n",
    "        # TODO: Pad sequence to max_length\n",
    "        # HINT: Add PAD tokens until length equals max_length\n",
    "        while len(indices) < max_length:\n",
    "            indices.append(None)  # Your code here - PAD token index\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert indices back to text\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx in self.inverse_vocab:\n",
    "                word = self.inverse_vocab[idx]\n",
    "                if word not in [self.pad_token, self.cls_token]:\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Test the tokenizer\n",
    "sample_texts = [\n",
    "    \"I love this movie! It's fantastic.\",\n",
    "    \"This film is terrible. I hate it.\",\n",
    "    \"Great acting and wonderful story.\",\n",
    "    \"Boring and predictable plot.\"\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=100)\n",
    "tokenizer.build_vocab(sample_texts)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"I love great movies!\"\n",
    "tokens = tokenizer.tokenize(test_text, max_length=10)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"\\nüß™ Tokenization Test:\")\n",
    "print(f\"   Original: {test_text}\")\n",
    "print(f\"   Tokens: {tokens[:6]}...\")  # Show first 6\n",
    "print(f\"   Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Token Embeddings\n",
    "\n",
    "Now let's convert token indices into dense vector representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Convert token indices to dense vectors\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create embedding layer\n",
    "        # HINT: Use nn.Embedding(vocab_size, d_model)\n",
    "        # HINT: d_model is the embedding dimension (typically 256, 512, etc.)\n",
    "        self.embedding = None  # Your code here\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Apply embedding and scale\n",
    "        # HINT: embedding(x) * sqrt(d_model) - common in transformers\n",
    "        # HINT: Scaling helps with training stability\n",
    "        return None  # Your code here\n",
    "\n",
    "# Test token embeddings\n",
    "vocab_size = 100\n",
    "d_model = 64  # Embedding dimension\n",
    "\n",
    "token_embed = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# Test with sample tokens\n",
    "sample_tokens = torch.tensor([[1, 5, 10, 2, 0, 0]])  # Batch size 1\n",
    "embeddings = token_embed(sample_tokens)\n",
    "\n",
    "print(f\"üìä Token Embedding Test:\")\n",
    "print(f\"   Input shape: {sample_tokens.shape}\")\n",
    "print(f\"   Output shape: {embeddings.shape}\")\n",
    "print(f\"   Embedding dimension: {d_model}\")\n",
    "print(f\"   Each token ‚Üí {d_model}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Positional Embeddings\n",
    "\n",
    "Add position information using sinusoidal encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional information using sinusoidal encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_length=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create positional encoding matrix\n",
    "        # HINT: Shape should be (max_length, d_model)\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        \n",
    "        # TODO: Create position vector\n",
    "        # HINT: position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        position = None  # Your code here\n",
    "        \n",
    "        # TODO: Create div_term for sinusoidal pattern\n",
    "        # HINT: div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        div_term = None  # Your code here\n",
    "        \n",
    "        # TODO: Apply sine to even indices\n",
    "        # HINT: pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0::2] = None  # Your code here\n",
    "        \n",
    "        # TODO: Apply cosine to odd indices  \n",
    "        # HINT: pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe[:, 1::2] = None  # Your code here\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Add positional encoding to embeddings\n",
    "        # HINT: x + self.pe[:x.size(1)] (add position encoding up to sequence length)\n",
    "        return None  # Your code here\n",
    "\n",
    "# Test positional encoding\n",
    "pos_encoding = PositionalEncoding(d_model=64, max_length=128)\n",
    "\n",
    "# Visualize positional encodings\n",
    "pe_matrix = pos_encoding.pe[:50, :].numpy()  # First 50 positions\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pe_matrix.T, cmap='RdYlBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Embedding Dimension')\n",
    "plt.title('Positional Encoding Visualization\\n(Each position has unique pattern)', \n",
    "          fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"üåä Positional Encoding Properties:\")\n",
    "print(\"   ‚Ä¢ Each position has a unique sinusoidal pattern\")\n",
    "print(\"   ‚Ä¢ Different frequencies across embedding dimensions\")\n",
    "print(\"   ‚Ä¢ Allows model to learn relative positions\")\n",
    "print(f\"   ‚Ä¢ Encoding shape: {pos_encoding.pe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Complete Embedding Layer\n",
    "\n",
    "Combine token and positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbeddings(nn.Module):\n",
    "    \"\"\"Complete embedding layer with tokens + positions\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize components\n",
    "        # HINT: Use the classes we just created\n",
    "        self.token_embedding = None  # Your code here\n",
    "        self.pos_encoding = None     # Your code here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # TODO: Combine token and positional embeddings\n",
    "        # HINT: Get token embeddings, add positional encoding, apply dropout\n",
    "        \n",
    "        # Step 1: Get token embeddings\n",
    "        token_embeds = None  # Your code here\n",
    "        \n",
    "        # Step 2: Add positional encoding\n",
    "        embeddings = None  # Your code here\n",
    "        \n",
    "        # Step 3: Apply dropout\n",
    "        embeddings = None  # Your code here\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test complete embeddings\n",
    "embeddings = TransformerEmbeddings(vocab_size=100, d_model=64)\n",
    "\n",
    "# Sample input\n",
    "input_ids = torch.tensor([[2, 10, 5, 20, 1, 0, 0]])  # [CLS] + words + [UNK] + [PAD]\n",
    "output = embeddings(input_ids)\n",
    "\n",
    "print(f\"üîó Complete Embeddings Test:\")\n",
    "print(f\"   Input IDs: {input_ids[0].tolist()[:5]}...\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Each token now has position-aware embeddings!\")\n",
    "\n",
    "# Visualize embedding differences\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Token embeddings only (first few dimensions)\n",
    "token_only = embeddings.token_embedding(input_ids)[0, :5, :8].detach().numpy()\n",
    "im1 = ax1.imshow(token_only.T, cmap='viridis', aspect='auto')\n",
    "ax1.set_title('Token Embeddings Only', fontweight='bold')\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Complete embeddings with position\n",
    "complete = output[0, :5, :8].detach().numpy()\n",
    "im2 = ax2.imshow(complete.T, cmap='viridis', aspect='auto')\n",
    "ax2.set_title('Token + Position Embeddings', fontweight='bold')\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ú® Notice how positional encoding changes the embeddings!\")\n",
    "print(\"   Same tokens at different positions have different representations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Self-Attention Mechanism (25 minutes)\n",
    "\n",
    "**Goal:** Implement the heart of the transformer - the attention mechanism\n",
    "\n",
    "## 3.1 Understanding Attention Intuition\n",
    "\n",
    "Attention allows each word to \"attend\" to (focus on) relevant words in the sequence. It's like asking: \"When processing this word, which other words should I pay attention to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate attention intuition with example sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "words = sentence.split()\n",
    "\n",
    "print(\"üß† Attention Intuition Example:\")\n",
    "print(f\"   Sentence: {sentence}\")\n",
    "print()\n",
    "\n",
    "# Manual attention examples\n",
    "attention_examples = {\n",
    "    \"cat\": [\"The\", \"sat\"],  # \"cat\" attends to \"The\" and \"sat\"\n",
    "    \"sat\": [\"cat\", \"on\"],   # \"sat\" attends to \"cat\" and \"on\"\n",
    "    \"mat\": [\"the\", \"on\"],   # \"mat\" attends to \"the\" and \"on\"\n",
    "}\n",
    "\n",
    "for word, attends_to in attention_examples.items():\n",
    "    print(f\"   When processing '{word}', pay attention to: {attends_to}\")\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"   ‚Ä¢ Each word can attend to multiple other words\")\n",
    "print(\"   ‚Ä¢ Attention weights are learned, not hand-coded\")\n",
    "print(\"   ‚Ä¢ Allows capturing long-range dependencies\")\n",
    "print(\"   ‚Ä¢ Different heads can focus on different relationships\")\n",
    "\n",
    "# Visualize attention matrix concept\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create sample attention matrix\n",
    "n_words = len(words)\n",
    "attention_matrix = np.random.rand(n_words, n_words)\n",
    "\n",
    "# Make it more realistic (each row sums to 1)\n",
    "attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Add some structure (diagonal and nearby words)\n",
    "for i in range(n_words):\n",
    "    attention_matrix[i, i] += 0.3  # Self-attention\n",
    "    if i > 0:\n",
    "        attention_matrix[i, i-1] += 0.2  # Previous word\n",
    "    if i < n_words - 1:\n",
    "        attention_matrix[i, i+1] += 0.2  # Next word\n",
    "\n",
    "# Renormalize\n",
    "attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "im = ax.imshow(attention_matrix, cmap='Blues', aspect='auto')\n",
    "ax.set_xticks(range(n_words))\n",
    "ax.set_yticks(range(n_words))\n",
    "ax.set_xticklabels(words)\n",
    "ax.set_yticklabels(words)\n",
    "ax.set_xlabel('Attending to (Keys)')\n",
    "ax.set_ylabel('Query words')\n",
    "ax.set_title('Sample Attention Matrix\\n(Each row shows what a word attends to)', fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_words):\n",
    "    for j in range(n_words):\n",
    "        text = ax.text(j, i, f'{attention_matrix[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"white\" if attention_matrix[i, j] > 0.5 else \"black\")\n",
    "\n",
    "plt.colorbar(im, label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Reading the Attention Matrix:\")\n",
    "print(\"   ‚Ä¢ Each row sums to 1.0 (probability distribution)\")\n",
    "print(\"   ‚Ä¢ Darker blue = stronger attention\")\n",
    "print(\"   ‚Ä¢ Row i shows what word i attends to\")\n",
    "print(\"   ‚Ä¢ Column j shows which words attend to word j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Scaled Dot-Product Attention\n",
    "\n",
    "The core attention mechanism: Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (batch_size, seq_len, d_k)\n",
    "        K: Key matrix (batch_size, seq_len, d_k)\n",
    "        V: Value matrix (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "        dropout: Optional dropout layer\n",
    "        \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Get the dimension for scaling\n",
    "    # HINT: d_k = Q.size(-1) - last dimension of Q\n",
    "    d_k = None  # Your code here\n",
    "    \n",
    "    # TODO: Compute attention scores\n",
    "    # HINT: scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # HINT: This computes QK^T and scales by sqrt(d_k)\n",
    "    scores = None  # Your code here\n",
    "    \n",
    "    # TODO: Apply mask if provided\n",
    "    # HINT: scores.masked_fill(mask == 0, -1e9) - set masked positions to very negative\n",
    "    if mask is not None:\n",
    "        scores = None  # Your code here\n",
    "    \n",
    "    # TODO: Apply softmax to get attention weights\n",
    "    # HINT: F.softmax(scores, dim=-1) - softmax over last dimension\n",
    "    attention_weights = None  # Your code here\n",
    "    \n",
    "    # TODO: Apply dropout if provided\n",
    "    if dropout is not None:\n",
    "        attention_weights = None  # Your code here\n",
    "    \n",
    "    # TODO: Apply attention to values\n",
    "    # HINT: torch.matmul(attention_weights, V) - weighted sum of values\n",
    "    output = None  # Your code here\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "batch_size, seq_len, d_model = 2, 6, 64\n",
    "d_k = d_v = d_model  # For simplicity\n",
    "\n",
    "# Create sample Q, K, V matrices\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "# Apply attention\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"üîç Scaled Dot-Product Attention Test:\")\n",
    "print(f\"   Input shapes:\")\n",
    "print(f\"      Q (Query): {Q.shape}\")\n",
    "print(f\"      K (Key): {K.shape}\")\n",
    "print(f\"      V (Value): {V.shape}\")\n",
    "print(f\"   Output shapes:\")\n",
    "print(f\"      Output: {output.shape}\")\n",
    "print(f\"      Attention weights: {attention_weights.shape}\")\n",
    "\n",
    "# Verify attention weights sum to 1\n",
    "weights_sum = attention_weights.sum(dim=-1)\n",
    "print(f\"   Attention weights sum: {weights_sum[0, 0]:.3f} (should be ~1.0)\")\n",
    "\n",
    "# Visualize attention pattern for first batch\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_weights[0].detach().numpy(), cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights Visualization\\n(First sample from batch)', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Attention mechanism working!\")\n",
    "print(\"   Each query position attends to all key positions\")\n",
    "print(\"   Attention weights are learned through backpropagation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Head Attention\n",
    "\n",
    "Instead of one attention function, use multiple \"heads\" to attend to different types of relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Verify that d_model is divisible by num_heads\n",
    "        # HINT: assert d_model % num_heads == 0\n",
    "        assert None  # Your code here\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # TODO: Linear projections for Q, K, V\n",
    "        # HINT: Create three nn.Linear layers, each mapping d_model -> d_model\n",
    "        self.W_q = None  # Your code here - Query projection\n",
    "        self.W_k = None  # Your code here - Key projection\n",
    "        self.W_v = None  # Your code here - Value projection\n",
    "        \n",
    "        # TODO: Output projection\n",
    "        # HINT: nn.Linear(d_model, d_model)\n",
    "        self.W_o = None  # Your code here\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # TODO: Linear projections\n",
    "        # HINT: Apply W_q, W_k, W_v to input x\n",
    "        Q = None  # Your code here\n",
    "        K = None  # Your code here\n",
    "        V = None  # Your code here\n",
    "        \n",
    "        # TODO: Reshape for multi-head attention\n",
    "        # HINT: Reshape from (batch, seq_len, d_model) to (batch, num_heads, seq_len, d_k)\n",
    "        # HINT: Use .view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        Q = None  # Your code here\n",
    "        K = None  # Your code here\n",
    "        V = None  # Your code here\n",
    "        \n",
    "        # TODO: Apply attention\n",
    "        # HINT: Use the scaled_dot_product_attention function we defined\n",
    "        # HINT: Pass dropout=self.dropout\n",
    "        attn_output, attention_weights = None  # Your code here\n",
    "        \n",
    "        # TODO: Concatenate heads\n",
    "        # HINT: Transpose back and reshape to (batch, seq_len, d_model)\n",
    "        # HINT: Use .transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        attn_output = None  # Your code here\n",
    "        \n",
    "        # TODO: Final linear projection\n",
    "        # HINT: Apply W_o to the concatenated output\n",
    "        output = None  # Your code here\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Sample input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attention_weights = mha(x)\n",
    "\n",
    "print(f\"üé≠ Multi-Head Attention Test:\")\n",
    "print(f\"   Input shape: {x.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"   Number of heads: {num_heads}\")\n",
    "print(f\"   Dimension per head: {d_model // num_heads}\")\n",
    "\n",
    "# Visualize different attention heads\n",
    "if attention_weights is not None:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head in range(min(8, num_heads)):  # Show up to 8 heads\n",
    "        head_weights = attention_weights[0, head].detach().numpy()\n",
    "        im = axes[head].imshow(head_weights, cmap='Blues', aspect='auto')\n",
    "        axes[head].set_title(f'Head {head + 1}', fontweight='bold')\n",
    "        axes[head].set_xlabel('Key Position')\n",
    "        axes[head].set_ylabel('Query Position')\n",
    "    \n",
    "    plt.suptitle('Different Attention Heads Learn Different Patterns', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üß† Multi-Head Benefits:\")\n",
    "    print(\"   ‚Ä¢ Each head can focus on different relationships\")\n",
    "    print(\"   ‚Ä¢ Some heads might focus on syntax, others on semantics\")\n",
    "    print(\"   ‚Ä¢ Increases model capacity and expressiveness\")\n",
    "    print(\"   ‚Ä¢ Allows parallel computation across heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Transformer Block Components (20 minutes)\n",
    "\n",
    "**Goal:** Build the complete transformer block with layer normalization and feed-forward networks\n",
    "\n",
    "## 4.1 Layer Normalization\n",
    "\n",
    "Layer normalization helps stabilize training in deep networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization is built into PyTorch, but let's understand it\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization for stable training\"\"\"\n",
    "    \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create learnable parameters\n",
    "        # HINT: gamma (scale) and beta (shift) parameters\n",
    "        # HINT: Use nn.Parameter(torch.ones(features)) for gamma\n",
    "        # HINT: Use nn.Parameter(torch.zeros(features)) for beta\n",
    "        self.gamma = None  # Your code here\n",
    "        self.beta = None   # Your code here\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Compute layer normalization\n",
    "        # HINT: Normalize across last dimension\n",
    "        # HINT: mean = x.mean(-1, keepdim=True)\n",
    "        # HINT: std = x.std(-1, keepdim=True)\n",
    "        # HINT: normalized = (x - mean) / (std + eps)\n",
    "        # HINT: return gamma * normalized + beta\n",
    "        \n",
    "        mean = None  # Your code here\n",
    "        std = None   # Your code here\n",
    "        normalized = None  # Your code here\n",
    "        return None  # Your code here\n",
    "\n",
    "# Compare with PyTorch's LayerNorm\n",
    "d_model = 64\n",
    "custom_ln = LayerNorm(d_model)\n",
    "pytorch_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "# Test input\n",
    "x = torch.randn(2, 10, d_model) * 5 + 10  # Large variance and mean\n",
    "\n",
    "# Apply both normalizations\n",
    "custom_out = custom_ln(x)\n",
    "pytorch_out = pytorch_ln(x)\n",
    "\n",
    "print(f\"üìä Layer Normalization Comparison:\")\n",
    "print(f\"   Input statistics:\")\n",
    "print(f\"      Mean: {x.mean():.3f}, Std: {x.std():.3f}\")\n",
    "print(f\"   After custom LayerNorm:\")\n",
    "print(f\"      Mean: {custom_out.mean():.3f}, Std: {custom_out.std():.3f}\")\n",
    "print(f\"   After PyTorch LayerNorm:\")\n",
    "print(f\"      Mean: {pytorch_out.mean():.3f}, Std: {pytorch_out.std():.3f}\")\n",
    "\n",
    "# Visualize normalization effect\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(x.flatten().detach().numpy(), bins=50, alpha=0.7, color='red')\n",
    "axes[0].set_title('Original Distribution', fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Custom LayerNorm\n",
    "axes[1].hist(custom_out.flatten().detach().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "axes[1].set_title('After Custom LayerNorm', fontweight='bold')\n",
    "\n",
    "# PyTorch LayerNorm\n",
    "axes[2].hist(pytorch_out.flatten().detach().numpy(), bins=50, alpha=0.7, color='green')\n",
    "axes[2].set_title('After PyTorch LayerNorm', fontweight='bold')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Layer normalization centers and scales the distribution\")\n",
    "print(\"   This helps with gradient flow and training stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feed-Forward Network\n",
    "\n",
    "Position-wise feed-forward network processes each position independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create two linear layers\n",
    "        # HINT: First layer: d_model -> d_ff (expansion)\n",
    "        # HINT: Second layer: d_ff -> d_model (projection back)\n",
    "        # HINT: Typically d_ff = 4 * d_model\n",
    "        self.linear1 = None  # Your code here\n",
    "        self.linear2 = None  # Your code here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement feed-forward pass\n",
    "        # HINT: linear1 -> ReLU -> dropout -> linear2\n",
    "        # HINT: Use F.relu() for activation\n",
    "        \n",
    "        # Step 1: First linear layer\n",
    "        x = None  # Your code here\n",
    "        \n",
    "        # Step 2: ReLU activation\n",
    "        x = None  # Your code here\n",
    "        \n",
    "        # Step 3: Dropout\n",
    "        x = None  # Your code here\n",
    "        \n",
    "        # Step 4: Second linear layer\n",
    "        x = None  # Your code here\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test feed-forward network\n",
    "d_model = 64\n",
    "d_ff = 4 * d_model  # Common choice: 4x expansion\n",
    "\n",
    "ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "# Test input\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"üîÑ Feed-Forward Network Test:\")\n",
    "print(f\"   Input shape: {x.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Hidden dimension: {d_ff} ({d_ff // d_model}x expansion)\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in ffn.parameters()):,}\")\n",
    "\n",
    "# Analyze parameter distribution\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "layer1_params = ffn.linear1.weight.numel() + ffn.linear1.bias.numel()\n",
    "layer2_params = ffn.linear2.weight.numel() + ffn.linear2.bias.numel()\n",
    "\n",
    "print(f\"\\nüìä Parameter Breakdown:\")\n",
    "print(f\"   Layer 1: {layer1_params:,} parameters\")\n",
    "print(f\"   Layer 2: {layer2_params:,} parameters\")\n",
    "print(f\"   Total: {total_params:,} parameters\")\n",
    "print(f\"\\nüí° Feed-forward network contains most of the transformer's parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Complete Transformer Block\n",
    "\n",
    "Now let's assemble everything into a complete transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete transformer block with attention and feed-forward\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize components\n",
    "        # HINT: Use the classes we've implemented\n",
    "        self.attention = None     # Your code here - MultiHeadAttention\n",
    "        self.feed_forward = None  # Your code here - FeedForward\n",
    "        \n",
    "        # TODO: Layer normalization layers\n",
    "        # HINT: We need two LayerNorm layers, one after attention and one after FFN\n",
    "        self.norm1 = None  # Your code here - nn.LayerNorm(d_model)\n",
    "        self.norm2 = None  # Your code here - nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # TODO: Multi-head attention with residual connection\n",
    "        # HINT: Apply attention, dropout, then add to input (residual)\n",
    "        # HINT: Then apply layer normalization\n",
    "        \n",
    "        # Step 1: Multi-head attention\n",
    "        attn_output, attention_weights = None  # Your code here\n",
    "        \n",
    "        # Step 2: Dropout and residual connection\n",
    "        x1 = None  # Your code here - x + dropout(attn_output)\n",
    "        \n",
    "        # Step 3: Layer normalization\n",
    "        x1 = None  # Your code here\n",
    "        \n",
    "        # TODO: Feed-forward with residual connection\n",
    "        # HINT: Apply feed-forward, dropout, residual, then layer norm\n",
    "        \n",
    "        # Step 4: Feed-forward\n",
    "        ff_output = None  # Your code here\n",
    "        \n",
    "        # Step 5: Dropout and residual connection\n",
    "        x2 = None  # Your code here - x1 + dropout(ff_output)\n",
    "        \n",
    "        # Step 6: Layer normalization\n",
    "        x2 = None  # Your code here\n",
    "        \n",
    "        return x2, attention_weights\n",
    "\n",
    "# Test complete transformer block\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 4 * d_model\n",
    "\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "# Test input\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output, attention_weights = transformer_block(x)\n",
    "\n",
    "print(f\"üèóÔ∏è Complete Transformer Block Test:\")\n",
    "print(f\"   Input shape: {x.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")\n",
    "\n",
    "# Verify residual connections preserve dimensions\n",
    "print(f\"\\n‚úÖ Residual connections working:\")\n",
    "print(f\"   Input and output have same shape\")\n",
    "print(f\"   Information can flow directly through the block\")\n",
    "print(f\"   Gradients can flow back easily (no vanishing gradients)\")\n",
    "\n",
    "# Visualize the transformer block architecture\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "# Define components with their positions\n",
    "components = [\n",
    "    {'name': 'Input', 'y': 0.1, 'color': 'lightblue'},\n",
    "    {'name': 'Multi-Head\\nAttention', 'y': 0.3, 'color': 'lightcoral'},\n",
    "    {'name': 'Add & Norm', 'y': 0.45, 'color': 'lightgray'},\n",
    "    {'name': 'Feed\\nForward', 'y': 0.6, 'color': 'lightgreen'},\n",
    "    {'name': 'Add & Norm', 'y': 0.75, 'color': 'lightgray'},\n",
    "    {'name': 'Output', 'y': 0.9, 'color': 'lightsteelblue'}\n",
    "]\n",
    "\n",
    "# Draw components\n",
    "for comp in components:\n",
    "    rect = plt.Rectangle((0.3, comp['y']-0.05), 0.4, 0.08, \n",
    "                        facecolor=comp['color'], edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.5, comp['y'], comp['name'], ha='center', va='center', \n",
    "           fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw main flow arrows\n",
    "for i in range(len(components)-1):\n",
    "    ax.arrow(0.5, components[i]['y']+0.04, 0, 0.06, \n",
    "            head_width=0.02, head_length=0.01, fc='blue', ec='blue')\n",
    "\n",
    "# Draw residual connections\n",
    "# First residual (around attention)\n",
    "ax.arrow(0.2, 0.1, 0, 0.3, head_width=0.01, head_length=0.01, \n",
    "         fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "ax.arrow(0.2, 0.4, 0.08, 0.04, head_width=0.01, head_length=0.01, \n",
    "         fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Second residual (around feed-forward)\n",
    "ax.arrow(0.8, 0.45, 0, 0.25, head_width=0.01, head_length=0.01, \n",
    "         fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "ax.arrow(0.8, 0.7, -0.08, 0.04, head_width=0.01, head_length=0.01, \n",
    "         fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "\n",
    "ax.text(0.15, 0.25, 'Residual\\nConnection', ha='center', va='center', \n",
    "       fontsize=10, color='red', fontweight='bold')\n",
    "ax.text(0.85, 0.58, 'Residual\\nConnection', ha='center', va='center', \n",
    "       fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Transformer Block Architecture', fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîó Key Architectural Features:\")\n",
    "print(\"   ‚Ä¢ Residual connections enable deep networks\")\n",
    "print(\"   ‚Ä¢ Layer normalization stabilizes training\")\n",
    "print(\"   ‚Ä¢ Multi-head attention captures relationships\")\n",
    "print(\"   ‚Ä¢ Feed-forward adds non-linear processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini Transformer Implementation (25 minutes)\n",
    "\n",
    "**Goal:** Combine all components into a working transformer and train it on sentiment analysis\n",
    "\n",
    "## 5.1 Complete Mini Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    \"\"\"Complete mini transformer for classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, \n",
    "                 max_length, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize embedding layer\n",
    "        # HINT: Use TransformerEmbeddings class we created\n",
    "        self.embeddings = None  # Your code here\n",
    "        \n",
    "        # TODO: Stack of transformer blocks\n",
    "        # HINT: Use nn.ModuleList to create multiple TransformerBlocks\n",
    "        # HINT: Create num_layers TransformerBlock instances\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            None  # Your code here - TransformerBlock for each layer\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # TODO: Final layer normalization\n",
    "        # HINT: nn.LayerNorm(d_model)\n",
    "        self.ln_f = None  # Your code here\n",
    "        \n",
    "        # TODO: Classification head\n",
    "        # HINT: nn.Linear(d_model, num_classes) for final classification\n",
    "        self.classifier = None  # Your code here\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # TODO: Embed inputs\n",
    "        # HINT: Apply embeddings to input_ids\n",
    "        x = None  # Your code here\n",
    "        \n",
    "        # Store attention weights for analysis\n",
    "        attention_weights = []\n",
    "        \n",
    "        # TODO: Pass through transformer blocks\n",
    "        # HINT: Apply each transformer block in sequence\n",
    "        for block in self.transformer_blocks:\n",
    "            x, attn_weights = None  # Your code here - apply block\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        # TODO: Final layer normalization\n",
    "        x = None  # Your code here\n",
    "        \n",
    "        # TODO: Global average pooling for classification\n",
    "        # HINT: Take mean over sequence dimension\n",
    "        # HINT: Use torch.mean(x, dim=1) to pool over sequence length\n",
    "        pooled = None  # Your code here\n",
    "        \n",
    "        # TODO: Classification\n",
    "        # HINT: Apply classifier layer to pooled representation\n",
    "        logits = None  # Your code here\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# Test complete transformer\n",
    "config = {\n",
    "    'vocab_size': 1000,\n",
    "    'd_model': 64,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 2,  # Small for quick training\n",
    "    'd_ff': 256,\n",
    "    'max_length': 32,\n",
    "    'num_classes': 2,  # Binary sentiment classification\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = MiniTransformer(**config).to(device)\n",
    "\n",
    "# Test input\n",
    "batch_size = 4\n",
    "seq_length = 16\n",
    "input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_length)).to(device)\n",
    "\n",
    "# Forward pass\n",
    "logits, attention_weights = model(input_ids)\n",
    "\n",
    "print(f\"ü§ñ Complete Mini Transformer Test:\")\n",
    "print(f\"   Input shape: {input_ids.shape}\")\n",
    "print(f\"   Output logits shape: {logits.shape}\")\n",
    "print(f\"   Number of attention layers: {len(attention_weights)}\")\n",
    "print(f\"   Attention weights shape (per layer): {attention_weights[0].shape}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Count parameters by component\n",
    "embed_params = sum(p.numel() for p in model.embeddings.parameters())\n",
    "transformer_params = sum(p.numel() for block in model.transformer_blocks for p in block.parameters())\n",
    "classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "\n",
    "print(f\"\\nüìä Parameter Breakdown:\")\n",
    "print(f\"   Embeddings: {embed_params:,} ({100*embed_params/sum(p.numel() for p in model.parameters()):.1f}%)\")\n",
    "print(f\"   Transformer blocks: {transformer_params:,} ({100*transformer_params/sum(p.numel() for p in model.parameters()):.1f}%)\")\n",
    "print(f\"   Classifier: {classifier_params:,} ({100*classifier_params/sum(p.numel() for p in model.parameters()):.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Mini transformer ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Sentiment Analysis Dataset\n",
    "\n",
    "Create a simple sentiment analysis dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sentiment analysis dataset\n",
    "positive_samples = [\n",
    "    \"I love this movie! It's fantastic and amazing.\",\n",
    "    \"Great film with excellent acting and wonderful story.\",\n",
    "    \"Absolutely brilliant! Best movie I've ever seen.\",\n",
    "    \"Outstanding performance by all actors. Highly recommended.\",\n",
    "    \"Perfect blend of comedy and drama. Really enjoyed it.\",\n",
    "    \"Incredible cinematography and beautiful soundtrack.\",\n",
    "    \"Thrilling adventure with great special effects.\",\n",
    "    \"Heartwarming story that made me cry happy tears.\",\n",
    "    \"Amazing direction and superb character development.\",\n",
    "    \"Masterpiece! Every scene was perfectly crafted.\"\n",
    "] * 10  # Repeat to get more samples\n",
    "\n",
    "negative_samples = [\n",
    "    \"This movie is terrible. I hate it completely.\",\n",
    "    \"Boring and predictable plot. Waste of time.\",\n",
    "    \"Awful acting and poor script. Very disappointed.\",\n",
    "    \"Worst film ever. No redeeming qualities.\",\n",
    "    \"Confusing story with bad character development.\",\n",
    "    \"Terrible special effects and annoying soundtrack.\",\n",
    "    \"Slow pacing and boring dialogue throughout.\",\n",
    "    \"Disappointing ending. Plot makes no sense.\",\n",
    "    \"Poor direction and weak performances by actors.\",\n",
    "    \"Complete disaster. Don't waste your money.\"\n",
    "] * 10  # Repeat to get more samples\n",
    "\n",
    "# Combine and create labels\n",
    "texts = positive_samples + negative_samples\n",
    "labels = [1] * len(positive_samples) + [0] * len(negative_samples)\n",
    "\n",
    "print(f\"üìö Sentiment Analysis Dataset:\")\n",
    "print(f\"   Total samples: {len(texts)}\")\n",
    "print(f\"   Positive samples: {len(positive_samples)}\")\n",
    "print(f\"   Negative samples: {len(negative_samples)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "tokenizer = SimpleTokenizer(vocab_size=500)\n",
    "tokenizer.build_vocab(texts)\n",
    "\n",
    "# Create dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        token_ids = self.tokenizer.tokenize(text, max_length=self.max_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Show sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nüîç Sample Data:\")\n",
    "print(f\"   Text: {train_texts[0]}\")\n",
    "print(f\"   Label: {train_labels[0]} ({'Positive' if train_labels[0] == 1 else 'Negative'})\")\n",
    "print(f\"   Token IDs: {sample['input_ids'][:10].tolist()}...\")\n",
    "print(f\"   Decoded: {tokenizer.decode(sample['input_ids'][:15].tolist())}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training the Mini Transformer\n",
    "\n",
    "Let's train our transformer on sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, val_loader, num_epochs=5, learning_rate=1e-3):\n",
    "    \"\"\"Train the mini transformer model\"\"\"\n",
    "    \n",
    "    # TODO: Setup training components\n",
    "    # HINT: Loss function for classification, optimizer, and optional scheduler\n",
    "    \n",
    "    # TODO: Define loss function\n",
    "    # HINT: nn.CrossEntropyLoss() for multi-class classification\n",
    "    criterion = None  # Your code here\n",
    "    \n",
    "    # TODO: Define optimizer\n",
    "    # HINT: optim.Adam works well for transformers\n",
    "    # HINT: Use weight_decay=0.01 for regularization\n",
    "    optimizer = None  # Your code here\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"üöÄ Training Mini Transformer\")\n",
    "    print(f\"Epochs: {num_epochs}, Learning Rate: {learning_rate}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # TODO: Training step\n",
    "            # HINT: 1. Zero gradients, 2. Forward pass, 3. Compute loss, 4. Backward, 5. Update\n",
    "            \n",
    "            # Step 1: Zero gradients\n",
    "            None  # Your code here\n",
    "            \n",
    "            # Step 2: Forward pass\n",
    "            logits, _ = None  # Your code here - model(input_ids)\n",
    "            \n",
    "            # Step 3: Compute loss\n",
    "            loss = None  # Your code here\n",
    "            \n",
    "            # Step 4: Backward pass\n",
    "            None  # Your code here\n",
    "            \n",
    "            # Step 5: Update weights\n",
    "            None  # Your code here\n",
    "            \n",
    "            # Statistics (implemented for you)\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_acc = 100. * train_correct / train_total\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                logits, _ = model(input_ids)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = 100. * train_correct / train_total\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Training Complete!\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Update model configuration for our vocabulary\n",
    "config['vocab_size'] = len(tokenizer.vocab)\n",
    "model = MiniTransformer(**config).to(device)\n",
    "\n",
    "print(f\"üìã Model Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Train the model\n",
    "history = train_transformer(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Analyze Training Results\n",
    "\n",
    "Visualize training progress and analyze attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Training and Validation Loss', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title('Training and Validation Accuracy', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà Training Analysis:\")\n",
    "print(f\"   Final training accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"   Final validation accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"   Best validation accuracy: {max(history['val_acc']):.2f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_val_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "if train_val_gap > 10:\n",
    "    print(f\"   ‚ö†Ô∏è Possible overfitting (gap: {train_val_gap:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Good generalization (gap: {train_val_gap:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Attention Visualization\n",
    "\n",
    "Let's see what our transformer learned to pay attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, tokenizer, text, max_length=16):\n",
    "    \"\"\"Visualize attention patterns for a given text\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    token_ids = tokenizer.tokenize(text, max_length=max_length)\n",
    "    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits, attention_weights = model(input_ids)\n",
    "        prediction = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get tokens for visualization\n",
    "    tokens = []\n",
    "    for idx in token_ids:\n",
    "        if idx in tokenizer.inverse_vocab:\n",
    "            token = tokenizer.inverse_vocab[idx]\n",
    "            if token != tokenizer.pad_token:\n",
    "                tokens.append(token)\n",
    "    \n",
    "    # Limit to actual tokens (remove padding)\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    # Plot attention for each layer and head\n",
    "    num_layers = len(attention_weights)\n",
    "    num_heads = attention_weights[0].shape[1]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_layers, min(4, num_heads), figsize=(16, 4*num_layers))\n",
    "    if num_layers == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        layer_attention = attention_weights[layer][0].cpu().numpy()  # First batch\n",
    "        \n",
    "        for head in range(min(4, num_heads)):  # Show first 4 heads\n",
    "            head_attention = layer_attention[head][:num_tokens, :num_tokens]\n",
    "            \n",
    "            im = axes[layer, head].imshow(head_attention, cmap='Blues', aspect='auto')\n",
    "            axes[layer, head].set_xticks(range(num_tokens))\n",
    "            axes[layer, head].set_yticks(range(num_tokens))\n",
    "            axes[layer, head].set_xticklabels(tokens, rotation=45)\n",
    "            axes[layer, head].set_yticklabels(tokens)\n",
    "            axes[layer, head].set_title(f'Layer {layer+1}, Head {head+1}', fontweight='bold')\n",
    "            \n",
    "            if head == 0:\n",
    "                axes[layer, head].set_ylabel('Query Tokens')\n",
    "            if layer == num_layers - 1:\n",
    "                axes[layer, head].set_xlabel('Key Tokens')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show prediction\n",
    "    pred_class = torch.argmax(prediction, dim=-1).item()\n",
    "    confidence = prediction[0, pred_class].item()\n",
    "    \n",
    "    print(f\"üìù Text: {text}\")\n",
    "    print(f\"üéØ Prediction: {'Positive' if pred_class == 1 else 'Negative'} (confidence: {confidence:.3f})\")\n",
    "    print(f\"üîç Attention patterns show what the model focuses on\")\n",
    "\n",
    "# Test attention visualization\n",
    "test_examples = [\n",
    "    \"I love this amazing movie!\",\n",
    "    \"This film is terrible and boring.\",\n",
    "    \"Great acting but poor story.\"\n",
    "]\n",
    "\n",
    "for text in test_examples:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    visualize_attention(model, tokenizer, text)\n",
    "\n",
    "print(f\"\\nüß† Attention Analysis:\")\n",
    "print(f\"   ‚Ä¢ Different heads learn different patterns\")\n",
    "print(f\"   ‚Ä¢ Some heads focus on specific sentiment words\")\n",
    "print(f\"   ‚Ä¢ Later layers show more refined attention patterns\")\n",
    "print(f\"   ‚Ä¢ Attention helps model understand context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Critical Analysis & Applications (10 minutes)\n",
    "\n",
    "**Goal:** Reflect on transformer capabilities, limitations, and real-world impact\n",
    "\n",
    "## 6.1 Performance Comparison\n",
    "\n",
    "Let's compare our transformer with simpler approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a simple baseline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare data for sklearn\n",
    "X_train = train_texts\n",
    "y_train = train_labels\n",
    "X_val = val_texts\n",
    "y_val = val_labels\n",
    "\n",
    "# TF-IDF + Logistic Regression baseline\n",
    "vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_val_tfidf)\n",
    "lr_accuracy = accuracy_score(y_val, y_pred_lr)\n",
    "\n",
    "# Get transformer accuracy on validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        logits, _ = model(input_ids)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "transformer_accuracy = 100. * correct / total\n",
    "\n",
    "# Comparison\n",
    "print(f\"üìä Model Comparison:\")\n",
    "print(f\"   TF-IDF + Logistic Regression: {lr_accuracy*100:.2f}%\")\n",
    "print(f\"   Mini Transformer: {transformer_accuracy:.2f}%\")\n",
    "\n",
    "improvement = transformer_accuracy - (lr_accuracy * 100)\n",
    "if improvement > 0:\n",
    "    print(f\"   üéØ Transformer improvement: +{improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"   üìù Note: Simple baseline is competitive for this small dataset\")\n",
    "\n",
    "# Complexity comparison\n",
    "transformer_params = sum(p.numel() for p in model.parameters())\n",
    "lr_params = X_train_tfidf.shape[1] + 1  # Features + bias\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Complexity Comparison:\")\n",
    "print(f\"   Transformer parameters: {transformer_params:,}\")\n",
    "print(f\"   Logistic Regression parameters: {lr_params:,}\")\n",
    "print(f\"   Parameter ratio: {transformer_params / lr_params:.1f}x more\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Transformers excel with larger datasets\")\n",
    "print(f\"   ‚Ä¢ Simple models can be competitive on small/simple tasks\")\n",
    "print(f\"   ‚Ä¢ Transformers capture complex patterns and context\")\n",
    "print(f\"   ‚Ä¢ Trade-off between complexity and performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Computational Complexity Analysis\n",
    "\n",
    "Understanding the computational costs of transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze computational complexity\n",
    "def analyze_complexity(d_model, sequence_length, num_heads, d_ff):\n",
    "    \"\"\"Analyze transformer computational complexity\"\"\"\n",
    "    \n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    # Attention complexity: O(L^2 * d_model) where L = sequence length\n",
    "    attention_ops = sequence_length ** 2 * d_model\n",
    "    \n",
    "    # Feed-forward complexity: O(L * d_model * d_ff)\n",
    "    feedforward_ops = sequence_length * d_model * d_ff\n",
    "    \n",
    "    total_ops = attention_ops + feedforward_ops\n",
    "    \n",
    "    return {\n",
    "        'attention': attention_ops,\n",
    "        'feedforward': feedforward_ops,\n",
    "        'total': total_ops\n",
    "    }\n",
    "\n",
    "# Analyze different sequence lengths\n",
    "sequence_lengths = [32, 128, 512, 1024, 2048]\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "\n",
    "results = []\n",
    "for seq_len in sequence_lengths:\n",
    "    complexity = analyze_complexity(d_model, seq_len, num_heads, d_ff)\n",
    "    results.append(complexity)\n",
    "\n",
    "# Plot complexity scaling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Linear vs quadratic scaling\n",
    "attention_ops = [r['attention'] for r in results]\n",
    "feedforward_ops = [r['feedforward'] for r in results]\n",
    "\n",
    "ax1.loglog(sequence_lengths, attention_ops, 'r-o', label='Attention (Quadratic)', linewidth=2)\n",
    "ax1.loglog(sequence_lengths, feedforward_ops, 'b-o', label='Feed-Forward (Linear)', linewidth=2)\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('Operations (log scale)')\n",
    "ax1.set_title('Computational Complexity Scaling', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage (attention matrices)\n",
    "memory_usage = [seq_len ** 2 * num_heads for seq_len in sequence_lengths]\n",
    "ax2.semilogy(sequence_lengths, memory_usage, 'g-o', linewidth=2)\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Attention Matrix Size (log scale)')\n",
    "ax2.set_title('Memory Usage for Attention', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Complexity Analysis:\")\n",
    "print(f\"   Attention: O(L¬≤¬∑d) - quadratic in sequence length\")\n",
    "print(f\"   Feed-forward: O(L¬∑d¬∑d_ff) - linear in sequence length\")\n",
    "print(f\"   Memory: O(L¬≤¬∑h) - quadratic in sequence length for attention matrices\")\n",
    "\n",
    "print(f\"\\nüéØ Scaling Implications:\")\n",
    "for i, seq_len in enumerate(sequence_lengths[:3]):\n",
    "    ops = results[i]['total']\n",
    "    print(f\"   Length {seq_len:4d}: {ops/1e6:.1f}M operations\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Challenges:\")\n",
    "print(f\"   ‚Ä¢ Long sequences are computationally expensive\")\n",
    "print(f\"   ‚Ä¢ Memory usage grows quadratically\")\n",
    "print(f\"   ‚Ä¢ Modern models use techniques like sparse attention\")\n",
    "print(f\"   ‚Ä¢ Trade-offs between sequence length and model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Real-World Applications and Ethics\n",
    "\n",
    "Let's reflect on the broader implications of transformer technology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display timeline of transformer evolution\n",
    "transformer_timeline = {\n",
    "    2017: \"Transformer (Attention Is All You Need)\",\n",
    "    2018: \"BERT (Bidirectional Encoder Representations)\",\n",
    "    2019: \"GPT-2 (Language model with 1.5B parameters)\",\n",
    "    2020: \"GPT-3 (175B parameters, few-shot learning)\",\n",
    "    2021: \"T5, Switch Transformer, Codex\",\n",
    "    2022: \"ChatGPT, GPT-3.5, PaLM (540B parameters)\",\n",
    "    2023: \"GPT-4, Claude, LLaMA, Bard\",\n",
    "    2024: \"Multimodal models, reasoning improvements\"\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "years = list(transformer_timeline.keys())\n",
    "models = list(transformer_timeline.values())\n",
    "\n",
    "# Create timeline\n",
    "y_pos = range(len(years))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(years)))\n",
    "\n",
    "bars = ax.barh(y_pos, [1]*len(years), color=colors, alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([f\"{year}: {model}\" for year, model in transformer_timeline.items()])\n",
    "ax.set_xlabel('Timeline')\n",
    "ax.set_title('Evolution of Transformer Models', fontsize=16, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Remove x-axis ticks since we just want the timeline\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üöÄ Transformer Revolution:\")\n",
    "print(\"   ‚Ä¢ From research paper to powering ChatGPT in just 6 years\")\n",
    "print(\"   ‚Ä¢ Exponential growth in model size and capabilities\")\n",
    "print(\"   ‚Ä¢ Foundation for the current AI revolution\")\n",
    "\n",
    "# Applications analysis\n",
    "applications = {\n",
    "    \"Natural Language Processing\": [\n",
    "        \"Machine translation (Google Translate)\",\n",
    "        \"Text summarization and generation\",\n",
    "        \"Question answering systems\",\n",
    "        \"Sentiment analysis and classification\"\n",
    "    ],\n",
    "    \"Code Generation\": [\n",
    "        \"GitHub Copilot (code completion)\", \n",
    "        \"Automated bug fixing\",\n",
    "        \"Code explanation and documentation\",\n",
    "        \"Programming language translation\"\n",
    "    ],\n",
    "    \"Multimodal AI\": [\n",
    "        \"Image captioning and description\",\n",
    "        \"Visual question answering\",\n",
    "        \"Text-to-image generation (DALL-E)\",\n",
    "        \"Video understanding and generation\"\n",
    "    ],\n",
    "    \"Scientific Applications\": [\n",
    "        \"Protein folding prediction (AlphaFold)\",\n",
    "        \"Drug discovery and molecular design\", \n",
    "        \"Scientific literature analysis\",\n",
    "        \"Mathematical theorem proving\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nüåç Real-World Applications:\")\n",
    "for category, apps in applications.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for app in apps:\n",
    "        print(f\"   ‚Ä¢ {app}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Critical Reflection Questions\n",
    "\n",
    "**TODO: Answer these questions based on your experience with transformers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Technical Understanding\n",
    "\n",
    "**How does the attention mechanism allow transformers to capture long-range dependencies better than RNNs?**\n",
    "\n",
    "[TODO: Explain how attention allows direct connections between distant words, while RNNs must pass information through many time steps. Discuss the vanishing gradient problem in RNNs and how attention circumvents it.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Architecture Design\n",
    "\n",
    "**Why do transformers use multi-head attention instead of single-head attention?**\n",
    "\n",
    "[TODO: Discuss how multiple heads can focus on different types of relationships (syntactic, semantic, positional), increased model capacity, parallel processing benefits, and specialization of different heads.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Scaling and Efficiency\n",
    "\n",
    "**What are the main computational bottlenecks in transformer models, and how might they be addressed?**\n",
    "\n",
    "[TODO: Identify quadratic attention complexity, memory requirements, discuss solutions like sparse attention, linear attention, efficient architectures, and hardware optimizations.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Comparison Analysis\n",
    "\n",
    "**Compare transformers with CNNs and RNNs. When would you choose each architecture?**\n",
    "\n",
    "**Transformers:**\n",
    "[TODO: List advantages like parallelization, long-range dependencies, attention interpretability, and disadvantages like computational cost, quadratic scaling]\n",
    "\n",
    "**CNNs:**\n",
    "[TODO: Discuss advantages for spatial data, efficiency, translation invariance, and when to use for computer vision tasks]\n",
    "\n",
    "**RNNs:**\n",
    "[TODO: Discuss advantages for sequential data with limited memory, online learning, but disadvantages with long sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Ethical Considerations\n",
    "\n",
    "**What are the main ethical concerns with large transformer-based language models like GPT-4?**\n",
    "\n",
    "**Bias and Fairness:**\n",
    "[TODO: Discuss how training data bias affects model outputs, representation issues, and fairness across different groups]\n",
    "\n",
    "**Misinformation:**\n",
    "[TODO: Address concerns about generating false information, deepfakes, and the challenge of distinguishing AI-generated content]\n",
    "\n",
    "**Economic Impact:**\n",
    "[TODO: Discuss job displacement, automation of knowledge work, and economic inequality]\n",
    "\n",
    "**Privacy and Security:**\n",
    "[TODO: Address data privacy concerns, potential for misuse, and security vulnerabilities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Future Directions\n",
    "\n",
    "**What future developments in transformer technology are you most excited about? Most concerned about?**\n",
    "\n",
    "**Exciting Developments:**\n",
    "[TODO: Discuss potential positive applications, scientific breakthroughs, educational tools, accessibility improvements]\n",
    "\n",
    "**Concerning Developments:**\n",
    "[TODO: Address potential negative uses, societal impacts, need for regulation, ensuring beneficial development]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Implementation Insights\n",
    "\n",
    "**What was the most challenging aspect of implementing the transformer architecture? What surprised you?**\n",
    "\n",
    "[TODO: Reflect on the implementation experience - perhaps the complexity of attention matrices, the importance of residual connections, how the pieces fit together, or the gap between theory and practice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Reflection\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! In this assignment, you have:\n",
    "\n",
    "**Built a transformer from scratch** with all core components including attention, embeddings, and feed-forward networks  \n",
    "**Implemented the attention mechanism** that revolutionized natural language processing  \n",
    "**Trained a working model** on sentiment analysis and achieved meaningful results  \n",
    "**Analyzed attention patterns** to understand what the model learned  \n",
    "**Explored computational complexity** and scaling challenges  \n",
    "**Reflected critically** on the broader implications of transformer technology  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**TODO: Write 4-5 key insights from this assignment:**\n",
    "\n",
    "1. [TODO: Your first key takeaway about the attention mechanism and its revolutionary impact]\n",
    "2. [TODO: Your second key takeaway about the architecture design and why each component matters]\n",
    "3. [TODO: Your third key takeaway about computational complexity and scaling challenges]\n",
    "4. [TODO: Your fourth key takeaway about real-world applications and their impact]\n",
    "5. [TODO: Your fifth key takeaway about ethical considerations and responsible AI development]\n",
    "\n",
    "### Evolution from Previous Assignments\n",
    "\n",
    "**TODO: Compare transformers to previous architectures you've studied:**\n",
    "\n",
    "**From Linear Models (CA1) to Transformers:**\n",
    "[TODO: Discuss the journey from simple linear relationships to complex attention mechanisms]\n",
    "\n",
    "**From Neural Networks (CA3) to Transformers:**\n",
    "[TODO: Compare basic MLPs to sophisticated transformer blocks]\n",
    "\n",
    "**From CNNs (CA4) to Transformers:**\n",
    "[TODO: Contrast spatial processing in CNNs with sequential attention in transformers]\n",
    "\n",
    "### Looking Forward\n",
    "\n",
    "**TODO: What aspects of transformers or NLP would you like to explore further?**\n",
    "\n",
    "[TODO: Mention interests in large language models, multimodal transformers, specific applications, or research directions]\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "**TODO: Write a comprehensive reflection (200-300 words) on your experience with transformers:**\n",
    "\n",
    "[TODO: Your final reflection here - discuss the elegance and power of the attention mechanism, how it connects to real-world AI systems, what surprised you about the implementation, thoughts on the rapid progress in AI, and considerations for the responsible development and deployment of transformer-based systems]\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Complete!**\n",
    "\n",
    "Make sure to:\n",
    "1. Complete all TODO sections\n",
    "2. Test your implementations thoroughly\n",
    "3. Answer all reflection questions thoughtfully\n",
    "4. Save your notebook and export as PDF\n",
    "5. Submit both .ipynb and .pdf files\n",
    "6. Include your name and student ID at the top\n",
    "\n",
    "You've just implemented the architecture that powers ChatGPT, BERT, and the current AI revolution. Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}