{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment 1: Introduction to Linear Regression\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Student ID:** [Your Student ID]  \n",
    "**Date:** [Today's Date]  \n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to your first machine learning assignment! In this notebook, you'll journey from mathematical foundations to practical implementation of linear regression. You'll build your own linear regression from scratch, then compare it with professional ML libraries.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand linear regression mathematics\n",
    "- Implement gradient descent from scratch\n",
    "- Apply ML to real housing price data\n",
    "- Reflect on ML vs. traditional programming\n",
    "\n",
    "**Estimated Time:** 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundation (30 minutes)\n",
    "\n",
    "Before we code, let's understand the mathematics behind linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understanding the Linear Equation\n",
    "\n",
    "Linear regression finds the best line through data points. For one feature:\n",
    "\n",
    "**y = mx + b**\n",
    "\n",
    "For multiple features:\n",
    "\n",
    "**y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ**\n",
    "\n",
    "Or in vector form: **y = X·θ**\n",
    "\n",
    "Where:\n",
    "- **y** = predicted value\n",
    "- **X** = feature matrix\n",
    "- **θ** = parameters (weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some simple synthetic data to visualize linear regression\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.rand(50, 1) * 10  # 50 random points between 0 and 10\n",
    "y_simple = 2.5 * X_simple.flatten() + 1.0 + np.random.randn(50) * 1.5  # y = 2.5x + 1 + noise\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, alpha=0.6, color='blue', label='Data Points')\n",
    "plt.xlabel('X (Feature)')\n",
    "plt.ylabel('y (Target)')\n",
    "plt.title('Simple Linear Relationship with Noise')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: {X_simple.shape}\")\n",
    "print(f\"Target shape: {y_simple.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Cost Function\n",
    "\n",
    "We need to measure how \"wrong\" our predictions are. The **Mean Squared Error (MSE)** cost function is:\n",
    "\n",
    "**J(θ) = (1/2m) × Σ(hθ(x⁽ⁱ⁾) - y⁽ⁱ⁾)²**\n",
    "\n",
    "Where:\n",
    "- **m** = number of training examples\n",
    "- **hθ(x⁽ⁱ⁾)** = prediction for example i\n",
    "- **y⁽ⁱ⁾** = actual value for example i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    X (array): Feature matrix with bias column\n",
    "    y (array): Target values\n",
    "    theta (array): Parameters [bias, weight1, weight2, ...]\n",
    "    \n",
    "    Returns:\n",
    "    cost (float): Mean squared error\n",
    "    \"\"\"\n",
    "    m = len(y)  # number of examples\n",
    "    \n",
    "    # TODO: Calculate predictions using X @ theta\n",
    "    predictions = X @ theta\n",
    "    \n",
    "    # TODO: Calculate squared errors\n",
    "    errors = predictions - y\n",
    "    squared_errors = errors ** 2\n",
    "    \n",
    "    # TODO: Calculate mean squared error\n",
    "    cost = np.sum(squared_errors) / (2 * m)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Test the cost function\n",
    "# Add bias column to X_simple\n",
    "X_simple_with_bias = np.column_stack([np.ones(X_simple.shape[0]), X_simple])\n",
    "\n",
    "# Test with random parameters\n",
    "test_theta = np.array([0.0, 1.0])  # [bias, weight]\n",
    "test_cost = compute_cost(X_simple_with_bias, y_simple, test_theta)\n",
    "\n",
    "print(f\"Test cost with theta=[0, 1]: {test_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradient Descent\n",
    "\n",
    "To minimize the cost function, we use **gradient descent**. We update parameters iteratively:\n",
    "\n",
    "**θⱼ := θⱼ - α × (∂J(θ)/∂θⱼ)**\n",
    "\n",
    "For linear regression, the gradient is:\n",
    "\n",
    "**∂J(θ)/∂θⱼ = (1/m) × Σ(hθ(x⁽ⁱ⁾) - y⁽ⁱ⁾) × xⱼ⁽ⁱ⁾**\n",
    "\n",
    "Where **α** is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradients for gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    X (array): Feature matrix with bias column\n",
    "    y (array): Target values  \n",
    "    theta (array): Current parameters\n",
    "    \n",
    "    Returns:\n",
    "    gradients (array): Gradients for each parameter\n",
    "    \"\"\"\n",
    "    m = len(y)  # number of examples\n",
    "    \n",
    "    # TODO: Calculate predictions\n",
    "    predictions = X @ theta\n",
    "    \n",
    "    # TODO: Calculate errors\n",
    "    errors = predictions - y\n",
    "    \n",
    "    # TODO: Calculate gradients using matrix operations\n",
    "    # Gradient = (1/m) * X.T @ errors\n",
    "    gradients = (1/m) * X.T @ errors\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Test gradient computation\n",
    "test_gradients = compute_gradients(X_simple_with_bias, y_simple, test_theta)\n",
    "print(f\"Gradients with theta=[0, 1]: {test_gradients}\")\n",
    "print(f\"Gradient for bias: {test_gradients[0]:.4f}\")\n",
    "print(f\"Gradient for weight: {test_gradients[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: From-Scratch Implementation (45 minutes)\n",
    "\n",
    "Now let's build our own Linear Regression class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation using gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate (float): Step size for gradient descent\n",
    "        max_iterations (int): Maximum number of iterations\n",
    "        tolerance (float): Convergence tolerance\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def _add_bias_column(self, X):\n",
    "        \"\"\"\n",
    "        Add bias column (column of ones) to feature matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Feature matrix\n",
    "        \n",
    "        Returns:\n",
    "        X_with_bias (array): Feature matrix with bias column\n",
    "        \"\"\"\n",
    "        # TODO: Add a column of ones to the beginning of X\n",
    "        bias_column = np.ones((X.shape[0], 1))\n",
    "        X_with_bias = np.column_stack([bias_column, X])\n",
    "        return X_with_bias\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the linear regression model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Training features\n",
    "        y (array): Training targets\n",
    "        \"\"\"\n",
    "        # Add bias column\n",
    "        X_with_bias = self._add_bias_column(X)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        n_features = X_with_bias.shape[1]\n",
    "        # TODO: Initialize theta with small random values\n",
    "        self.theta = np.random.normal(0, 0.01, n_features)\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.cost_history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # TODO: Compute current cost\n",
    "            current_cost = compute_cost(X_with_bias, y, self.theta)\n",
    "            self.cost_history.append(current_cost)\n",
    "            \n",
    "            # TODO: Compute gradients\n",
    "            gradients = compute_gradients(X_with_bias, y, self.theta)\n",
    "            \n",
    "            # TODO: Update parameters\n",
    "            new_theta = self.theta - self.learning_rate * gradients\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(self.theta, new_theta, atol=self.tolerance):\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "                \n",
    "            self.theta = new_theta\n",
    "            \n",
    "            # Print progress every 100 iterations\n",
    "            if (iteration + 1) % 100 == 0:\n",
    "                print(f\"Iteration {iteration + 1}: Cost = {current_cost:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Features to predict\n",
    "        \n",
    "        Returns:\n",
    "        predictions (array): Predicted values\n",
    "        \"\"\"\n",
    "        if self.theta is None:\n",
    "            raise Exception(\"Model must be trained first. Call fit() method.\")\n",
    "        \n",
    "        # TODO: Add bias column and make predictions\n",
    "        X_with_bias = self._add_bias_column(X)\n",
    "        predictions = X_with_bias @ self.theta\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R-squared score.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Features\n",
    "        y (array): True values\n",
    "        \n",
    "        Returns:\n",
    "        r2 (float): R-squared score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # TODO: Calculate R-squared\n",
    "        ss_res = np.sum((y - y_pred) ** 2)  # Residual sum of squares\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)  # Total sum of squares\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return r2\n",
    "\n",
    "print(\"LinearRegression class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Test Your Implementation\n",
    "\n",
    "Let's test your linear regression on the synthetic data we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train our model\n",
    "print(\"Training our from-scratch Linear Regression...\")\n",
    "our_model = LinearRegression(learning_rate=0.1, max_iterations=1000)\n",
    "our_model.fit(X_simple, y_simple)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ours = our_model.predict(X_simple)\n",
    "\n",
    "# Calculate metrics\n",
    "our_r2 = our_model.score(X_simple, y_simple)\n",
    "our_mse = mean_squared_error(y_simple, y_pred_ours)\n",
    "\n",
    "print(f\"\\nOur Model Results:\")\n",
    "print(f\"Final parameters (theta): {our_model.theta}\")\n",
    "print(f\"R² Score: {our_r2:.4f}\")\n",
    "print(f\"MSE: {our_mse:.4f}\")\n",
    "print(f\"Bias (intercept): {our_model.theta[0]:.4f}\")\n",
    "print(f\"Weight (slope): {our_model.theta[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with scikit-learn\n",
    "sklearn_model = SklearnLinearRegression()\n",
    "sklearn_model.fit(X_simple, y_simple)\n",
    "y_pred_sklearn = sklearn_model.predict(X_simple)\n",
    "\n",
    "sklearn_r2 = sklearn_model.score(X_simple, y_simple)\n",
    "sklearn_mse = mean_squared_error(y_simple, y_pred_sklearn)\n",
    "\n",
    "print(f\"Scikit-learn Results:\")\n",
    "print(f\"R² Score: {sklearn_r2:.4f}\")\n",
    "print(f\"MSE: {sklearn_mse:.4f}\")\n",
    "print(f\"Bias (intercept): {sklearn_model.intercept_:.4f}\")\n",
    "print(f\"Weight (slope): {sklearn_model.coef_[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"R² difference: {abs(our_r2 - sklearn_r2):.6f}\")\n",
    "print(f\"MSE difference: {abs(our_mse - sklearn_mse):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Data and predictions\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_simple, y_simple, alpha=0.6, color='blue', label='Data')\n",
    "plt.plot(X_simple, y_pred_ours, 'r-', label=f'Our Model (R²={our_r2:.3f})', linewidth=2)\n",
    "plt.plot(X_simple, y_pred_sklearn, 'g--', label=f'Scikit-learn (R²={sklearn_r2:.3f})', linewidth=2)\n",
    "plt.xlabel('X (Feature)')\n",
    "plt.ylabel('y (Target)')\n",
    "plt.title('Model Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cost function history\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(our_model.cost_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actual vs Predicted\n",
    "plt.subplot(1, 3, 3)\n",
    "min_val = min(y_simple.min(), y_pred_ours.min())\n",
    "max_val = max(y_simple.max(), y_pred_ours.max())\n",
    "plt.scatter(y_simple, y_pred_ours, alpha=0.6, color='red', label='Our Model')\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reflection Questions\n",
    "\n",
    "**TODO: Answer these questions in the markdown cells below:**\n",
    "\n",
    "1. How close are your results to scikit-learn's implementation?\n",
    "2. What do you notice about the convergence of the cost function?\n",
    "3. What happens if you change the learning rate? Try different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer for Question 1:**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer for Question 2:**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer for Question 3:**\n",
    "\n",
    "[TODO: Experiment with different learning rates and write your observations here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Real-World Application (30 minutes)\n",
    "\n",
    "Now let's apply our linear regression to real housing price data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "print(\"Loading Boston Housing dataset...\")\n",
    "boston_data = load_boston()\n",
    "X_boston = boston_data.data\n",
    "y_boston = boston_data.target\n",
    "feature_names = boston_data.feature_names\n",
    "\n",
    "print(f\"Dataset shape: {X_boston.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Target range: ${y_boston.min():.1f}K - ${y_boston.max():.1f}K\")\n",
    "print(f\"\\nFirst few samples:\")\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df_boston = pd.DataFrame(X_boston, columns=feature_names)\n",
    "df_boston['PRICE'] = y_boston\n",
    "print(df_boston.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Exploration\n",
    "\n",
    "Let's understand our data before applying machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df_boston.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {df_boston.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some key relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Select some interesting features to plot\n",
    "interesting_features = ['RM', 'LSTAT', 'DIS', 'CRIM', 'TAX', 'AGE']\n",
    "feature_descriptions = {\n",
    "    'RM': 'Average number of rooms',\n",
    "    'LSTAT': '% lower status population',\n",
    "    'DIS': 'Distance to employment centers',\n",
    "    'CRIM': 'Crime rate',\n",
    "    'TAX': 'Property tax rate',\n",
    "    'AGE': '% built before 1940'\n",
    "}\n",
    "\n",
    "for i, feature in enumerate(interesting_features):\n",
    "    axes[i].scatter(df_boston[feature], df_boston['PRICE'], alpha=0.6)\n",
    "    axes[i].set_xlabel(f'{feature} ({feature_descriptions[feature]})')\n",
    "    axes[i].set_ylabel('House Price ($1000s)')\n",
    "    axes[i].set_title(f'Price vs {feature}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(df_boston[feature], df_boston['PRICE'])[0, 1]\n",
    "    axes[i].text(0.05, 0.95, f'r = {correlation:.3f}', \n",
    "                transform=axes[i].transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Preprocessing\n",
    "\n",
    "For gradient descent to work well, we need to scale our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_boston, y_boston, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling applied:\")\n",
    "print(f\"Original range - min: {X_train.min():.2f}, max: {X_train.max():.2f}\")\n",
    "print(f\"Scaled range - min: {X_train_scaled.min():.2f}, max: {X_train_scaled.max():.2f}\")\n",
    "print(f\"Scaled mean: {X_train_scaled.mean():.6f}, std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply Both Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our from-scratch model\n",
    "print(\"Training our Linear Regression on housing data...\")\n",
    "our_housing_model = LinearRegression(learning_rate=0.1, max_iterations=2000)\n",
    "our_housing_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_ours = our_housing_model.predict(X_train_scaled)\n",
    "y_test_pred_ours = our_housing_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics for our model\n",
    "our_train_r2 = our_housing_model.score(X_train_scaled, y_train)\n",
    "our_test_r2 = our_housing_model.score(X_test_scaled, y_test)\n",
    "our_train_mse = mean_squared_error(y_train, y_train_pred_ours)\n",
    "our_test_mse = mean_squared_error(y_test, y_test_pred_ours)\n",
    "\n",
    "print(f\"\\nOur Model - Housing Data Results:\")\n",
    "print(f\"Training R²: {our_train_r2:.4f}\")\n",
    "print(f\"Test R²: {our_test_r2:.4f}\")\n",
    "print(f\"Training MSE: {our_train_mse:.4f}\")\n",
    "print(f\"Test MSE: {our_test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scikit-learn model for comparison\n",
    "print(\"Training scikit-learn model...\")\n",
    "sklearn_housing_model = SklearnLinearRegression()\n",
    "sklearn_housing_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_sklearn = sklearn_housing_model.predict(X_train_scaled)\n",
    "y_test_pred_sklearn = sklearn_housing_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "sklearn_train_r2 = sklearn_housing_model.score(X_train_scaled, y_train)\n",
    "sklearn_test_r2 = sklearn_housing_model.score(X_test_scaled, y_test)\n",
    "sklearn_train_mse = mean_squared_error(y_train, y_train_pred_sklearn)\n",
    "sklearn_test_mse = mean_squared_error(y_test, y_test_pred_sklearn)\n",
    "\n",
    "print(f\"Scikit-learn - Housing Data Results:\")\n",
    "print(f\"Training R²: {sklearn_train_r2:.4f}\")\n",
    "print(f\"Test R²: {sklearn_test_r2:.4f}\")\n",
    "print(f\"Training MSE: {sklearn_train_mse:.4f}\")\n",
    "print(f\"Test MSE: {sklearn_test_mse:.4f}\")\n",
    "\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"Test R² difference: {abs(our_test_r2 - sklearn_test_r2):.6f}\")\n",
    "print(f\"Test MSE difference: {abs(our_test_mse - sklearn_test_mse):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize housing data results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Training predictions\n",
    "axes[0, 0].scatter(y_train, y_train_pred_ours, alpha=0.6, color='red', label='Our Model')\n",
    "axes[0, 0].scatter(y_train, y_train_pred_sklearn, alpha=0.6, color='blue', label='Scikit-learn')\n",
    "min_val = min(y_train.min(), y_train_pred_ours.min())\n",
    "max_val = max(y_train.max(), y_train_pred_ours.max())\n",
    "axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Actual Price ($1000s)')\n",
    "axes[0, 0].set_ylabel('Predicted Price ($1000s)')\n",
    "axes[0, 0].set_title(f'Training Set Predictions')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test predictions\n",
    "axes[0, 1].scatter(y_test, y_test_pred_ours, alpha=0.6, color='red', label=f'Our Model (R²={our_test_r2:.3f})')\n",
    "axes[0, 1].scatter(y_test, y_test_pred_sklearn, alpha=0.6, color='blue', label=f'Scikit-learn (R²={sklearn_test_r2:.3f})')\n",
    "min_val = min(y_test.min(), y_test_pred_ours.min())\n",
    "max_val = max(y_test.max(), y_test_pred_ours.max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Actual Price ($1000s)')\n",
    "axes[0, 1].set_ylabel('Predicted Price ($1000s)')\n",
    "axes[0, 1].set_title('Test Set Predictions')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning curve\n",
    "axes[1, 0].plot(our_housing_model.cost_history, 'b-', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Cost (MSE)')\n",
    "axes[1, 0].set_title('Learning Curve - Housing Data')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature importance (weights)\n",
    "feature_weights = our_housing_model.theta[1:]  # Exclude bias term\n",
    "sorted_indices = np.argsort(np.abs(feature_weights))[::-1]\n",
    "axes[1, 1].bar(range(len(feature_weights)), np.abs(feature_weights)[sorted_indices])\n",
    "axes[1, 1].set_xlabel('Features')\n",
    "axes[1, 1].set_ylabel('Absolute Weight')\n",
    "axes[1, 1].set_title('Feature Importance (|Weights|)')\n",
    "axes[1, 1].set_xticks(range(len(feature_weights)))\n",
    "axes[1, 1].set_xticklabels([feature_names[i] for i in sorted_indices], rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Analysis Questions\n",
    "\n",
    "**TODO: Answer these questions based on your results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How well does your model perform on the housing data? What does the R² score tell you?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Which features seem most important for predicting house prices? Look at the feature weights.**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How do training and test performance compare? What does this suggest about your model?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Critical Reflection (15 minutes)\n",
    "\n",
    "Now let's reflect on the bigger picture: How does machine learning differ from traditional programming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ML vs Traditional Programming\n",
    "\n",
    "**Traditional Programming:**\n",
    "- We write explicit rules and logic\n",
    "- Input + Program → Output\n",
    "- Deterministic and predictable\n",
    "- Example: `if temperature > 80: recommend_shorts()`\n",
    "\n",
    "**Machine Learning:**\n",
    "- We provide examples and let the algorithm learn patterns\n",
    "- Input + Output → Program (Model)\n",
    "- Statistical and probabilistic\n",
    "- Example: Learn from 10,000 weather/clothing combinations\n",
    "\n",
    "**TODO: Provide a specific example from your housing price prediction experience:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example from Housing Prediction:**\n",
    "\n",
    "**Traditional Programming Approach:**\n",
    "[TODO: Describe how you might write traditional code to estimate house prices]\n",
    "\n",
    "**Machine Learning Approach:**\n",
    "[TODO: Describe how ML learned to predict house prices from your experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 When to Use Machine Learning\n",
    "\n",
    "**TODO: Based on your experience, answer these questions:**\n",
    "\n",
    "**1. When would you choose machine learning over traditional programming?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. When would traditional programming be better than machine learning?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the main challenges you encountered with machine learning in this assignment?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Limitations of Linear Regression\n",
    "\n",
    "**TODO: Reflect on what you've learned:**\n",
    "\n",
    "**1. What assumptions does linear regression make? When might these be violated?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How could you improve your housing price predictions?**\n",
    "\n",
    "[TODO: Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Experimentation (Optional)\n",
    "\n",
    "If you have extra time, try these experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 1: Try different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = LinearRegression(learning_rate=lr, max_iterations=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    results.append((lr, test_r2, len(model.cost_history)))\n",
    "    print(f\"Learning rate {lr}: R² = {test_r2:.4f}, Iterations = {len(model.cost_history)}\")\n",
    "\n",
    "# TODO: What do you observe about different learning rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations about Learning Rates:**\n",
    "\n",
    "[TODO: Write your observations here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2: Feature selection - try using only the most important features\n",
    "# Get the top 5 most important features\n",
    "feature_weights = our_housing_model.theta[1:]  # Exclude bias\n",
    "top_features_idx = np.argsort(np.abs(feature_weights))[::-1][:5]\n",
    "\n",
    "print(f\"Top 5 most important features:\")\n",
    "for i, idx in enumerate(top_features_idx):\n",
    "    print(f\"{i+1}. {feature_names[idx]}: weight = {feature_weights[idx]:.4f}\")\n",
    "\n",
    "# Train model with only top features\n",
    "X_train_top = X_train_scaled[:, top_features_idx]\n",
    "X_test_top = X_test_scaled[:, top_features_idx]\n",
    "\n",
    "top_model = LinearRegression(learning_rate=0.1, max_iterations=1000)\n",
    "top_model.fit(X_train_top, y_train)\n",
    "\n",
    "top_test_r2 = top_model.score(X_test_top, y_test)\n",
    "print(f\"\\nR² with all features: {our_test_r2:.4f}\")\n",
    "print(f\"R² with top 5 features: {top_test_r2:.4f}\")\n",
    "print(f\"Difference: {our_test_r2 - top_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Submission\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! In this assignment, you have:\n",
    "\n",
    "✅ **Understood the mathematics** behind linear regression  \n",
    "✅ **Implemented gradient descent** from scratch  \n",
    "✅ **Built a complete ML pipeline** for housing price prediction  \n",
    "✅ **Compared your implementation** with professional libraries  \n",
    "✅ **Reflected critically** on ML vs traditional programming  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**TODO: Write 2-3 key insights from this assignment:**\n",
    "\n",
    "1. [TODO: Your first key takeaway]\n",
    "2. [TODO: Your second key takeaway]  \n",
    "3. [TODO: Your third key takeaway]\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "**TODO: Write a brief (100-150 words) final reflection on your experience with this assignment:**\n",
    "\n",
    "[TODO: Your final reflection here]\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Complete!** \n",
    "\n",
    "Make sure to:\n",
    "1. Save your notebook\n",
    "2. Export as HTML\n",
    "3. Submit both .ipynb and .html files\n",
    "4. Include your name and student ID at the top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}