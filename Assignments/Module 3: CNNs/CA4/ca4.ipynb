{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Coding Assignment 4: Advanced Computer Vision with CNNs\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Student ID:** [Your Student ID]  \n",
    "**Date:** [Today's Date]  \n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to advanced computer vision! In this assignment, you'll implement state-of-the-art CNN techniques used in modern AI systems. You'll work with ResNet architectures, transfer learning, model interpretability, and real-world optimization challenges.\n",
    "\n",
    "**Learning Goals:**\n",
    "- Implement ResNet-style architectures with skip connections\n",
    "- Apply transfer learning with pre-trained models\n",
    "- Work with complex color image datasets (CIFAR-10)\n",
    "- Analyze model interpretability using Grad-CAM\n",
    "- Optimize models for deployment and efficiency\n",
    "- Reflect on ethics and bias in computer vision systems\n",
    "\n",
    "**Estimated Time:** 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Progress tracking and utilities\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Advanced CNN Architecture (45 minutes)\n",
    "\n",
    "**Goal:** Implement ResNet-style architectures with skip connections and modern techniques\n",
    "\n",
    "## 1.1 Understanding ResNet Architecture\n",
    "\n",
    "ResNet (Residual Networks) revolutionized deep learning by solving the vanishing gradient problem with skip connections. Instead of learning `H(x)`, residual blocks learn `F(x) = H(x) - x`, making optimization easier.\n",
    "\n",
    "**Key Components:**\n",
    "- **Skip Connections:** `y = F(x) + x`\n",
    "- **Batch Normalization:** Normalizes layer inputs\n",
    "- **ReLU Activation:** Non-linear activation function\n",
    "- **Downsampling:** Reduces spatial dimensions while increasing channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-resnet-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference between traditional and residual learning\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Traditional CNN block\n",
    "ax1.text(0.5, 0.9, 'Input x', ha='center', va='center', fontsize=12, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax1.text(0.5, 0.7, 'Conv + BN + ReLU', ha='center', va='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax1.text(0.5, 0.5, 'Conv + BN + ReLU', ha='center', va='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax1.text(0.5, 0.3, 'Learn H(x)', ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='orange'))\n",
    "ax1.text(0.5, 0.1, 'Output H(x)', ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "\n",
    "# Add arrows\n",
    "for y in [0.8, 0.6, 0.4, 0.2]:\n",
    "    ax1.arrow(0.5, y, 0, -0.15, head_width=0.03, head_length=0.02, \n",
    "              fc='black', ec='black')\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Traditional CNN Block', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Residual block\n",
    "ax2.text(0.3, 0.9, 'Input x', ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax2.text(0.3, 0.7, 'Conv + BN + ReLU', ha='center', va='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax2.text(0.3, 0.5, 'Conv + BN', ha='center', va='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax2.text(0.7, 0.3, '+', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "ax2.text(0.3, 0.3, 'Learn F(x)', ha='center', va='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='orange'))\n",
    "ax2.text(0.7, 0.1, 'Output F(x) + x', ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "\n",
    "# Main path arrows\n",
    "for y in [0.8, 0.6, 0.4]:\n",
    "    ax2.arrow(0.3, y, 0, -0.15, head_width=0.02, head_length=0.02,\n",
    "              fc='blue', ec='blue')\n",
    "\n",
    "# Skip connection\n",
    "ax2.arrow(0.4, 0.9, 0.25, 0, head_width=0.02, head_length=0.02,\n",
    "          fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "ax2.arrow(0.65, 0.9, 0, -0.55, head_width=0.02, head_length=0.02,\n",
    "          fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "ax2.arrow(0.65, 0.35, 0.03, -0.02, head_width=0.02, head_length=0.02,\n",
    "          fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Final arrow\n",
    "ax2.arrow(0.7, 0.25, 0, -0.1, head_width=0.02, head_length=0.02,\n",
    "          fc='black', ec='black')\n",
    "\n",
    "ax2.text(0.55, 0.95, 'Skip Connection', ha='center', va='center', fontsize=10,\n",
    "         color='red', fontweight='bold')\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Residual Block', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîë Key Advantages of Skip Connections:\")\n",
    "print(\"   ‚Ä¢ Solve vanishing gradient problem\")\n",
    "print(\"   ‚Ä¢ Enable training of very deep networks (100+ layers)\")\n",
    "print(\"   ‚Ä¢ Improve gradient flow during backpropagation\")\n",
    "print(\"   ‚Ä¢ Allow identity mappings when needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resnet-implementation",
   "metadata": {},
   "source": [
    "## 1.2 Implementing Residual Blocks\n",
    "\n",
    "Let's implement the core building blocks of ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual-block-class",
   "metadata": {},
   "outputs": [],
   "source": "class BasicBlock(nn.Module):\n    \"\"\"Basic residual block for ResNet-18 and ResNet-34\"\"\"\n    \n    expansion = 1  # Output channels multiplier\n    \n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        \n        # TODO: Implement the first convolutional layer\n        # HINT: Use 3x3 conv, specified stride, padding=1, bias=False\n        # HINT: self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=?, stride=?, padding=?, bias=?)\n        self.conv1 = None  # Your code here\n        \n        # TODO: Implement batch normalization for first conv\n        # HINT: BatchNorm2d takes the number of output channels\n        self.bn1 = None  # Your code here\n        \n        # TODO: Implement the second convolutional layer\n        # HINT: Use 3x3 conv, stride=1, padding=1, bias=False\n        # HINT: Both input and output channels should be out_channels\n        self.conv2 = None  # Your code here\n        \n        # TODO: Implement batch normalization for second conv\n        self.bn2 = None  # Your code here\n        \n        # Downsample for skip connection when dimensions don't match\n        self.downsample = downsample\n        self.stride = stride\n        \n    def forward(self, x):\n        # Store input for skip connection\n        identity = x\n        \n        # TODO: Implement the main path\n        # HINT: Follow this sequence: conv1 -> batch_norm1 -> relu -> conv2 -> batch_norm2\n        # HINT: Use F.relu() for activation\n        # HINT: Structure: out = self.conv1(x), then out = self.bn1(out), etc.\n        \n        # Step 1: First convolution and batch norm\n        out = None  # Apply conv1 to x\n        out = None  # Apply bn1 to out  \n        out = None  # Apply ReLU to out\n        \n        # Step 2: Second convolution and batch norm (no ReLU yet!)\n        out = None  # Apply conv2 to out\n        out = None  # Apply bn2 to out\n        \n        # TODO: Apply downsample to identity if needed\n        # HINT: Check if self.downsample is not None, then apply it to identity\n        if self.downsample is not None:\n            identity = None  # Apply downsample to identity\n        \n        # TODO: Add skip connection and apply final ReLU\n        # HINT: Add identity to out, then apply ReLU to the result\n        out = None  # Add skip connection: out + identity\n        out = None  # Apply final ReLU\n        \n        return out\n\n# Test the basic block (this will fail until you implement the TODOs above)\nprint(\"üß± Testing Basic ResNet Block Implementation:\")\nprint(\"‚ö†Ô∏è This will fail until you implement the TODOs above!\")\n\ntry:\n    test_block = BasicBlock(64, 64)\n    test_input = torch.randn(1, 64, 32, 32)\n    test_output = test_block(test_input)\n    print(f\"‚úÖ SUCCESS! Input shape: {test_input.shape}\")\n    print(f\"‚úÖ SUCCESS! Output shape: {test_output.shape}\")\n    print(f\"‚úÖ SUCCESS! Parameters: {sum(p.numel() for p in test_block.parameters()):,}\")\n    \n    # Validation check\n    if test_output.shape == test_input.shape:\n        print(\"üéØ EXCELLENT! Skip connection preserved input dimensions\")\n    else:\n        print(\"‚ùå ERROR: Output shape doesn't match input shape\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Implementation incomplete: {e}\")\n    print(\"üí° Make sure to replace all 'None' with proper implementations\")\n    print(\"üí° Check that your conv layers and batch norms are correctly defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "advanced-cnn-architecture",
   "metadata": {},
   "source": [
    "## 1.3 Complete Advanced CNN Architecture\n",
    "\n",
    "Now let's build a complete ResNet-style architecture for CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-cnn-class",
   "metadata": {},
   "outputs": [],
   "source": "class AdvancedCNN(nn.Module):\n    \"\"\"ResNet-style CNN for CIFAR-10 classification\"\"\"\n    \n    def __init__(self, block=BasicBlock, layers=[2, 2, 2, 2], num_classes=10):\n        super(AdvancedCNN, self).__init__()\n        \n        self.in_channels = 64\n        \n        # TODO: Initial convolution layer\n        # HINT: For CIFAR-10, use 3x3 conv instead of 7x7 (like ImageNet ResNet)\n        # HINT: Use stride=1, padding=1, bias=False to preserve spatial dimensions\n        # HINT: Input channels=3 (RGB), output channels=64\n        self.conv1 = None  # Your code here\n        \n        # TODO: Batch normalization for initial conv\n        self.bn1 = None  # Your code here\n        \n        # TODO: Create residual layers using _make_layer helper\n        # HINT: Each layer contains multiple residual blocks\n        # HINT: layer1: 64 channels, layers[0] blocks, stride=1\n        # HINT: layer2: 128 channels, layers[1] blocks, stride=2 (downsamples)\n        # HINT: layer3: 256 channels, layers[2] blocks, stride=2 (downsamples)  \n        # HINT: layer4: 512 channels, layers[3] blocks, stride=2 (downsamples)\n        self.layer1 = None  # Your code here - use self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = None  # Your code here - use self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = None  # Your code here\n        self.layer4 = None  # Your code here\n        \n        # TODO: Global average pooling - reduces feature maps to 1x1\n        # HINT: Use nn.AdaptiveAvgPool2d((1, 1)) to get 1x1 output regardless of input size\n        self.avgpool = None  # Your code here\n        \n        # TODO: Final classifier layer\n        # HINT: Input features = 512 * block.expansion, output = num_classes\n        self.fc = None  # Your code here\n        \n        # TODO: Dropout for regularization\n        # HINT: Use nn.Dropout with p=0.1\n        self.dropout = None  # Your code here\n        \n        # Initialize weights\n        self._initialize_weights()\n        \n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        \"\"\"Create a layer with multiple residual blocks\"\"\"\n        downsample = None\n        \n        # TODO: Create downsample layer if needed\n        # HINT: Downsample needed when stride != 1 OR when channels don't match\n        # HINT: Check: stride != 1 or self.in_channels != out_channels * block.expansion\n        # HINT: Downsample uses 1x1 conv + batch norm to match dimensions\n        if None:  # Your condition here\n            downsample = nn.Sequential(\n                None,  # 1x1 conv: self.in_channels -> out_channels * block.expansion, stride=stride\n                None   # Batch norm for out_channels * block.expansion\n            )\n        \n        layers = []\n        \n        # TODO: First block (may need downsampling)\n        # HINT: Use block(self.in_channels, out_channels, stride, downsample)\n        layers.append(None)  # Your code here\n        \n        # Update in_channels for subsequent blocks\n        self.in_channels = out_channels * block.expansion\n        \n        # TODO: Add remaining blocks\n        # HINT: Loop from 1 to blocks (since first block already added)\n        # HINT: Remaining blocks: block(self.in_channels, out_channels) - no stride or downsample\n        for _ in range(1, blocks):\n            layers.append(None)  # Your code here\n        \n        return nn.Sequential(*layers)\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize network weights using He initialization\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        # TODO: Implement forward pass\n        # HINT: Follow this sequence: initial conv -> residual layers -> global pooling -> classifier\n        \n        # TODO: Initial convolution block\n        # HINT: conv1 -> bn1 -> relu\n        x = None  # Apply conv1\n        x = None  # Apply bn1  \n        x = None  # Apply ReLU\n        \n        # TODO: Pass through residual layers\n        # HINT: Apply layer1, layer2, layer3, layer4 in sequence\n        x = None  # Apply layer1\n        x = None  # Apply layer2\n        x = None  # Apply layer3\n        x = None  # Apply layer4\n        \n        # TODO: Global average pooling\n        # HINT: Reduces spatial dimensions to 1x1\n        x = None  # Apply avgpool\n        \n        # TODO: Flatten for classifier\n        # HINT: Use torch.flatten(x, 1) to flatten all dims except batch\n        x = None  # Flatten\n        \n        # TODO: Apply dropout and final classifier\n        x = None  # Apply dropout\n        x = None  # Apply fc layer\n        \n        return x\n    \n    def count_parameters(self):\n        \"\"\"Count total trainable parameters\"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n# Test the advanced CNN architecture (will fail until BasicBlock and AdvancedCNN are implemented)\nprint(\"üèóÔ∏è Testing Advanced CNN Architecture\")\nprint(\"‚ö†Ô∏è This requires both BasicBlock and AdvancedCNN to be implemented!\")\n\ntry:\n    advanced_cnn = AdvancedCNN().to(device)\n    \n    # Test forward pass\n    test_input = torch.randn(1, 3, 32, 32).to(device)\n    test_output = advanced_cnn(test_input)\n    \n    print(f\"‚úÖ SUCCESS! Architecture created successfully!\")\n    print(f\"‚úÖ Input shape: {test_input.shape}\")\n    print(f\"‚úÖ Output shape: {test_output.shape}\")\n    print(f\"‚úÖ Total parameters: {advanced_cnn.count_parameters():,}\")\n    \n    # Validation checks\n    if test_output.shape == (1, 10):\n        print(\"üéØ EXCELLENT! Output has correct shape for CIFAR-10 (batch_size, 10)\")\n    else:\n        print(f\"‚ùå ERROR: Expected output shape (1, 10), got {test_output.shape}\")\n        \n    param_count = advanced_cnn.count_parameters()\n    if 1000000 < param_count < 12000000:  # Reasonable range for ResNet\n        print(f\"üéØ EXCELLENT! Parameter count {param_count:,} is in reasonable range\")\n    else:\n        print(f\"‚ö†Ô∏è WARNING: Parameter count {param_count:,} seems unusual\")\n    \n    print(f\"\\nüìã Architecture Summary:\")\n    print(f\"   ‚Ä¢ Initial conv: 3 ‚Üí 64 channels\")\n    print(f\"   ‚Ä¢ Layer 1: 64 ‚Üí 64 channels (2 blocks)\")\n    print(f\"   ‚Ä¢ Layer 2: 64 ‚Üí 128 channels (2 blocks, downsample)\")\n    print(f\"   ‚Ä¢ Layer 3: 128 ‚Üí 256 channels (2 blocks, downsample)\")\n    print(f\"   ‚Ä¢ Layer 4: 256 ‚Üí 512 channels (2 blocks, downsample)\")\n    print(f\"   ‚Ä¢ Global average pooling + classifier\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Implementation incomplete: {e}\")\n    print(\"üí° Common issues:\")\n    print(\"   ‚Ä¢ Make sure BasicBlock is implemented first\")\n    print(\"   ‚Ä¢ Check that all 'None' are replaced with proper code\")\n    print(\"   ‚Ä¢ Verify layer dimensions and channel counts\")\n    print(\"   ‚Ä¢ Ensure _make_layer method is correctly implemented\")"
  },
  {
   "cell_type": "markdown",
   "id": "architecture-comparison",
   "metadata": {},
   "source": [
    "## 1.4 Architecture Comparison\n",
    "\n",
    "Let's compare different architectural choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-architectures",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Architecture comparison experiment\nprint(\"üîç Architecture Comparison Challenge\")\nprint(\"‚ö†Ô∏è  Complete this section after implementing BasicBlock and AdvancedCNN\")\n\n# TODO: Create different architecture variants for comparison\n# HINT: Use different layer configurations: [1,1,1,1], [2,2,2,2], [3,4,6,3]\n# HINT: Compare shallow vs standard vs deep ResNet architectures\n\nprint(\"üéØ Your Task:\")\nprint(\"1. Create 3 different AdvancedCNN models with varying depths\")\nprint(\"2. Count parameters for each model\")\nprint(\"3. Measure inference time for each model\")\nprint(\"4. Analyze the trade-offs between model complexity and speed\")\nprint(\"5. Create visualizations comparing the architectures\")\n\nprint(\"\\nüí° Implementation Guide:\")\nprint(\"   ‚Ä¢ Shallow ResNet: AdvancedCNN(layers=[1, 1, 1, 1])\")\nprint(\"   ‚Ä¢ Standard ResNet: AdvancedCNN(layers=[2, 2, 2, 2])\")  \nprint(\"   ‚Ä¢ Deep ResNet: AdvancedCNN(layers=[3, 4, 6, 3])\")\n\nprint(\"\\nüìä What to analyze:\")\nprint(\"   ‚Ä¢ Parameter count scaling with depth\")\nprint(\"   ‚Ä¢ Inference time vs model size\")\nprint(\"   ‚Ä¢ Memory usage considerations\")\nprint(\"   ‚Ä¢ Accuracy vs efficiency trade-offs\")\n\nprint(\"\\nüé® Suggested visualizations:\")\nprint(\"   ‚Ä¢ Bar charts comparing parameter counts\")\nprint(\"   ‚Ä¢ Scatter plot of inference time vs parameters\")\nprint(\"   ‚Ä¢ Architecture diagram showing layer depths\")\n\n# TODO: Implement this comparison after completing the basic architecture\ntry:\n    # Test if AdvancedCNN is properly implemented\n    test_model = AdvancedCNN(layers=[1, 1, 1, 1])\n    print(\"‚úÖ Ready for architecture comparison!\")\n    \n    # Your comparison code will go here\n    # architectures = {\n    #     'Shallow ResNet': AdvancedCNN(layers=[1, 1, 1, 1]),\n    #     'Standard ResNet': AdvancedCNN(layers=[2, 2, 2, 2]),\n    #     'Deep ResNet': AdvancedCNN(layers=[3, 4, 6, 3]),\n    # }\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è AdvancedCNN not ready: {e}\")\n    print(\"üí° Complete the architecture implementation first\")"
  },
  {
   "cell_type": "markdown",
   "id": "part2-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: CIFAR-10 Classification Challenge (60 minutes)\n",
    "\n",
    "**Goal:** Work with complex color image dataset and implement comprehensive data augmentation\n",
    "\n",
    "## 2.1 CIFAR-10 Dataset Exploration\n",
    "\n",
    "CIFAR-10 is significantly more challenging than MNIST:\n",
    "- **32√ó32 color images** (vs 28√ó28 grayscale)\n",
    "- **10 diverse classes** (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, trucks)\n",
    "- **Real-world objects** with varying poses, lighting, backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-cifar10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print(\"üì¶ Loading CIFAR-10 Dataset\")\n",
    "\n",
    "# Basic transforms for exploration\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# TODO: Load CIFAR-10 train and test sets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=basic_transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=basic_transform\n",
    ")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Classes: {cifar10_classes}\")\n",
    "\n",
    "# Analyze class distribution\n",
    "train_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "print(f\"\\nüìä Class Distribution:\")\n",
    "for i, (class_name, count) in enumerate(zip(cifar10_classes, class_counts)):\n",
    "    print(f\"   {i}: {class_name:12} - {count:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-cifar10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CIFAR-10 samples\n",
    "def show_cifar_samples():\n",
    "    \"\"\"Display sample images from each CIFAR-10 class\"\"\"\n",
    "    \n",
    "    # Load dataset without normalization for visualization\n",
    "    viz_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=False,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 8))\n",
    "    fig.suptitle('CIFAR-10 Dataset Samples', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Find samples for each class\n",
    "    class_samples = {i: [] for i in range(10)}\n",
    "    \n",
    "    for idx, (image, label) in enumerate(viz_dataset):\n",
    "        if len(class_samples[label]) < 2:\n",
    "            class_samples[label].append((image, idx))\n",
    "        \n",
    "        # Stop when we have enough samples\n",
    "        if all(len(samples) >= 2 for samples in class_samples.values()):\n",
    "            break\n",
    "    \n",
    "    # TODO: Display samples\n",
    "    for class_idx in range(10):\n",
    "        for sample_idx in range(2):\n",
    "            image, orig_idx = class_samples[class_idx][sample_idx]\n",
    "            \n",
    "            # Convert tensor to numpy for matplotlib\n",
    "            img_np = image.permute(1, 2, 0).numpy()\n",
    "            \n",
    "            axes[sample_idx, class_idx].imshow(img_np)\n",
    "            axes[sample_idx, class_idx].set_title(\n",
    "                f'{cifar10_classes[class_idx]}\\n#{orig_idx}', fontsize=10\n",
    "            )\n",
    "            axes[sample_idx, class_idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_cifar_samples()\n",
    "\n",
    "print(\"üîç Key Observations:\")\n",
    "print(\"   ‚Ä¢ Much more complex than MNIST digits\")\n",
    "print(\"   ‚Ä¢ Significant intra-class variation (different poses, colors)\")\n",
    "print(\"   ‚Ä¢ Challenging inter-class similarities (cat vs dog, deer vs horse)\")\n",
    "print(\"   ‚Ä¢ Real-world objects with backgrounds and occlusion\")\n",
    "print(\"   ‚Ä¢ This is why we need advanced CNN architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-augmentation",
   "metadata": {},
   "source": [
    "## 2.2 Advanced Data Augmentation Pipeline\n",
    "\n",
    "Data augmentation is crucial for CIFAR-10 to achieve good generalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "implement-augmentation",
   "metadata": {},
   "outputs": [],
   "source": "# Define comprehensive data augmentation\nprint(\"üé® Implementing Advanced Data Augmentation\")\n\n# TODO: Create training transforms with augmentation\n# HINT: Data augmentation helps the model generalize by showing it variations of the training data\n# HINT: Common augmentations: flip, rotate, translate, color changes, erasing\n# HINT: Always end with ToTensor() and Normalize()\n\n# HINT: CIFAR-10 normalization values:\n# mean = (0.4914, 0.4822, 0.4465) - computed from training set\n# std = (0.2023, 0.1994, 0.2010) - computed from training set\n\ntrain_transform = transforms.Compose([\n    # TODO: Add horizontal flip augmentation\n    # HINT: Use transforms.RandomHorizontalFlip(p=0.5) for 50% chance\n    None,  # Your code here\n    \n    # TODO: Add rotation augmentation  \n    # HINT: Use transforms.RandomRotation(degrees=10) for ¬±10 degree rotation\n    None,  # Your code here\n    \n    # TODO: Add translation augmentation\n    # HINT: Use transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)) for ¬±10% translation\n    None,  # Your code here\n    \n    # TODO: Add color jitter augmentation\n    # HINT: Use transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    None,  # Your code here\n    \n    # TODO: Add random erasing augmentation (optional but effective)\n    # HINT: Use transforms.RandomErasing(p=0.1, scale=(0.02, 0.25)) to randomly erase patches\n    None,  # Your code here\n    \n    # TODO: Convert to tensor\n    # HINT: Use transforms.ToTensor()\n    None,  # Your code here\n    \n    # TODO: Normalize with CIFAR-10 statistics\n    # HINT: Use transforms.Normalize(mean, std) with values above\n    None   # Your code here\n])\n\n# TODO: Create test transforms (no augmentation for consistent evaluation)\n# HINT: Test set should only have ToTensor() and Normalize() - no augmentation!\ntest_transform = transforms.Compose([\n    # TODO: Convert to tensor\n    None,  # Your code here\n    \n    # TODO: Normalize with same statistics as training\n    None   # Your code here\n])\n\n# Test your transforms\nprint(\"üß™ Testing Transform Implementations...\")\n\ntry:\n    # Create augmented datasets\n    train_dataset_aug = torchvision.datasets.CIFAR10(\n        root='./data', train=True, transform=train_transform\n    )\n\n    test_dataset_clean = torchvision.datasets.CIFAR10(\n        root='./data', train=False, transform=test_transform\n    )\n    \n    # Test that transforms work\n    sample_img, sample_label = train_dataset_aug[0]\n    \n    print(f\"‚úÖ SUCCESS! Augmented dataset created\")\n    print(f\"‚úÖ Training transforms: {len([t for t in train_transform.transforms if t is not None])} operations\")\n    print(f\"‚úÖ Test transforms: {len([t for t in test_transform.transforms if t is not None])} operations\")\n    print(f\"‚úÖ Sample image shape: {sample_img.shape}\")\n    print(f\"‚úÖ Sample image range: [{sample_img.min():.3f}, {sample_img.max():.3f}]\")\n    \n    # Validation checks\n    if sample_img.shape == (3, 32, 32):\n        print(\"üéØ EXCELLENT! Sample has correct CIFAR-10 dimensions\")\n    else:\n        print(f\"‚ùå ERROR: Expected (3, 32, 32), got {sample_img.shape}\")\n        \n    if -3 < sample_img.min() < 0 and 2 < sample_img.max() < 4:\n        print(\"üéØ EXCELLENT! Normalization applied correctly (values in expected range)\")\n    else:\n        print(f\"‚ö†Ô∏è WARNING: Unexpected normalized range [{sample_img.min():.3f}, {sample_img.max():.3f}]\")\n        print(\"   Expected roughly [-2.5, 2.5] after normalization\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Implementation incomplete: {e}\")\n    print(\"üí° Common issues:\")\n    print(\"   ‚Ä¢ Replace all 'None' with proper transform implementations\")\n    print(\"   ‚Ä¢ Make sure ToTensor() and Normalize() are included\")\n    print(\"   ‚Ä¢ Check that normalization values are tuples: (mean1, mean2, mean3), (std1, std2, std3)\")\n    print(\"   ‚Ä¢ Ensure transforms are in logical order\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation effects\n",
    "def visualize_augmentation():\n",
    "    \"\"\"Show the effect of data augmentation on sample images\"\"\"\n",
    "    \n",
    "    # Get original image\n",
    "    original_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=False,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    # Sample image\n",
    "    img_idx = 100\n",
    "    original_img, label = original_dataset[img_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 8))\n",
    "    fig.suptitle(f'Data Augmentation Effects - {cifar10_classes[label]}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Show original\n",
    "    axes[0, 0].imshow(original_img.permute(1, 2, 0))\n",
    "    axes[0, 0].set_title('Original', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # TODO: Show augmented versions\n",
    "    augment_only = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Create different augmentation pipelines for visualization\n",
    "    augmentations = [\n",
    "        transforms.Compose([transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor()]),\n",
    "        transforms.Compose([transforms.RandomRotation(degrees=15), transforms.ToTensor()]),\n",
    "        transforms.Compose([transforms.ColorJitter(brightness=0.3), transforms.ToTensor()]),\n",
    "        transforms.Compose([transforms.ColorJitter(contrast=0.3), transforms.ToTensor()]),\n",
    "        transforms.Compose([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), transforms.ToTensor()]),\n",
    "        transforms.Compose([transforms.ColorJitter(saturation=0.3), transforms.ToTensor()]),\n",
    "        transforms.Compose([transforms.RandomErasing(p=1.0), transforms.ToTensor()])\n",
    "    ]\n",
    "    \n",
    "    aug_names = ['Horizontal Flip', 'Rotation', 'Brightness', 'Contrast', \n",
    "                 'Translation', 'Saturation', 'Random Erasing']\n",
    "    \n",
    "    # Convert to PIL for augmentation\n",
    "    pil_img = transforms.ToPILImage()(original_img)\n",
    "    \n",
    "    for i, (aug, name) in enumerate(zip(augmentations, aug_names)):\n",
    "        aug_img = aug(pil_img)\n",
    "        \n",
    "        axes[0, i+1].imshow(aug_img.permute(1, 2, 0))\n",
    "        axes[0, i+1].set_title(name, fontsize=10)\n",
    "        axes[0, i+1].axis('off')\n",
    "    \n",
    "    # Show multiple random augmentations\n",
    "    full_aug = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    for i in range(8):\n",
    "        aug_img = full_aug(pil_img)\n",
    "        axes[1, i].imshow(aug_img.permute(1, 2, 0))\n",
    "        axes[1, i].set_title(f'Combined Aug {i+1}', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_augmentation()\n",
    "\n",
    "print(\"‚ú® Augmentation Benefits:\")\n",
    "print(\"   ‚Ä¢ Increases effective dataset size\")\n",
    "print(\"   ‚Ä¢ Improves model robustness to variations\")\n",
    "print(\"   ‚Ä¢ Reduces overfitting on training data\")\n",
    "print(\"   ‚Ä¢ Simulates real-world image variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## 2.3 Training Setup and Execution\n",
    "\n",
    "Now let's train our advanced CNN on CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-utilities",
   "metadata": {},
   "outputs": [],
   "source": "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.1):\n    \"\"\"Comprehensive training function with learning rate scheduling\"\"\"\n    \n    # TODO: Setup training components\n    # HINT: You need loss function, optimizer, and learning rate scheduler\n    \n    # TODO: Define loss function for multi-class classification\n    # HINT: Use nn.CrossEntropyLoss() for CIFAR-10 classification\n    criterion = None  # Your code here\n    \n    # TODO: Define optimizer\n    # HINT: Use optim.SGD with momentum=0.9, weight_decay=5e-4 for good results\n    # HINT: SGD(model.parameters(), lr=learning_rate, momentum=?, weight_decay=?)\n    optimizer = None  # Your code here\n    \n    # TODO: Define learning rate scheduler\n    # HINT: Use optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    # HINT: This reduces LR by 10x every 7 epochs\n    scheduler = None  # Your code here\n    \n    # Training history tracking\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': [],\n        'learning_rates': []\n    }\n    \n    print(f\"üöÄ Training {model.__class__.__name__}\")\n    print(f\"Epochs: {num_epochs}, Initial LR: {learning_rate}\")\n    print(f\"Optimizer: SGD with momentum, Weight decay: 5e-4\")\n    print(\"=\" * 60)\n    \n    best_val_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        # TODO: TRAINING PHASE\n        # HINT: Set model to training mode for dropout/batch norm\n        model.train()  # Already implemented for you\n        \n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1:2d}/{num_epochs} [Train]')\n        \n        for batch_idx, (data, target) in enumerate(train_pbar):\n            data, target = data.to(device), target.to(device)\n            \n            # TODO: Training step - implement the core training loop\n            # HINT: 1. Zero gradients, 2. Forward pass, 3. Compute loss, 4. Backward pass, 5. Update weights\n            \n            # Step 1: Zero gradients\n            None  # Your code here - use optimizer\n            \n            # Step 2: Forward pass\n            output = None  # Your code here - pass data through model\n            \n            # Step 3: Compute loss\n            loss = None  # Your code here - use criterion\n            \n            # Step 4: Backward pass\n            None  # Your code here - compute gradients\n            \n            # Step 5: Update weights\n            None  # Your code here - use optimizer\n            \n            # Statistics tracking (implemented for you)\n            train_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            train_total += target.size(0)\n            train_correct += (predicted == target).sum().item()\n            \n            # Update progress bar\n            if batch_idx % 50 == 0:\n                current_acc = 100. * train_correct / train_total\n                train_pbar.set_postfix({\n                    'Loss': f'{loss.item():.4f}',\n                    'Acc': f'{current_acc:.2f}%',\n                    'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n                })\n        \n        # TODO: VALIDATION PHASE\n        # HINT: Set model to evaluation mode and use torch.no_grad() for efficiency\n        \n        # Set model to evaluation mode\n        model.eval()  # Already implemented\n        \n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        # TODO: Disable gradient computation for validation\n        # HINT: Use 'with torch.no_grad():' context manager\n        with None:  # Your code here\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                \n                # TODO: Forward pass only (no training)\n                output = None  # Your code here - model forward pass\n                val_loss += criterion(output, target).item()\n                \n                # Calculate accuracy\n                _, predicted = torch.max(output.data, 1)\n                val_total += target.size(0)\n                val_correct += (predicted == target).sum().item()\n        \n        # Calculate epoch metrics\n        epoch_train_loss = train_loss / len(train_loader)\n        epoch_train_acc = 100. * train_correct / train_total\n        epoch_val_loss = val_loss / len(val_loader)\n        epoch_val_acc = 100. * val_correct / val_total\n        \n        # Store history\n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc)\n        history['val_loss'].append(epoch_val_loss)\n        history['val_acc'].append(epoch_val_acc)\n        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n        \n        # TODO: Update learning rate\n        # HINT: Use scheduler.step() to update learning rate according to schedule\n        None  # Your code here\n        \n        # Save best model (implemented for you)\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n        \n        # Print epoch summary\n        print(f\"Epoch {epoch+1:2d}: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, \"\n              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n    \n    print(f\"\\nüéØ Training Complete!\")\n    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n    \n    return history, best_val_acc\n\n# TODO: Create data loaders\n# HINT: Data loaders handle batching and shuffling for training\n\nbatch_size = 128\n\n# TODO: Create train/validation split\n# HINT: Use 90% for training, 10% for validation\n# HINT: Use torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_size = None  # Calculate as int(0.9 * len(train_dataset_aug))\nval_size = None    # Calculate as len(train_dataset_aug) - train_size\n\n# TODO: Split the augmented training dataset\n# HINT: train_subset, val_subset = torch.utils.data.random_split(train_dataset_aug, [train_size, val_size])\ntrain_subset, val_subset = None  # Your code here\n\n# TODO: Create data loaders\n# HINT: Use DataLoader(dataset, batch_size, shuffle=True/False, num_workers=2)\n# HINT: Training data should be shuffled, validation/test data should not\ntrain_loader = None  # Your code here - DataLoader for train_subset, shuffle=True\nval_loader = None    # Your code here - DataLoader for val_subset, shuffle=False  \ntest_loader = None   # Your code here - DataLoader for test_dataset_clean, shuffle=False\n\nprint(\"üß™ Testing Data Loader Setup...\")\n\ntry:\n    print(f\"üì¶ Data Loaders Created:\")\n    print(f\"   Training batches: {len(train_loader)}\")\n    print(f\"   Validation batches: {len(val_loader)}\")\n    print(f\"   Test batches: {len(test_loader)}\")\n    print(f\"   Batch size: {batch_size}\")\n    \n    # Test a batch\n    sample_batch = next(iter(train_loader))\n    print(f\"‚úÖ Sample batch shape: {sample_batch[0].shape}\")\n    print(f\"‚úÖ Sample labels shape: {sample_batch[1].shape}\")\n    \n    # Validation checks\n    if sample_batch[0].shape[0] == batch_size:\n        print(\"üéØ EXCELLENT! Batch size is correct\")\n    else:\n        print(f\"‚ö†Ô∏è Note: Last batch has {sample_batch[0].shape[0]} samples (normal)\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Data loader setup incomplete: {e}\")\n    print(\"üí° Common issues:\")\n    print(\"   ‚Ä¢ Make sure train_dataset_aug and test_dataset_clean are defined\")\n    print(\"   ‚Ä¢ Check that train_size and val_size calculations are correct\")\n    print(\"   ‚Ä¢ Verify DataLoader parameters are properly set\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-advanced-cnn",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Train your advanced CNN on CIFAR-10\nprint(\"üî• Advanced CNN Training Challenge\")\nprint(\"‚ö†Ô∏è  This section requires all previous components to be implemented!\")\n\nprint(\"üéØ Your Training Task:\")\nprint(\"1. Create a fresh AdvancedCNN model\")\nprint(\"2. Train it using your train_model function\")\nprint(\"3. Monitor training progress and validation accuracy\")\nprint(\"4. Analyze the results and training curves\")\nprint(\"5. Compare with transfer learning approaches\")\n\nprint(\"\\n‚öôÔ∏è Training Parameters to Experiment With:\")\nprint(\"   ‚Ä¢ Number of epochs: Start with 10, increase to 50+ for best results\")\nprint(\"   ‚Ä¢ Learning rate: Try 0.1, 0.01, 0.001\")\nprint(\"   ‚Ä¢ Batch size: 64, 128, 256\")\nprint(\"   ‚Ä¢ Optimization: SGD vs Adam\")\n\nprint(\"\\nüìä What to Monitor:\")\nprint(\"   ‚Ä¢ Training vs validation accuracy (watch for overfitting)\")\nprint(\"   ‚Ä¢ Loss curves (should decrease over time)\")\nprint(\"   ‚Ä¢ Learning rate schedule effects\")\nprint(\"   ‚Ä¢ Time per epoch and total training time\")\n\nprint(\"\\nüéØ Expected Performance:\")\nprint(\"   ‚Ä¢ Random baseline: ~10% (1/10 classes)\")\nprint(\"   ‚Ä¢ Simple CNN: ~60-70%\")\nprint(\"   ‚Ä¢ Well-tuned ResNet: ~80-90%\")\nprint(\"   ‚Ä¢ State-of-the-art: ~95%+\")\n\n# TODO: Implement your training here\ntry:\n    # Test if all components are ready\n    print(\"\\nüß™ Testing Component Readiness...\")\n    \n    # Check model\n    advanced_cnn = AdvancedCNN().to(device)\n    print(\"‚úÖ AdvancedCNN model ready\")\n    \n    # Check data loaders\n    if 'train_loader' in locals() and train_loader is not None:\n        print(\"‚úÖ Data loaders ready\")\n    else:\n        print(\"‚ùå Data loaders not ready - implement data loading section first\")\n        \n    # Check training function\n    if 'train_model' in locals():\n        print(\"‚úÖ Training function defined\")\n    else:\n        print(\"‚ùå Training function not ready - implement training section first\")\n    \n    print(\"\\nüöÄ Ready to train! Add your training code here:\")\n    print(\"# Your training code:\")\n    print(\"# start_time = time.time()\")\n    print(\"# history, best_acc = train_model(\")\n    print(\"#     advanced_cnn, train_loader, val_loader,\")\n    print(\"#     num_epochs=10, learning_rate=0.1\")\n    print(\"# )\")\n    print(\"# training_time = time.time() - start_time\")\n    print(\"# print(f'Training completed in {training_time:.1f} seconds')\")\n    print(\"# print(f'Best validation accuracy: {best_acc:.2f}%')\")\n    \n    # TODO: Uncomment and modify the training code above\n    # Remember: Training will take 15-30 minutes depending on your hardware\n    \n    print(\"\\nüí° Training Tips:\")\n    print(\"   ‚Ä¢ Use GPU if available for faster training\")\n    print(\"   ‚Ä¢ Start with fewer epochs to test your implementation\")\n    print(\"   ‚Ä¢ Save model checkpoints during training\")\n    print(\"   ‚Ä¢ Monitor for overfitting (val_acc plateau while train_acc increases)\")\n    print(\"   ‚Ä¢ Try different hyperparameters if results are poor\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Components not ready: {e}\")\n    print(\"üí° Complete the previous sections first:\")\n    print(\"   ‚Ä¢ BasicBlock implementation\")\n    print(\"   ‚Ä¢ AdvancedCNN implementation\")\n    print(\"   ‚Ä¢ Data augmentation and loading\")\n    print(\"   ‚Ä¢ Training function implementation\")"
  },
  {
   "cell_type": "markdown",
   "id": "part3-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Transfer Learning & Fine-tuning (45 minutes)\n",
    "\n",
    "**Goal:** Apply transfer learning with pre-trained models for efficient training\n",
    "\n",
    "Transfer learning leverages models pre-trained on ImageNet to achieve better performance with less training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transfer-learning-setup",
   "metadata": {},
   "outputs": [],
   "source": "# Setup for transfer learning\nprint(\"üîÑ Setting up Transfer Learning\")\n\ndef create_pretrained_model(model_name='resnet18', num_classes=10, feature_extract=True):\n    \"\"\"Create a pre-trained model for transfer learning\"\"\"\n    \n    model = None\n    \n    if model_name == 'resnet18':\n        # TODO: Load pre-trained ResNet18\n        # HINT: Use models.resnet18(pretrained=True) to load ImageNet weights\n        model = None  # Your code here\n        \n        # TODO: Freeze parameters for feature extraction\n        # HINT: If feature_extract=True, set param.requires_grad = False for all parameters\n        # HINT: Use: for param in model.parameters(): param.requires_grad = False\n        if feature_extract:\n            # Your code here - loop through model.parameters() and freeze them\n            pass\n        \n        # TODO: Replace final layer for CIFAR-10 (10 classes instead of 1000)\n        # HINT: Get input features: num_features = model.fc.in_features\n        # HINT: Replace: model.fc = nn.Linear(num_features, num_classes)\n        num_features = None  # Get model.fc.in_features\n        model.fc = None      # Create new Linear layer\n        \n    elif model_name == 'vgg16':\n        # TODO: Load pre-trained VGG16\n        # HINT: Use models.vgg16(pretrained=True)\n        model = None  # Your code here\n        \n        # TODO: Freeze feature extraction layers\n        # HINT: For VGG, freeze model.features.parameters() if feature_extract=True\n        if feature_extract:\n            # Your code here - freeze model.features.parameters()\n            pass\n        \n        # TODO: Replace final classifier layer\n        # HINT: VGG classifier is model.classifier[6] (the last layer)\n        # HINT: Replace: model.classifier[6] = nn.Linear(4096, num_classes)\n        model.classifier[6] = None  # Your code here\n    \n    return model\n\n# TODO: Create different transfer learning approaches\n# HINT: Feature extraction = freeze backbone, only train classifier (faster)\n# HINT: Fine-tuning = train all layers (slower, potentially better)\n\nprint(\"üîß Creating Transfer Learning Models...\")\n\ntry:\n    transfer_models = {\n        # TODO: Create ResNet18 with feature extraction\n        # HINT: Use create_pretrained_model('resnet18', num_classes=10, feature_extract=True)\n        'ResNet18 Feature Extraction': None,  # Your code here\n        \n        # TODO: Create ResNet18 with fine-tuning  \n        # HINT: Use create_pretrained_model('resnet18', num_classes=10, feature_extract=False)\n        'ResNet18 Fine-tuning': None,  # Your code here\n        \n        # TODO: Create VGG16 with feature extraction\n        'VGG16 Feature Extraction': None  # Your code here\n    }\n\n    # Move models to device and analyze\n    print(\"üìä Transfer Learning Model Analysis:\")\n    print(\"=\" * 70)\n    \n    for name, model in transfer_models.items():\n        if model is not None:\n            model = model.to(device)\n            \n            # Count trainable vs total parameters\n            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n            total_params = sum(p.numel() for p in model.parameters())\n            \n            print(f\"{name:25} | {trainable_params:>8,} / {total_params:>8,} trainable params\")\n        else:\n            print(f\"{name:25} | NOT IMPLEMENTED\")\n\n    print(f\"\\n‚úÖ Transfer learning models created\")\n    print(f\"üìù Key Concepts:\")\n    print(f\"   ‚Ä¢ Feature Extraction: Only train final layer (faster, less overfitting)\")\n    print(f\"   ‚Ä¢ Fine-tuning: Train all layers (slower, potentially better performance)\")\n    print(f\"   ‚Ä¢ Pre-trained models provide learned features from ImageNet\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Transfer learning setup incomplete: {e}\")\n    print(\"üí° Common issues:\")\n    print(\"   ‚Ä¢ Replace all 'None' with proper implementations\")\n    print(\"   ‚Ä¢ Make sure to import torchvision.models as models\")\n    print(\"   ‚Ä¢ Check that pretrained=True in model loading\")\n    print(\"   ‚Ä¢ Verify parameter freezing loop syntax\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-transfer-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transfer learning approaches\n",
    "print(\"‚öñÔ∏è Comparing Transfer Learning Approaches\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Modified training function for transfer learning\n",
    "def train_transfer_model(model, train_loader, val_loader, model_name, num_epochs=5):\n",
    "    \"\"\"Train transfer learning model with appropriate learning rate\"\"\"\n",
    "    \n",
    "    # Different learning rates for different approaches\n",
    "    if 'Feature Extraction' in model_name:\n",
    "        learning_rate = 0.001  # Higher LR for feature extraction\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        learning_rate = 0.0001  # Lower LR for fine-tuning\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"\\nüîÑ Training {model_name}\")\n",
    "    print(f\"Learning Rate: {learning_rate}, Epochs: {num_epochs}\")\n",
    "    \n",
    "    history = {'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for data, target in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                \n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"   Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return history, val_acc\n",
    "\n",
    "# TODO: Train and compare transfer learning models\n",
    "# Note: Using fewer epochs for demonstration (increase for better results)\n",
    "transfer_results = {}\n",
    "\n",
    "# Train each transfer learning approach\n",
    "for name, model in list(transfer_models.items())[:2]:  # Train first 2 models\n",
    "    start_time = time.time()\n",
    "    history, final_acc = train_transfer_model(\n",
    "        model, train_loader, val_loader, name, num_epochs=3\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    transfer_results[name] = {\n",
    "        'history': history,\n",
    "        'final_accuracy': final_acc,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Completed in {training_time:.1f}s, Final Accuracy: {final_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìä Transfer Learning Results Summary:\")\n",
    "for name, results in transfer_results.items():\n",
    "    print(f\"   {name}: {results['final_accuracy']:.2f}% in {results['training_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-transfer-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transfer learning comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learning curves comparison\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (name, results) in enumerate(transfer_results.items()):\n",
    "    epochs = range(1, len(results['history']['val_acc']) + 1)\n",
    "    ax1.plot(epochs, results['history']['val_acc'], \n",
    "             color=colors[i], linewidth=2, marker='o', \n",
    "             label=f\"{name.split()[0]} ({results['final_accuracy']:.1f}%)\", alpha=0.8)\n",
    "\n",
    "ax1.set_title('Transfer Learning: Validation Accuracy', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Validation Accuracy (%)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Performance vs Training Time\n",
    "names = list(transfer_results.keys())\n",
    "accuracies = [transfer_results[name]['final_accuracy'] for name in names]\n",
    "times = [transfer_results[name]['training_time'] for name in names]\n",
    "\n",
    "scatter = ax2.scatter(times, accuracies, s=100, c=colors[:len(names)], alpha=0.7)\n",
    "for i, name in enumerate(names):\n",
    "    ax2.annotate(name.split()[0], (times[i], accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax2.set_title('Performance vs Training Time', fontweight='bold')\n",
    "ax2.set_xlabel('Training Time (seconds)')\n",
    "ax2.set_ylabel('Final Validation Accuracy (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Transfer Learning Insights:\")\n",
    "print(\"   ‚Ä¢ Feature extraction is faster but may have lower accuracy\")\n",
    "print(\"   ‚Ä¢ Fine-tuning takes longer but can achieve better performance\")\n",
    "print(\"   ‚Ä¢ Pre-trained models provide significant advantage over training from scratch\")\n",
    "print(\"   ‚Ä¢ Choose approach based on dataset size and computational budget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Model Analysis & Interpretability (30 minutes)\n",
    "\n",
    "**Goal:** Analyze what our CNN learns and implement Grad-CAM for visual explanations\n",
    "\n",
    "Understanding what CNNs learn is crucial for building trust and debugging models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradcam-implementation",
   "metadata": {},
   "outputs": [],
   "source": "# Implement Grad-CAM for CNN interpretability\nclass GradCAM:\n    \"\"\"Gradient-weighted Class Activation Mapping\"\"\"\n    \n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        \n        # Register hooks (implemented for you)\n        self._register_hooks()\n    \n    def _register_hooks(self):\n        \"\"\"Register forward and backward hooks\"\"\"\n        \n        def forward_hook(module, input, output):\n            self.activations = output\n        \n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0]\n        \n        # Find target layer and register hooks\n        target_layer = dict(self.model.named_modules())[self.target_layer_name]\n        target_layer.register_forward_hook(forward_hook)\n        target_layer.register_backward_hook(backward_hook)\n    \n    def generate_cam(self, input_tensor, class_idx=None):\n        \"\"\"Generate Grad-CAM heatmap\"\"\"\n        \n        # TODO: Forward pass to get model output\n        # HINT: Set model to eval mode and pass input through model\n        self.model.eval()\n        output = None  # Your code here - forward pass\n        \n        # TODO: Get target class index\n        # HINT: If class_idx is None, use torch.argmax(output) to get predicted class\n        if class_idx is None:\n            class_idx = None  # Your code here\n        \n        # TODO: Backward pass to compute gradients\n        # HINT: Zero gradients first, then backpropagate from target class\n        # HINT: Use output[0, class_idx].backward() to get gradients for specific class\n        self.model.zero_grad()\n        None  # Your code here - backward pass\n        \n        # TODO: Compute Grad-CAM using gradients and activations\n        # HINT: self.gradients and self.activations were captured by hooks\n        \n        # Remove batch dimension\n        gradients = self.gradients[0]    # Shape: [channels, height, width]\n        activations = self.activations[0]  # Shape: [channels, height, width]\n        \n        # TODO: Compute importance weights by global average pooling of gradients\n        # HINT: Use torch.mean(gradients, dim=(1, 2)) to average over spatial dimensions\n        weights = None  # Your code here - global average pool\n        \n        # TODO: Compute weighted combination of activation maps\n        # HINT: Initialize cam as zeros with same spatial size as activations\n        # HINT: Loop through channels and add: cam += weight[i] * activations[i]\n        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)  # [height, width]\n        \n        # Your code here - weighted combination\n        for i, w in enumerate(weights):\n            cam += None  # w * activations[i]\n        \n        # TODO: Apply ReLU and normalize\n        # HINT: Use F.relu(cam) to remove negative values\n        # HINT: Normalize by dividing by torch.max(cam) if max > 0\n        cam = None  # Apply ReLU\n        if torch.max(cam) > 0:\n            cam = None  # Normalize by max value\n        \n        return cam.detach().cpu().numpy()\n\nprint(\"üîç Testing Grad-CAM Implementation...\")\n\ntry:\n    # Test Grad-CAM (requires a trained model)\n    print(\"‚ö†Ô∏è  Grad-CAM requires a trained model to work properly\")\n    print(\"‚úÖ Grad-CAM class structure implemented\")\n    print(\"üí° This will be tested after training your model\")\n    \n    print(\"\\nüß† Grad-CAM Concept:\")\n    print(\"   ‚Ä¢ Grad-CAM shows which image regions influence predictions\")\n    print(\"   ‚Ä¢ Uses gradients flowing back to target layer\")\n    print(\"   ‚Ä¢ Weights activation maps by their importance\")\n    print(\"   ‚Ä¢ Helps understand what the CNN is 'looking at'\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Grad-CAM implementation incomplete: {e}\")\n    print(\"üí° Common issues:\")\n    print(\"   ‚Ä¢ Replace all 'None' with proper implementations\")\n    print(\"   ‚Ä¢ Check tensor dimensions and operations\")\n    print(\"   ‚Ä¢ Make sure to use F.relu and torch.max correctly\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-gradcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Grad-CAM to analyze model decisions\n",
    "def visualize_gradcam_results(model, test_loader, num_samples=8):\n",
    "    \"\"\"Visualize Grad-CAM results for sample predictions\"\"\"\n",
    "    \n",
    "    # Initialize Grad-CAM\n",
    "    gradcam = GradCAM(model, 'layer4.1.conv2')  # Target last conv layer\n",
    "    \n",
    "    # Get sample images\n",
    "    model.eval()\n",
    "    images, labels, predictions, cams = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            for i in range(min(num_samples, data.size(0))):\n",
    "                if len(images) >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                # Get single image\n",
    "                img = data[i:i+1]\n",
    "                label = target[i].item()\n",
    "                \n",
    "                # Get prediction\n",
    "                output = model(img)\n",
    "                pred = torch.argmax(output, dim=1).item()\n",
    "                \n",
    "                # Generate Grad-CAM\n",
    "                cam = gradcam.generate_cam(img, class_idx=pred)\n",
    "                \n",
    "                # Store results\n",
    "                # Denormalize image for visualization\n",
    "                img_denorm = img[0].cpu()\n",
    "                img_denorm = img_denorm * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "                img_denorm = img_denorm + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "                img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "                \n",
    "                images.append(img_denorm.permute(1, 2, 0).numpy())\n",
    "                labels.append(label)\n",
    "                predictions.append(pred)\n",
    "                cams.append(cam)\n",
    "            \n",
    "            if len(images) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # TODO: Visualize results\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(20, 10))\n",
    "    fig.suptitle('Grad-CAM Analysis: What Does the CNN See?', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(images[i])\n",
    "        axes[0, i].set_title(f'Original\\nTrue: {cifar10_classes[labels[i]]}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Grad-CAM heatmap\n",
    "        im1 = axes[1, i].imshow(cams[i], cmap='jet', alpha=0.7)\n",
    "        axes[1, i].set_title(f'Grad-CAM\\nPred: {cifar10_classes[predictions[i]]}', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[2, i].imshow(images[i])\n",
    "        # Resize CAM to match image size\n",
    "        cam_resized = np.array(Image.fromarray(cams[i]).resize((32, 32)))\n",
    "        axes[2, i].imshow(cam_resized, cmap='jet', alpha=0.4)\n",
    "        \n",
    "        correct = '‚úì' if labels[i] == predictions[i] else '‚úó'\n",
    "        axes[2, i].set_title(f'Overlay {correct}', fontsize=10)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze results\n",
    "    correct_predictions = sum(1 for i in range(num_samples) if labels[i] == predictions[i])\n",
    "    print(f\"\\nüìä Analysis Results:\")\n",
    "    print(f\"   Accuracy on samples: {correct_predictions}/{num_samples} ({100*correct_predictions/num_samples:.1f}%)\")\n",
    "    print(f\"   ‚úÖ Red areas show where the CNN focuses attention\")\n",
    "    print(f\"   üîç Does the model focus on relevant object parts?\")\n",
    "    \n",
    "    return images, labels, predictions, cams\n",
    "\n",
    "# Apply Grad-CAM analysis\n",
    "print(\"üéØ Applying Grad-CAM Analysis...\")\n",
    "gradcam_results = visualize_gradcam_results(advanced_cnn, test_loader, num_samples=6)\n",
    "\n",
    "print(\"\\nüß† Interpretability Insights:\")\n",
    "print(\"   ‚Ä¢ Grad-CAM shows which image regions influence predictions\")\n",
    "print(\"   ‚Ä¢ Helps identify if model focuses on relevant features\")\n",
    "print(\"   ‚Ä¢ Can reveal biases or shortcut learning\")\n",
    "print(\"   ‚Ä¢ Useful for debugging and building trust in AI systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Optimization & Deployment (30 minutes)\n",
    "\n",
    "**Goal:** Optimize models for real-world deployment with quantization and pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-optimization",
   "metadata": {},
   "outputs": [],
   "source": "# Model optimization techniques\nprint(\"‚ö° Model Optimization for Deployment\")\n\ndef analyze_model_size(model, model_name):\n    \"\"\"Analyze model size and complexity\"\"\"\n    \n    # TODO: Count total parameters\n    # HINT: Use sum(p.numel() for p in model.parameters()) to count all parameters\n    total_params = None  # Your code here\n    \n    # TODO: Estimate model size in MB\n    # HINT: Assuming float32 (4 bytes per parameter): total_params * 4 / (1024 * 1024)\n    model_size_mb = None  # Your code here\n    \n    # Measure inference time (implemented for you)\n    model.eval()\n    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n    \n    # Warm up\n    for _ in range(10):\n        with torch.no_grad():\n            _ = model(dummy_input)\n    \n    # Measure inference time\n    start_time = time.time()\n    for _ in range(100):\n        with torch.no_grad():\n            _ = model(dummy_input)\n    avg_inference_time = (time.time() - start_time) / 100 * 1000  # ms\n    \n    return {\n        'name': model_name,\n        'parameters': total_params,\n        'size_mb': model_size_mb,\n        'inference_time_ms': avg_inference_time\n    }\n\n# TODO: Implement dynamic quantization\ndef quantize_model(model):\n    \"\"\"Apply dynamic quantization to reduce model size\"\"\"\n    \n    # TODO: Move model to CPU for quantization\n    # HINT: Use model.cpu() since quantization typically works on CPU\n    model_cpu = None  # Your code here\n    \n    # TODO: Apply dynamic quantization\n    # HINT: Use torch.quantization.quantize_dynamic()\n    # HINT: Target layer types: {nn.Linear, nn.Conv2d}\n    # HINT: Use dtype=torch.qint8 for 8-bit quantization\n    quantized_model = None  # Your code here - torch.quantization.quantize_dynamic(...)\n    \n    return quantized_model\n\nprint(\"üß™ Testing Model Optimization...\")\n\ntry:\n    # This will work once you have a trained model\n    print(\"‚ö†Ô∏è  Model optimization requires a trained model\")\n    print(\"üí° Complete the training sections first, then return here\")\n    \n    # Placeholder analysis (will work with dummy model)\n    print(\"\\nüìä Optimization Concepts:\")\n    print(\"   ‚Ä¢ Quantization: Reduce precision (float32 ‚Üí int8) for smaller models\")\n    print(\"   ‚Ä¢ Pruning: Remove less important weights/neurons\")\n    print(\"   ‚Ä¢ Knowledge Distillation: Train smaller model to mimic larger one\")\n    print(\"   ‚Ä¢ Mobile deployment: Trade accuracy for speed/size\")\n    \n    print(\"\\nüéØ Expected Benefits:\")\n    print(\"   ‚Ä¢ Model size reduction: 2-4x smaller\")\n    print(\"   ‚Ä¢ Inference speedup: 1.5-3x faster\")\n    print(\"   ‚Ä¢ Memory usage: Significantly reduced\")\n    print(\"   ‚Ä¢ Accuracy loss: Usually <1-2% if done carefully\")\n\nexcept Exception as e:\n    print(f\"‚ùå Optimization implementation incomplete: {e}\")\n    print(\"üí° Common issues:\")\n    print(\"   ‚Ä¢ Make sure to implement parameter counting\")\n    print(\"   ‚Ä¢ Check quantization function implementation\")\n    print(\"   ‚Ä¢ Verify that model is moved to CPU for quantization\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment pipeline\n",
    "print(\"üöÄ Creating Deployment Pipeline\")\n",
    "\n",
    "class CNNPredictor:\n",
    "    \"\"\"Production-ready CNN predictor\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, use_quantized=False):\n",
    "        self.device = torch.device('cpu')  # CPU for deployment\n",
    "        self.use_quantized = use_quantized\n",
    "        \n",
    "        # Load model\n",
    "        if use_quantized:\n",
    "            self.model = quantized_model\n",
    "        else:\n",
    "            self.model = AdvancedCNN()\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        \n",
    "        # Class names\n",
    "        self.classes = cifar10_classes\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"Make prediction on a single image\"\"\"\n",
    "        \n",
    "        # TODO: Preprocess image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply transforms\n",
    "        input_tensor = self.transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        return {\n",
    "            'class': self.classes[predicted_class],\n",
    "            'class_id': predicted_class,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': probabilities[0].numpy()\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, images):\n",
    "        \"\"\"Make predictions on a batch of images\"\"\"\n",
    "        \n",
    "        batch_tensor = torch.stack([self.transform(img) for img in images])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(batch_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(len(images)):\n",
    "            results.append({\n",
    "                'class': self.classes[predicted_classes[i].item()],\n",
    "                'class_id': predicted_classes[i].item(),\n",
    "                'confidence': probabilities[i, predicted_classes[i]].item(),\n",
    "                'probabilities': probabilities[i].numpy()\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test deployment pipeline\n",
    "print(\"üß™ Testing Deployment Pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Create predictor with quantized model\n",
    "    predictor = CNNPredictor('best_model.pth', use_quantized=True)\n",
    "    \n",
    "    # Get test image\n",
    "    test_image, test_label = test_dataset_clean[0]\n",
    "    \n",
    "    # Convert tensor to PIL Image for predictor\n",
    "    test_image_pil = transforms.ToPILImage()(test_image)\n",
    "    \n",
    "    # Make prediction\n",
    "    result = predictor.predict(test_image_pil)\n",
    "    \n",
    "    print(f\"‚úÖ Deployment pipeline working!\")\n",
    "    print(f\"   Predicted: {result['class']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"   Actual: {cifar10_classes[test_label]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Deployment test failed: {e}\")\n",
    "    print(\"   This is expected if model wasn't fully trained\")\n",
    "\n",
    "print(\"\\nüè≠ Deployment Considerations:\")\n",
    "print(\"   ‚Ä¢ Model quantization reduces size and improves speed\")\n",
    "print(\"   ‚Ä¢ CPU deployment is common for edge devices\")\n",
    "print(\"   ‚Ä¢ Preprocessing pipeline must match training\")\n",
    "print(\"   ‚Ä¢ Consider model versioning and A/B testing\")\n",
    "print(\"   ‚Ä¢ Monitor model performance in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Critical Analysis & Ethics (30 minutes)\n",
    "\n",
    "**Goal:** Reflect on the broader implications of computer vision systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-questions",
   "metadata": {},
   "source": [
    "## 6.1 Technical Analysis\n",
    "\n",
    "**TODO: Answer these questions based on your experiments:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1-architecture",
   "metadata": {},
   "source": [
    "**1. How did ResNet-style skip connections improve your model compared to a simple CNN?**\n",
    "\n",
    "[TODO: Compare the training behavior, final accuracy, and convergence speed. Discuss why skip connections help with very deep networks.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2-transfer-learning",
   "metadata": {},
   "source": [
    "**2. What were the advantages and disadvantages of transfer learning vs training from scratch?**\n",
    "\n",
    "**Advantages of Transfer Learning:**\n",
    "[TODO: List benefits like faster convergence, better performance with limited data, reduced computational cost]\n",
    "\n",
    "**Disadvantages of Transfer Learning:**\n",
    "[TODO: List limitations like domain mismatch, potential negative transfer, less interpretability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3-gradcam",
   "metadata": {},
   "source": [
    "**3. What did Grad-CAM reveal about your model's decision-making process?**\n",
    "\n",
    "[TODO: Analyze whether the model focused on relevant object parts, identify any concerning patterns or biases, discuss reliability of visual explanations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4-optimization",
   "metadata": {},
   "source": [
    "**4. How did model quantization affect performance and efficiency?**\n",
    "\n",
    "[TODO: Compare model size, inference speed, and accuracy. Discuss trade-offs and when quantization is appropriate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethics-analysis",
   "metadata": {},
   "source": [
    "## 6.2 Ethical Considerations in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5-bias",
   "metadata": {},
   "source": [
    "**5. What types of bias might exist in computer vision systems like the ones you built?**\n",
    "\n",
    "**Data Bias:**\n",
    "[TODO: Discuss how training data composition affects model behavior, underrepresentation of certain groups or scenarios]\n",
    "\n",
    "**Algorithmic Bias:**\n",
    "[TODO: Discuss how model architecture and training procedures might introduce bias]\n",
    "\n",
    "**Evaluation Bias:**\n",
    "[TODO: Discuss how evaluation metrics and test sets might not capture all relevant performance aspects]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6-applications",
   "metadata": {},
   "source": [
    "**6. Consider three real-world applications of computer vision. What are the potential risks and benefits?**\n",
    "\n",
    "**Application 1: Medical Image Diagnosis**\n",
    "Benefits: [TODO: List potential benefits like faster diagnosis, consistency, accessibility]\n",
    "Risks: [TODO: List potential risks like misdiagnosis, overreliance on AI, bias against certain populations]\n",
    "\n",
    "**Application 2: Autonomous Vehicles**\n",
    "Benefits: [TODO: List benefits like reduced accidents, accessibility for disabled, efficiency]\n",
    "Risks: [TODO: List risks like algorithmic bias, accountability issues, job displacement]\n",
    "\n",
    "**Application 3: Surveillance Systems**\n",
    "Benefits: [TODO: List benefits like crime prevention, security, efficiency]\n",
    "Risks: [TODO: List risks like privacy violations, false positives, authoritarian use]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7-fairness",
   "metadata": {},
   "source": [
    "**7. How can we make computer vision systems more fair and trustworthy?**\n",
    "\n",
    "**Technical Solutions:**\n",
    "[TODO: Discuss diverse training data, bias detection methods, robust evaluation, interpretability tools]\n",
    "\n",
    "**Policy Solutions:**\n",
    "[TODO: Discuss regulation, accountability frameworks, transparency requirements, public oversight]\n",
    "\n",
    "**Social Solutions:**\n",
    "[TODO: Discuss diverse development teams, community input, education, democratic governance]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q8-future",
   "metadata": {},
   "source": [
    "**8. What future developments in computer vision excite you most? What concerns you most?**\n",
    "\n",
    "**Exciting Developments:**\n",
    "[TODO: Discuss potential positive applications, technological advances, societal benefits]\n",
    "\n",
    "**Concerning Developments:**\n",
    "[TODO: Discuss potential negative applications, risks, societal concerns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary and Reflection\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! In this advanced assignment, you have:\n",
    "\n",
    "**Mastered advanced CNN architectures** with ResNet-style skip connections  \n",
    "**Applied transfer learning** for efficient training and better performance  \n",
    "**Worked with complex datasets** requiring sophisticated preprocessing  \n",
    "**Implemented model interpretability** using Grad-CAM visualizations  \n",
    "**Optimized models for deployment** with quantization techniques  \n",
    "**Reflected critically** on ethics and bias in computer vision systems  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**TODO: Write 4-5 key insights from this assignment:**\n",
    "\n",
    "1. [TODO: Your first key takeaway about advanced CNN architectures and their capabilities]\n",
    "2. [TODO: Your second key takeaway about transfer learning and its practical benefits]\n",
    "3. [TODO: Your third key takeaway about model interpretability and explainable AI]\n",
    "4. [TODO: Your fourth key takeaway about deployment optimization and real-world constraints]\n",
    "5. [TODO: Your fifth key takeaway about ethics and responsibility in AI development]\n",
    "\n",
    "### Comparison to Previous Assignments\n",
    "\n",
    "**TODO: Compare this advanced CNN work to your previous assignments:**\n",
    "\n",
    "**Evolution from Linear Models (CA1) to CNNs:**\n",
    "[TODO: Discuss the progression in complexity and capability]\n",
    "\n",
    "**Advancement from Basic CNNs (IC) to Advanced Techniques:**\n",
    "[TODO: Compare simple MNIST CNNs to complex CIFAR-10 architectures with transfer learning]\n",
    "\n",
    "### Looking Forward\n",
    "\n",
    "**TODO: What aspects of computer vision would you like to explore further?**\n",
    "\n",
    "[TODO: Mention interests in object detection, semantic segmentation, generative models, video analysis, or domain-specific applications]\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "**TODO: Write a comprehensive reflection (200-300 words) on your experience with advanced computer vision:**\n",
    "\n",
    "[TODO: Your final reflection here - discuss the complexity of modern AI systems, the importance of responsible development, what surprised you about transfer learning and interpretability, how this connects to real-world AI applications, and your thoughts on the future of computer vision technology]\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Complete!**\n",
    "\n",
    "Make sure to:\n",
    "1. Complete all TODO sections\n",
    "2. Test your implementations thoroughly\n",
    "3. Answer all reflection questions thoughtfully\n",
    "4. Save your notebook and export as PDF\n",
    "5. Submit both .ipynb and .pdf files\n",
    "6. Include your name and student ID at the top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}